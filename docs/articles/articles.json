[
  {
    "path": "articles/2021-10-19-great-pumpkins/",
    "title": "Pumpkins, Pumpkins, Pumpkins",
    "description": "Learning about giant pumpkin contest winners.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2021-10-22",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\nIntial EDA\r\n\r\nRows: 28,065\r\nColumns: 14\r\n$ id                <chr> \"2013-F\", \"2013-F\", \"2013-F\", \"2013-F\", \"2…\r\n$ place             <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"5\", \"7\", \"8\", \"9…\r\n$ weight_lbs        <chr> \"154.50\", \"146.50\", \"145.00\", \"140.80\", \"1…\r\n$ grower_name       <chr> \"Ellenbecker, Todd & Sequoia\", \"Razo, Stev…\r\n$ city              <chr> \"Gleason\", \"New Middletown\", \"Glenson\", \"C…\r\n$ state_prov        <chr> \"Wisconsin\", \"Ohio\", \"Wisconsin\", \"Wiscons…\r\n$ country           <chr> \"United States\", \"United States\", \"United …\r\n$ gpc_site          <chr> \"Nekoosa Giant Pumpkin Fest\", \"Ohio Valley…\r\n$ seed_mother       <chr> \"209 Werner\", \"150.5 Snyder\", \"209 Werner\"…\r\n$ pollinator_father <chr> \"Self\", NA, \"103 Mackinnon\", \"209 Werner '…\r\n$ ott               <chr> \"184.0\", \"194.0\", \"177.0\", \"194.0\", \"0.0\",…\r\n$ est_weight        <chr> \"129.00\", \"151.00\", \"115.00\", \"151.00\", \"0…\r\n$ pct_chart         <chr> \"20.0\", \"-3.0\", \"26.0\", \"-7.0\", \"0.0\", \"-1…\r\n$ variety           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\r\n\r\n\r\nTable 1: Data summary\r\nName\r\npumpkins\r\nNumber of rows\r\n28065\r\nNumber of columns\r\n14\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n3\r\nfactor\r\n6\r\nnumeric\r\n5\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nid\r\n0\r\n1\r\n6\r\n6\r\n0\r\n54\r\n0\r\ngrower_name\r\n0\r\n1\r\n4\r\n79\r\n0\r\n7982\r\n0\r\ncountry\r\n0\r\n1\r\n5\r\n79\r\n0\r\n75\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\ncity\r\n2779\r\n0.90\r\nFALSE\r\n3218\r\nSte: 292, Nap: 183, Por: 178, St.: 162\r\nstate_prov\r\n0\r\n1.00\r\nFALSE\r\n188\r\nOth: 2242, Ont: 2021, Wis: 1910, Cal: 1211\r\ngpc_site\r\n0\r\n1.00\r\nFALSE\r\n220\r\nOhi: 759, Wie: 749, Ear: 722, Bau: 548\r\nseed_mother\r\n8537\r\n0.70\r\nFALSE\r\n9996\r\nunk: 277, Unk: 260, 214: 122, 200: 104\r\npollinator_father\r\n10302\r\n0.63\r\nFALSE\r\n4538\r\nope: 2658, Ope: 2065, sel: 2020, Sel: 1875\r\nvariety\r\n27341\r\n0.03\r\nFALSE\r\n86\r\nBig: 349, Dom: 150, Del: 37, Big: 33\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nplace\r\n2381\r\n0.92\r\n520.66\r\n499.83\r\n1.0\r\n119\r\n265.0\r\n909.00\r\n1798.0\r\n▇▂▂▂▁\r\nweight_lbs\r\n5267\r\n0.81\r\n303.52\r\n295.23\r\n0.1\r\n70\r\n169.5\r\n526.38\r\n999.8\r\n▇▂▂▂▂\r\nott\r\n3211\r\n0.89\r\n202.46\r\n154.89\r\n0.0\r\n0\r\n233.0\r\n338.00\r\n1132.0\r\n▇▇▁▁▁\r\nest_weight\r\n8233\r\n0.71\r\n273.48\r\n314.90\r\n0.0\r\n0\r\n135.0\r\n518.00\r\n998.0\r\n▇▂▂▂▂\r\npct_chart\r\n3211\r\n0.89\r\n0.45\r\n17.06\r\n-100.0\r\n-3\r\n0.0\r\n3.00\r\n830.0\r\n▇▁▁▁▁\r\n\r\nUgh. We need to do some data cleaning.\r\n\r\n                                                                                         state_prov\r\n  1498 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(91 exhibition only,\\r\\n               29 damaged)\r\n     151 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               2 damaged)\r\n 1569 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(154 exhibition only,\\r\\n               24 damaged)\r\n     159 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(1 exhibition only,\\r\\n               2 damaged)\r\n     160 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(3 exhibition only,\\r\\n               2 damaged)\r\n 1681 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(108 exhibition only,\\r\\n               31 damaged)\r\n 1742 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(104 exhibition only,\\r\\n               46 damaged)\r\n     179 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(2 exhibition only,\\r\\n               2 damaged)\r\n  1798 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(99 exhibition only,\\r\\n               40 damaged)\r\n     185 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               4 damaged)\r\n 1883 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(111 exhibition only,\\r\\n               37 damaged)\r\n 1900 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(128 exhibition only,\\r\\n               29 damaged)\r\n 1905 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(103 exhibition only,\\r\\n               43 damaged)\r\n     192 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(2 exhibition only,\\r\\n               3 damaged)\r\n    194 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(10 exhibition only,\\r\\n               4 damaged)\r\n     197 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(7 exhibition only,\\r\\n               4 damaged)\r\n 1980 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(150 exhibition only,\\r\\n               32 damaged)\r\n     200 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(7 exhibition only,\\r\\n               4 damaged)\r\n    203 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(11 exhibition only,\\r\\n               2 damaged)\r\n     206 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(3 exhibition only,\\r\\n               3 damaged)\r\n     206 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               2 damaged)\r\n    219 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(6 exhibition only,\\r\\n               12 damaged)\r\n     219 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(7 exhibition only,\\r\\n               4 damaged)\r\n    226 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(14 exhibition only,\\r\\n               2 damaged)\r\n     227 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(4 exhibition only,\\r\\n               1 damaged)\r\n    246 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(18 exhibition only,\\r\\n               2 damaged)\r\n    253 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(18 exhibition only,\\r\\n               5 damaged)\r\n    254 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(9 exhibition only,\\r\\n               10 damaged)\r\n    256 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(11 exhibition only,\\r\\n               1 damaged)\r\n    271 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(22 exhibition only,\\r\\n               2 damaged)\r\n    272 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(15 exhibition only,\\r\\n               4 damaged)\r\n    273 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(22 exhibition only,\\r\\n               0 damaged)\r\n    275 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(13 exhibition only,\\r\\n               0 damaged)\r\n    278 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(19 exhibition only,\\r\\n               2 damaged)\r\n    283 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(27 exhibition only,\\r\\n               2 damaged)\r\n    289 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(26 exhibition only,\\r\\n               4 damaged)\r\n    290 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(15 exhibition only,\\r\\n               5 damaged)\r\n    291 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(16 exhibition only,\\r\\n               4 damaged)\r\n     292 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(9 exhibition only,\\r\\n               3 damaged)\r\n    294 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(23 exhibition only,\\r\\n               3 damaged)\r\n    300 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(14 exhibition only,\\r\\n               1 damaged)\r\n    314 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(21 exhibition only,\\r\\n               2 damaged)\r\n    314 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(31 exhibition only,\\r\\n               3 damaged)\r\n    316 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(24 exhibition only,\\r\\n               1 damaged)\r\n    319 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(29 exhibition only,\\r\\n               2 damaged)\r\n    326 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(29 exhibition only,\\r\\n               3 damaged)\r\n    328 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(29 exhibition only,\\r\\n               2 damaged)\r\n    330 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(24 exhibition only,\\r\\n               2 damaged)\r\n    334 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(27 exhibition only,\\r\\n               2 damaged)\r\n    364 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(49 exhibition only,\\r\\n               6 damaged)\r\n    366 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(38 exhibition only,\\r\\n               6 damaged)\r\n    370 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(27 exhibition only,\\r\\n               5 damaged)\r\n    383 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(47 exhibition only,\\r\\n               6 damaged)\r\n   451 Entries.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t(73 exhibition only,\\r\\n               13 damaged)\r\n                                                                                            Alabama\r\n                                                                                             Alaska\r\n                                                                                            Alberta\r\n                                                                                            Antwerp\r\n                                                                                             Aragon\r\n                                                                                            Arizona\r\n                                                                                           Arkansas\r\n                                                                                 Baden-Wuerttemberg\r\n                                                                                         Basilicata\r\n                                                                                     Basque Country\r\n                                                                                            Bavaria\r\n                                                                                             Berlin\r\n                                                                                               Bern\r\n                                                                                        Brandenburg\r\n                                                                                   British Columbia\r\n                                                                                         Burgenland\r\n                                                                                         California\r\n                                                                                           Campania\r\n                                                                                          Carinthia\r\n                                                                                          Catalonia\r\n                                                                                    Central Finland\r\n                                                                                           Colorado\r\n                                                                                        Connecticut\r\n                                                                                           Delaware\r\n                                                                                      East Flanders\r\n                                                                                     Emilia-Romagna\r\n                                                                                            England\r\n                                                                                    Flemish Brabant\r\n                                                                                            Florida\r\n                                                                                          Friesland\r\n                                                                                            Galicia\r\n                                                                                         Gelderland\r\n                                                                                            Georgia\r\n                                                                                        Graubuenden\r\n                                                                                     Greater Poland\r\n                                                                                             Hawaii\r\n                                                                                              Hesse\r\n                                                                                              Idaho\r\n                                                                                           Illinois\r\n                                                                                            Indiana\r\n                                                                                               Iowa\r\n                                                                                             Kansas\r\n                                                                                           Kentucky\r\n                                                                                        Kymenlaakso\r\n                                                                                           La Rioja\r\n                                                                                            Lapland\r\n                                                                                              Lazio\r\n                                                                                      Lesser Poland\r\n                                                                                            Limburg\r\n                                                                                           Lombardy\r\n                                                                                          Louisiana\r\n                                                                                      Lower Austria\r\n                                                                                       Lower Saxony\r\n                                                                                     Lower Silesian\r\n                                                                                             Lubusz\r\n                                                                                              Maine\r\n                                                                                           Manitoba\r\n                                                                                           Maryland\r\n                                                                                           Masovian\r\n                                                                                      Massachusetts\r\n                                                                             Mecklenburg-Vorpommern\r\n                                                                                           Michigan\r\n                                                                                          Minnesota\r\n                                                                                        Mississippi\r\n                                                                                           Missouri\r\n                                                                                            Montana\r\n                                                                                            Navarre\r\n                                                                                           Nebraska\r\n                                                                                             Nevada\r\n                                                                                      New Brunswick\r\n                                                                                      New Hampshire\r\n                                                                                         New Jersey\r\n                                                                                         New Mexico\r\n                                                                                           New York\r\n                                                                                      North Brabant\r\n                                                                                     North Carolina\r\n                                                                                       North Dakota\r\n                                                                                      North Holland\r\n                                                                                      North Karelia\r\n                                                                             North Rhine-Westphalia\r\n                                                                                   Northern Savonia\r\n                                                                                        Nova Scotia\r\n                                                                                               Ohio\r\n                                                                                           Oklahoma\r\n                                                                                            Ontario\r\n                                                                                              Opole\r\n                                                                                             Oregon\r\n                                                                                              Other\r\n                                                                                         Overijssel\r\n                                                                                  Paijanne Tavastia\r\n                                                                                       Pennsylvania\r\n                                                                                           Piedmont\r\n                                                                                          Pirkanmaa\r\n                                                                                          Podlaskie\r\n                                                                                         Pomeranian\r\n                                                                               Prince Edward Island\r\n                                                                                             Quebec\r\n                                                                               Rhineland-Palatinate\r\n                                                                                       Rhode Island\r\n                                                                                           Saarland\r\n                                                                                           Sardinia\r\n                                                                                       Saskatchewan\r\n                                                                                          Satakunta\r\n                                                                                             Saxony\r\n                                                                                      Saxony-Anhalt\r\n                                                                                           Silesian\r\n                                                                                     South Carolina\r\n                                                                                       South Dakota\r\n                                                                                      South Holland\r\n                                                                                   Southern Savonia\r\n                                                                                             Styria\r\n                                                                                      Subcarpathian\r\n                                                                                    Tavastia Proper\r\n                                                                                          Tennessee\r\n                                                                                              Texas\r\n                                                                                          Thuringia\r\n                                                                                              Tirol\r\n                                                                                            Tuscany\r\n                                                                                             Umbria\r\n                                                                                      Upper Austria\r\n                                                                                               Utah\r\n                                                                                            Utrecht\r\n                                                                                            Uusimaa\r\n                                                                                Valencian Community\r\n                                                                                             Veneto\r\n                                                                                            Vermont\r\n                                                                                             Vienna\r\n                                                                                           Virginia\r\n                                                                                         Vorarlberg\r\n                                                                                         Washington\r\n                                                                                      West Virginia\r\n                                                                                          Wisconsin\r\n                                                                                            Wyoming\r\n                                                                                            Zeeland\r\n    n      percent\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n    1 3.563157e-05\r\n   75 2.672368e-03\r\n   61 2.173526e-03\r\n  300 1.068947e-02\r\n  232 8.266524e-03\r\n   12 4.275788e-04\r\n   12 4.275788e-04\r\n  131 4.667736e-03\r\n  129 4.596472e-03\r\n    1 3.563157e-05\r\n    2 7.126314e-05\r\n  144 5.130946e-03\r\n    1 3.563157e-05\r\n   12 4.275788e-04\r\n  657 2.340994e-02\r\n  183 6.520577e-03\r\n    5 1.781578e-04\r\n 1211 4.314983e-02\r\n   36 1.282737e-03\r\n   23 8.195261e-04\r\n   46 1.639052e-03\r\n   51 1.817210e-03\r\n  604 2.152147e-02\r\n  459 1.635489e-02\r\n    5 1.781578e-04\r\n   31 1.104579e-03\r\n  142 5.059683e-03\r\n  231 8.230893e-03\r\n   39 1.389631e-03\r\n    1 3.563157e-05\r\n    6 2.137894e-04\r\n    1 3.563157e-05\r\n    8 2.850526e-04\r\n   29 1.033316e-03\r\n    2 7.126314e-05\r\n    8 2.850526e-04\r\n    3 1.068947e-04\r\n   57 2.030999e-03\r\n   42 1.496526e-03\r\n  192 6.841261e-03\r\n  674 2.401568e-02\r\n  599 2.134331e-02\r\n   78 2.779262e-03\r\n  345 1.229289e-02\r\n   12 4.275788e-04\r\n    8 2.850526e-04\r\n    1 3.563157e-05\r\n    4 1.425263e-04\r\n   15 5.344735e-04\r\n   44 1.567789e-03\r\n  166 5.914841e-03\r\n   25 8.907892e-04\r\n  382 1.361126e-02\r\n   11 3.919473e-04\r\n    7 2.494210e-04\r\n    2 7.126314e-05\r\n  332 1.182968e-02\r\n  262 9.335471e-03\r\n    7 2.494210e-04\r\n    1 3.563157e-05\r\n  415 1.478710e-02\r\n    8 2.850526e-04\r\n 1128 4.019241e-02\r\n  708 2.522715e-02\r\n    3 1.068947e-04\r\n  165 5.879209e-03\r\n   35 1.247105e-03\r\n   75 2.672368e-03\r\n   69 2.458578e-03\r\n    7 2.494210e-04\r\n  211 7.518261e-03\r\n  219 7.803314e-03\r\n   22 7.838945e-04\r\n    3 1.068947e-04\r\n  802 2.857652e-02\r\n   15 5.344735e-04\r\n  427 1.521468e-02\r\n   42 1.496526e-03\r\n   16 5.701051e-04\r\n   34 1.211473e-03\r\n  302 1.076073e-02\r\n    2 7.126314e-05\r\n 1049 3.737752e-02\r\n 1190 4.240157e-02\r\n   61 2.173526e-03\r\n 2021 7.201140e-02\r\n    5 1.781578e-04\r\n  873 3.110636e-02\r\n 2242 7.988598e-02\r\n   25 8.907892e-04\r\n    9 3.206841e-04\r\n  992 3.534652e-02\r\n   69 2.458578e-03\r\n    1 3.563157e-05\r\n    4 1.425263e-04\r\n    2 7.126314e-05\r\n   24 8.551577e-04\r\n  457 1.628363e-02\r\n  137 4.881525e-03\r\n  308 1.097452e-02\r\n   15 5.344735e-04\r\n    1 3.563157e-05\r\n   15 5.344735e-04\r\n    1 3.563157e-05\r\n  173 6.164262e-03\r\n   99 3.527525e-03\r\n   21 7.482630e-04\r\n    9 3.206841e-04\r\n  201 7.161945e-03\r\n   61 2.173526e-03\r\n    2 7.126314e-05\r\n   26 9.264208e-04\r\n    6 2.137894e-04\r\n    6 2.137894e-04\r\n  251 8.943524e-03\r\n   10 3.563157e-04\r\n  175 6.235525e-03\r\n    1 3.563157e-05\r\n  144 5.130946e-03\r\n   16 5.701051e-04\r\n   36 1.282737e-03\r\n  518 1.845715e-02\r\n   23 8.195261e-04\r\n   53 1.888473e-03\r\n    9 3.206841e-04\r\n   11 3.919473e-04\r\n  381 1.357563e-02\r\n   56 1.995368e-03\r\n  146 5.202209e-03\r\n    4 1.425263e-04\r\n 1118 3.983609e-02\r\n   89 3.171210e-03\r\n 1910 6.805630e-02\r\n   85 3.028683e-03\r\n    3 1.068947e-04\r\n\r\nLet’s filter out all of those entries that have damaged as state_prov. This should simplify our summaries.\r\n\r\n [1] Wisconsin    California   Ohio         Michigan     Washington  \r\n [6] Pennsylvania Oregon       New York     Minnesota    Indiana     \r\n134 Levels: Alabama Alaska Alberta Antwerp Aragon Arizona ... Zeeland\r\n\r\nLet’s visualize the top 10 states by median pumpkin weight in these contests. California has the highest median pumpkin weight of these states.\r\n\r\n\r\n\r\nLet’s separate the pumpkins out by type, and compare the distributions over the years for United States, United Kingdom and Canada. You can click on the legend to remove or show the different countries.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2021-10-12-seafood/",
    "title": "Tidy Tuesday: Seafood Production and Consumption",
    "description": "Understanding global cephalopod production.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2021-10-12",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nUnderstanding Seafood Production\r\nLoading the Data\r\n\r\nUnderstanding Seafood Production\r\nFor Tidy Tuesday, this week I decided to tackle a relatively easy question this week by understanding seafood production over the years. Since I am a big octopus fan. Which countries were responsible for the top production of cephalopods over the years?\r\nLoading the Data\r\n\r\n\r\nfarmed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/aquaculture-farmed-fish-production.csv')\r\nconsumption <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/fish-and-seafood-consumption-per-capita.csv')\r\nproduction <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-12/seafood-and-fish-production-thousand-tonnes.csv')\r\n\r\n\r\n\r\n\r\n\r\nproduction <- janitor::clean_names(production)\r\nskimr::skim(production)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nproduction\r\nNumber of rows\r\n10326\r\nNumber of columns\r\n10\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n2\r\nnumeric\r\n8\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nentity\r\n0\r\n1.00\r\n4\r\n39\r\n0\r\n215\r\n0\r\ncode\r\n1734\r\n0.83\r\n3\r\n8\r\n0\r\n181\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nyear\r\n0\r\n1.00\r\n1987.70\r\n15.35\r\n1961\r\n1974.0\r\n1988.0\r\n2001.00\r\n2013\r\n▇▆▇▇▇\r\ncommodity_balances_livestock_and_fish_primary_equivalent_pelagic_fish_2763_production_5510_tonnes\r\n1586\r\n0.85\r\n877685.06\r\n3244678.56\r\n0\r\n1300.0\r\n35667.0\r\n330158.00\r\n43756110\r\n▇▁▁▁▁\r\ncommodity_balances_livestock_and_fish_primary_equivalent_crustaceans_2765_production_5510_tonnes\r\n1146\r\n0.89\r\n128038.08\r\n680253.04\r\n0\r\n97.0\r\n3076.0\r\n29500.00\r\n12607540\r\n▇▁▁▁▁\r\ncommodity_balances_livestock_and_fish_primary_equivalent_cephalopods_2766_production_5510_tonnes\r\n2836\r\n0.73\r\n67800.81\r\n291850.11\r\n0\r\n0.0\r\n546.0\r\n12795.00\r\n4285298\r\n▇▁▁▁▁\r\ncommodity_balances_livestock_and_fish_primary_equivalent_demersal_fish_2762_production_5510_tonnes\r\n1822\r\n0.82\r\n498916.69\r\n1831355.82\r\n0\r\n799.5\r\n19591.5\r\n191905.50\r\n22261372\r\n▇▁▁▁▁\r\ncommodity_balances_livestock_and_fish_primary_equivalent_freshwater_fish_2761_production_5510_tonnes\r\n527\r\n0.95\r\n465803.05\r\n2569244.83\r\n0\r\n681.5\r\n10600.0\r\n78768.50\r\n52335573\r\n▇▁▁▁▁\r\ncommodity_balances_livestock_and_fish_primary_equivalent_molluscs_other_2767_production_5510_tonnes\r\n2860\r\n0.72\r\n238475.03\r\n1343302.70\r\n0\r\n5.0\r\n1394.0\r\n37754.25\r\n17952945\r\n▇▁▁▁▁\r\ncommodity_balances_livestock_and_fish_primary_equivalent_marine_fish_other_2764_production_5510_tonnes\r\n1670\r\n0.84\r\n214545.61\r\n935412.65\r\n0\r\n939.0\r\n5700.0\r\n45368.50\r\n10865669\r\n▇▁▁▁▁\r\n\r\ncolnames(production) <- stringr::str_replace(colnames(production), \"commodity_balances_livestock_and_fish_primary_equivalent_\",replacement = \"\")\r\n\r\ncolnames(production) <- stringr::str_replace(colnames(production), \"_production_5510_tonnes\", replacement=\"\")\r\n\r\nskimr::skim(production)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nproduction\r\nNumber of rows\r\n10326\r\nNumber of columns\r\n10\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n2\r\nnumeric\r\n8\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nentity\r\n0\r\n1.00\r\n4\r\n39\r\n0\r\n215\r\n0\r\ncode\r\n1734\r\n0.83\r\n3\r\n8\r\n0\r\n181\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nyear\r\n0\r\n1.00\r\n1987.70\r\n15.35\r\n1961\r\n1974.0\r\n1988.0\r\n2001.00\r\n2013\r\n▇▆▇▇▇\r\npelagic_fish_2763\r\n1586\r\n0.85\r\n877685.06\r\n3244678.56\r\n0\r\n1300.0\r\n35667.0\r\n330158.00\r\n43756110\r\n▇▁▁▁▁\r\ncrustaceans_2765\r\n1146\r\n0.89\r\n128038.08\r\n680253.04\r\n0\r\n97.0\r\n3076.0\r\n29500.00\r\n12607540\r\n▇▁▁▁▁\r\ncephalopods_2766\r\n2836\r\n0.73\r\n67800.81\r\n291850.11\r\n0\r\n0.0\r\n546.0\r\n12795.00\r\n4285298\r\n▇▁▁▁▁\r\ndemersal_fish_2762\r\n1822\r\n0.82\r\n498916.69\r\n1831355.82\r\n0\r\n799.5\r\n19591.5\r\n191905.50\r\n22261372\r\n▇▁▁▁▁\r\nfreshwater_fish_2761\r\n527\r\n0.95\r\n465803.05\r\n2569244.83\r\n0\r\n681.5\r\n10600.0\r\n78768.50\r\n52335573\r\n▇▁▁▁▁\r\nmolluscs_other_2767\r\n2860\r\n0.72\r\n238475.03\r\n1343302.70\r\n0\r\n5.0\r\n1394.0\r\n37754.25\r\n17952945\r\n▇▁▁▁▁\r\nmarine_fish_other_2764\r\n1670\r\n0.84\r\n214545.61\r\n935412.65\r\n0\r\n939.0\r\n5700.0\r\n45368.50\r\n10865669\r\n▇▁▁▁▁\r\n\r\nI’ll pivot the production data frame to a longer one using pivot_longer().\r\n\r\n\r\nproduction_long <- production %>% tidyr::pivot_longer(cols=contains(\"_\"), names_to = \"seafood_type\",values_to = \"production\")\r\n\r\nhead(production_long)\r\n\r\n\r\n# A tibble: 6 × 5\r\n  entity      code   year seafood_type         production\r\n  <chr>       <chr> <dbl> <chr>                     <dbl>\r\n1 Afghanistan AFG    1961 pelagic_fish_2763            NA\r\n2 Afghanistan AFG    1961 crustaceans_2765             NA\r\n3 Afghanistan AFG    1961 cephalopods_2766             NA\r\n4 Afghanistan AFG    1961 demersal_fish_2762           NA\r\n5 Afghanistan AFG    1961 freshwater_fish_2761        300\r\n6 Afghanistan AFG    1961 molluscs_other_2767          NA\r\n\r\nNow we have the long form data frame, we can now ask some interesting time questions and compare across categories. As you can see below, total seafood production has risen steadily over the years.\r\n\r\n\r\nproduction_long %>%\r\n  group_by(year, seafood_type) %>%\r\n  summarize(production = mean(production, na.rm=TRUE)) %>%\r\n  ggplot() + \r\n  aes(x=year, y=production, fill=seafood_type) +\r\n  geom_area() + \r\n  viridis::scale_fill_viridis(discrete=TRUE) +\r\n  hrbrthemes::theme_ipsum() + \r\n  ggtitle(\"Production of Seafood has Risen Steadily over the Years\")\r\n\r\n\r\n\r\n\r\nDrilling into the cephalpods, I’m interested in percent production of the total for the top 10 producing countries.\r\nInteresting that Japan’s share of production has decreased steadily, and that China is a leading producer lately.\r\n\r\n\r\nproduction_long <- production %>% \r\n  tidyr::pivot_longer(cols=contains(\"_\"), \r\n  names_to = \"seafood_type\",values_to = \"production\")\r\n\r\ntop_squid_eaters <- production_long %>%\r\n  filter(seafood_type == \"cephalopods_2766\") %>%\r\n  filter(code != \"OWID_WRL\") %>%\r\n  group_by(code) %>%\r\n  summarize(total_eating = sum(production)) %>%\r\n  arrange(desc(total_eating)) %>%\r\n  slice(1:10) %>%\r\n  pull(code)\r\n\r\ntotal_production <- production_long %>%\r\n  filter(seafood_type == \"cephalopods_2766\") %>%\r\n  filter(code != \"OWID_WRL\") %>%\r\n  filter(code %in% top_squid_eaters) %>%\r\n  group_by(year) %>%\r\n  summarize(total_eating = sum(production, na.rm=TRUE)) \r\n\r\ntotal_ceph <- production_long %>%\r\n  filter(seafood_type == \"cephalopods_2766\") %>%\r\n  filter(code %in% top_squid_eaters) %>%\r\n  group_by(year, code) %>%\r\n  summarize(production = mean(production, na.rm=TRUE), entity) %>%\r\n  left_join(y=total_production, by=\"year\") %>%\r\n  mutate(percent = production/total_eating * 100) %>%\r\n  ggplot() + \r\n  aes(x=year, y=percent, fill=entity) +\r\n  geom_area() + \r\n  viridis::scale_fill_viridis(discrete=TRUE, option=\"plasma\") +\r\n  hrbrthemes::theme_ipsum() \r\n\r\ntotal_ceph\r\n\r\n\r\n\r\n\r\n\r\n\r\ntotal_ceph + annotate(geom=\"text\", x= 1969, y=60, label = \"Japan\", colour=\"lightgrey\", size=4) +\r\n  annotate(geom=\"text\", x=2005, y=80, label=\"China\", colour=\"lightgrey\", size=4) + \r\n  annotate(geom=\"text\", x=1991, y=44, label=\"South Korea\", color=\"lightgrey\", size=4) +\r\n    annotate(geom=\"text\", x=2008, y=40, label=\"Peru\", color=\"lightgrey\", size=4) +\r\n  labs(title=\"Top 10 cephalopod producers\", subtitle = \"Japan, South Korea, Peru, and China compete for top market share\") + scale_x_continuous(breaks = c(1960, 1970, 1980, 1990, 2000, 2010))\r\n\r\n\r\n\r\nggsave(\"top_mollusk_production.jpg\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2021-10-05-registered-nurses/",
    "title": "Registered Nurses in the United States and Territories",
    "description": "Understanding wages for Registered Nurses.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nResearch Question(s)\r\nLoading Data\r\nInitial EDA\r\nScaling the data by state\r\n10th Percentile\r\n90th Percentile\r\n\r\nMaking heatmaps with dendrograms\r\nPivoting the data to be wider\r\nHeatmap with No scaling\r\nScaling by state\r\n\r\nConclusions\r\n\r\nResearch Question(s)\r\nWhich states have the highest overall wages for registered nurses? When did this happen?\r\nHave wages increased overall for registered nurses across all states?\r\nLoading Data\r\nWe’ll use the Tidy Tuesday code to directly load the data from the GitHub repository. We’ll also pass it into janitor::clean_names() to standardize the column names. (Life is too short to have to worry about whitespace and capitalization.)\r\n\r\n\r\nnurses <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv') %>% janitor::clean_names()\r\n\r\n\r\n\r\nInitial EDA\r\nWe can see there are 22 columns overall. 21 of these are numeric.\r\n\r\n\r\nskimr::skim(nurses)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nnurses\r\nNumber of rows\r\n1242\r\nNumber of columns\r\n22\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nnumeric\r\n21\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nstate\r\n0\r\n1\r\n4\r\n20\r\n0\r\n54\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nyear\r\n0\r\n1.00\r\n2009.00\r\n6.64\r\n1998.00\r\n2.00300e+03\r\n2009.00\r\n2015.00\r\n2020.00\r\n▇▆▇▆▇\r\ntotal_employed_rn\r\n5\r\n1.00\r\n47703.88\r\n50241.05\r\n240.00\r\n1.22100e+04\r\n31160.00\r\n60230.00\r\n307060.00\r\n▇▂▁▁▁\r\nemployed_standard_error_percent\r\n5\r\n1.00\r\n4.36\r\n3.04\r\n0.70\r\n2.50000e+00\r\n3.50\r\n5.10\r\n26.10\r\n▇▂▁▁▁\r\nhourly_wage_avg\r\n6\r\n1.00\r\n28.48\r\n6.65\r\n9.23\r\n2.37000e+01\r\n28.25\r\n32.39\r\n57.96\r\n▁▇▆▁▁\r\nhourly_wage_median\r\n6\r\n1.00\r\n27.86\r\n6.72\r\n8.64\r\n2.30800e+01\r\n27.58\r\n31.72\r\n56.93\r\n▁▇▇▁▁\r\nannual_salary_avg\r\n6\r\n1.00\r\n59248.30\r\n13829.14\r\n19190.00\r\n4.93000e+04\r\n58750.00\r\n67377.50\r\n120560.00\r\n▁▇▆▁▁\r\nannual_salary_median\r\n6\r\n1.00\r\n57957.92\r\n13978.95\r\n17970.00\r\n4.79950e+04\r\n57375.00\r\n65987.50\r\n118410.00\r\n▁▇▇▁▁\r\nwage_salary_standard_error_percent\r\n6\r\n1.00\r\n1.27\r\n0.70\r\n0.40\r\n9.00000e-01\r\n1.10\r\n1.42\r\n7.50\r\n▇▁▁▁▁\r\nhourly_10th_percentile\r\n6\r\n1.00\r\n20.23\r\n4.66\r\n6.38\r\n1.68100e+01\r\n20.04\r\n23.54\r\n36.62\r\n▁▆▇▃▁\r\nhourly_25th_percentile\r\n6\r\n1.00\r\n23.54\r\n5.51\r\n7.33\r\n1.94700e+01\r\n23.24\r\n27.01\r\n45.18\r\n▁▇▇▂▁\r\nhourly_75th_percentile\r\n6\r\n1.00\r\n32.92\r\n8.07\r\n10.04\r\n2.72100e+01\r\n32.61\r\n37.33\r\n71.07\r\n▁▇▅▁▁\r\nhourly_90th_percentile\r\n6\r\n1.00\r\n38.16\r\n9.23\r\n12.33\r\n3.25100e+01\r\n37.50\r\n43.41\r\n83.35\r\n▁▇▅▁▁\r\nannual_10th_percentile\r\n6\r\n1.00\r\n42087.70\r\n9694.20\r\n13260.00\r\n3.49575e+04\r\n41670.00\r\n48955.00\r\n76180.00\r\n▁▆▇▃▁\r\nannual_25th_percentile\r\n6\r\n1.00\r\n48968.81\r\n11469.49\r\n15260.00\r\n4.04875e+04\r\n48335.00\r\n56195.00\r\n93970.00\r\n▁▇▇▂▁\r\nannual_75th_percentile\r\n6\r\n1.00\r\n68464.53\r\n16777.63\r\n20890.00\r\n5.65975e+04\r\n67835.00\r\n77637.50\r\n147830.00\r\n▁▇▅▁▁\r\nannual_90th_percentile\r\n6\r\n1.00\r\n79367.01\r\n19201.21\r\n25650.00\r\n6.76200e+04\r\n78015.00\r\n90290.00\r\n173370.00\r\n▁▇▅▁▁\r\nlocation_quotient\r\n649\r\n0.48\r\n1.01\r\n0.19\r\n0.32\r\n9.00000e-01\r\n1.01\r\n1.13\r\n1.50\r\n▁▁▇▇▁\r\ntotal_employed_national_aggregate\r\n4\r\n1.00\r\n134075563.81\r\n6133532.52\r\n124143490.00\r\n1.29059e+08\r\n131713800.00\r\n138885360.00\r\n147838700.00\r\n▅▇▅▃▃\r\ntotal_employed_healthcare_national_aggregate\r\n4\r\n1.00\r\n7268640.12\r\n943177.74\r\n5854360.00\r\n6.22654e+06\r\n7250140.00\r\n8076300.00\r\n8727310.00\r\n▇▃▅▅▆\r\ntotal_employed_healthcare_state_aggregate\r\n2\r\n1.00\r\n134743.23\r\n143540.40\r\n110.00\r\n3.34475e+04\r\n87435.00\r\n175292.50\r\n844930.00\r\n▇▂▁▁▁\r\nyearly_total_employed_state_aggregate\r\n0\r\n1.00\r\n2387208.60\r\n2774288.09\r\n110.00\r\n5.96520e+05\r\n1557110.00\r\n2888682.50\r\n17382400.00\r\n▇▂▁▁▁\r\n\r\n\r\n\r\nhead(nurses)\r\n\r\n\r\n# A tibble: 6 × 22\r\n  state       year total_employed_rn employed_standar… hourly_wage_avg\r\n  <chr>      <dbl>             <dbl>             <dbl>           <dbl>\r\n1 Alabama     2020             48850               2.9            29.0\r\n2 Alaska      2020              6240              13              45.8\r\n3 Arizona     2020             55520               3.7            38.6\r\n4 Arkansas    2020             25300               4.2            30.6\r\n5 California  2020            307060               2              58.0\r\n6 Colorado    2020             52330               2.8            37.4\r\n# … with 17 more variables: hourly_wage_median <dbl>,\r\n#   annual_salary_avg <dbl>, annual_salary_median <dbl>,\r\n#   wage_salary_standard_error_percent <dbl>,\r\n#   hourly_10th_percentile <dbl>, hourly_25th_percentile <dbl>,\r\n#   hourly_75th_percentile <dbl>, hourly_90th_percentile <dbl>,\r\n#   annual_10th_percentile <dbl>, annual_25th_percentile <dbl>,\r\n#   annual_75th_percentile <dbl>, annual_90th_percentile <dbl>, …\r\n\r\nLooking at how years are divided.\r\n\r\n\r\nnurses %>%\r\n  count(year)\r\n\r\n\r\n# A tibble: 23 × 2\r\n    year     n\r\n   <dbl> <int>\r\n 1  1998    54\r\n 2  1999    54\r\n 3  2000    54\r\n 4  2001    54\r\n 5  2002    54\r\n 6  2003    54\r\n 7  2004    54\r\n 8  2005    54\r\n 9  2006    54\r\n10  2007    54\r\n# … with 13 more rows\r\n\r\nHmmm. 54 entries per year. This includes: D.C., Virgin Islands, Puerto Rico, and Guam in addition to the 50 states.\r\n\r\n\r\nnurses %>%\r\n  count(state)\r\n\r\n\r\n# A tibble: 54 × 2\r\n   state                    n\r\n   <chr>                <int>\r\n 1 Alabama                 23\r\n 2 Alaska                  23\r\n 3 Arizona                 23\r\n 4 Arkansas                23\r\n 5 California              23\r\n 6 Colorado                23\r\n 7 Connecticut             23\r\n 8 Delaware                23\r\n 9 District of Columbia    23\r\n10 Florida                 23\r\n# … with 44 more rows\r\n\r\nThe mean total number of nurses overall states shows an upward trend, except for a blip in 2012-2013.\r\n\r\n\r\nnurses %>%\r\n  group_by(year) %>%\r\n  summarize(mean_employed_rn = mean(total_employed_rn, na.rm=TRUE)) %>%\r\n  ggplot() +\r\n  aes(x=year, y=mean_employed_rn) %>%\r\n  geom_line()\r\n\r\n\r\n\r\n\r\nLet’s visualize whether hourly wages are increasing or decreasing across the dataset by making a heatmap. On the x-axis, we will visualize year, and we will visualize by state on our y-axis. We’re going to map the fill value to hourly_wage_median:\r\n\r\n\r\nnurses %>%\r\n  mutate(state=forcats::fct_rev(state)) %>%\r\n  ggplot() +\r\n  aes(x=year, y=state, fill=hourly_wage_median) +\r\n  geom_tile()\r\n\r\n\r\n\r\n\r\nScaling the data by state\r\nLooking for trends in the nurses data, let’s try and scale each income so we can emphasize whether there were increases or decreases within each state. We’re just looking for trends here and whether the slope of these trends is the same for each state.\r\nNote that by scaling within a state (transforming each value to a z-score), we are losing information, but we can see whether wages are steadily increasing for each of the states/territories.\r\nIn general, with some exceptions (Guam and Virgin Islands), most registered nurses saw an increase in median hourly wages from 1998 to 2020.\r\n\r\n\r\nnurses %>%\r\n  mutate(state=forcats::fct_rev(state)) %>%\r\n  group_by(state) %>%\r\n  mutate(scaled_income = scale(hourly_wage_median)) %>%\r\n  ggplot() +\r\n  aes(x=year, y=state, fill=scaled_income) +\r\n  geom_tile(color=\"grey10\") +\r\n  scale_fill_distiller() +\r\n  bplots::theme_avenir()\r\n\r\n\r\n\r\n\r\nSince we looked at median hourly income, the question is whether these trends are the same or different for the 10th and 90th percentiles of registered nurses.\r\n10th Percentile\r\n\r\n\r\nnurses %>%\r\n  mutate(state=forcats::fct_rev(state)) %>%\r\n  group_by(state) %>%\r\n  mutate(scaled_income = scale(hourly_10th_percentile)) %>%\r\n  ggplot() +\r\n  aes(x=year, y=state, fill=scaled_income) +\r\n  geom_tile(color=\"grey10\") +\r\n  scale_fill_distiller() +\r\n  bplots::theme_avenir() +\r\n  theme(axis.text.x=element_text(angle=90))\r\n\r\n\r\n\r\n\r\n90th Percentile\r\nFor the most part, if you are in the 90th percentile of hourly wages, you have seen a leveling off of income after about 2008. After 2008, the 90th income seems pretty static and unchanging.\r\n\r\n\r\nnurses %>%\r\n  mutate(state=forcats::fct_rev(state)) %>%\r\n  group_by(state) %>%\r\n  mutate(scaled_income = scale(hourly_90th_percentile)) %>%\r\n  ggplot() +\r\n  aes(x=year, y=state, fill=scaled_income) +\r\n  geom_tile(color=\"grey10\") +\r\n  scale_fill_distiller() +\r\n  bplots::theme_avenir() +\r\n  ggtitle(\"90 percentile RNs have slower increases in income than the 10%\")\r\n\r\n\r\n\r\n\r\nMaking heatmaps with dendrograms\r\nPivoting the data to be wider\r\nOne question we might ask are whether there are groupings by states in terms of the wage increases.\r\nWe can do this by pivoting the data and using the {heatmaply} package to make a matrix input suitable for heatmaply::heatmaply().\r\nHere, we take hourly_wage_median and use it in the values of our matrix. Our rows correspond to state and our columns correspond to year.\r\n\r\n\r\nnurse_median_frame <- nurses %>%\r\n  select(state, year, hourly_wage_median) %>%\r\n  arrange(year) %>%\r\n  tidyr::pivot_wider(names_from = year, values_from = hourly_wage_median) \r\n\r\nnurse_median_matrix <- nurse_median_frame[,-1]\r\nrownames(nurse_median_matrix) <- nurse_median_frame[[\"state\"]]\r\nnurse_median_matrix <- as.matrix(nurse_median_matrix)\r\n\r\nhead(nurse_median_matrix)\r\n\r\n\r\n            1998  1999  2000  2001  2002  2003  2004  2005  2006\r\nAlabama    17.63 18.09 19.60 19.99 20.60 20.81 21.23 22.43 23.52\r\nAlaska     22.37 23.02 24.90 26.13 26.45 26.47 28.69 28.54 30.41\r\nArizona    19.37 20.26 21.97 22.23 23.35 23.88 25.12 26.90 28.06\r\nArkansas   16.66 17.18 18.02 18.44 19.20 19.98 21.17 22.63 23.62\r\nCalifornia 23.95 25.12 26.50 27.36 28.38 29.47 31.61 33.15 35.23\r\nColorado   19.79 20.47 21.77 22.56 23.17 23.88 25.60 26.91 28.15\r\n            2007  2008  2009  2010  2011  2012  2013  2014  2015\r\nAlabama    24.92 25.80 26.48 26.44 26.41 26.02 26.20 26.39 26.70\r\nAlaska     33.48 34.42 35.33 37.39 38.67 38.73 40.08 41.12 42.37\r\nArizona    29.17 30.59 31.78 33.11 34.42 34.24 34.14 34.00 34.38\r\nArkansas   24.17 24.78 25.10 25.28 25.90 26.16 26.56 26.72 26.76\r\nCalifornia 36.77 38.93 39.86 41.03 42.51 43.88 45.34 46.38 48.27\r\nColorado   29.69 30.76 31.74 31.81 32.35 32.22 32.73 32.83 32.95\r\n            2016  2017  2018  2019  2020\r\nAlabama    26.68 27.20 27.85 28.27 28.19\r\nAlaska     41.01 41.45 42.14 43.54 45.23\r\nArizona    34.94 35.70 36.43 36.93 37.98\r\nArkansas   27.26 27.68 28.68 29.01 29.97\r\nCalifornia 48.30 48.43 50.20 53.18 56.93\r\nColorado   33.05 34.27 35.03 36.10 36.78\r\n\r\nHeatmap with No scaling\r\nWe can now ask questions about the actual income values. We make heatmaply only look at computing a dendrogram for the rows (states) to look for clustering patterns.\r\nNote we have to set our scale argument to none here.\r\n\r\n\r\nheatmaply(nurse_median_matrix, dendrogram = \"row\", \r\n          Colv = c(1:23), scale=\"none\",\r\n          main = \"Oregon, California, and Hawaii have the highest median wage from 2017-2020\")\r\n\r\n\r\n\r\n\r\nScaling by state\r\nIf we are interested in relative (scaled) values, the dendrogram is a little less interesting. Overall you can see that all states showed an increase in hourly median wage over the years.\r\n\r\n\r\nheatmaply(nurse_median_matrix, dendrogram = \"row\", \r\n          Colv = c(1:23), scale=\"row\", \r\n          main=\"Upward trends overall in terms of hourly median wage\")\r\n\r\n\r\n\r\n\r\nConclusions\r\nThis was a nice dataset to get back into Tidy Tuesday.\r\nMedian wages have increased across all states for Registered Nurses.\r\nHawaii, Oregon, and California have the highest overall wages for Registered Nurses\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2021-09-13-how-to-network-in-a-non-slimy-way/",
    "title": "How to Network in a Non-slimy Way",
    "description": "Use your curiosity to connect with others in a meaningful way.",
    "author": [],
    "date": "2021-09-13",
    "categories": [],
    "contents": "\r\nEveryone talks about the importance of networking, but there is still a negative perception of networking. In some ways, there is this negative perception that it is inauthentic, slimy, and self-serving.\r\nHere’s the thing. Networking is not about selling yourself. It’s about finding real and meaningful connections to people. And that comes from being curious about other people, not from selling yourself.\r\nAfter a long time of networking the wrong way, I think I’ve found a way of networking that works for me. There are some major ideas to this:\r\nOnly network as your authentic self, don’t try to impress other people.\r\nBring your curiosity and do informational interviews.\r\nParticipate in activities that allow for natural networking opportunities.\r\nPractice with people you are comfortable with. Then practice again.\r\nModerate your expectations from each connection. Don’t put all of your networking ideas in one basket.\r\nPromote the work of others.\r\n1. Networking as Your Authentic Self\r\nBe Yourself. This can be really hard, especially if you’re uncomfortable in large social situations. You have to find ways to make yourself comfortable.\r\nI will be honest and say that I can be tremendously awkward. But I’ve learned to let my curiosity show, and that seems to be what works for me.\r\nDon’t come in with the intention to blow away or impress someone with your accomplishments. Another pitfall of networking is being too eager to impress someone. This can result in a lot of discomfort on both sides.\r\nThe best thing about informational interviewing is that it helps you find the deep connections with someone else. If you ask questions about these deep connections, you’ll start forging bonds. Remember, play the long game.\r\nWhen you meet someone, ask yourself: how could we work together? How could we help each other? This gets you into the right frame of mind. For one, you aren’t intimidated by the accomplishments of someone you meet. On the other hand, you aren’t being condescending to them. Remember, curiosity is a great way to forge connections.\r\nIf you can help someone with a five minute favor, that is a low key way to connect.\r\n2. Bring Your Curiosity\r\nLet curiosity be your guide. Ask authentic questions. I was taught early on that informational interviewing was the most important way to network. What is an informational interview? I define it as being curious about someone and asking good questions to get the conversation going.\r\nActive listening and asking questions are some of the fastest ways to connect with people. It’s true, but you need to find a style that works for you.\r\nYou have to find yourself in a mindset where you are curious about the other person. And that mindset is where you can start asking interesting questions that show that you’re really curious about them and their passions. Curiosity is the great democratizer and leveler.\r\nIf someone asks an interesting question during a session, that’s a good person to talk with afterwards. You have something to talk about. Just be careful of those people who seem to be intentionally antagonistic; you want someone who is also curious.\r\nDon’t be so quick to swipe left. Give people a chance to share their story. It is true that most people are not immediately relevant to you and your chosen field. But these people can be an incredible asset to you. Again, let curiosity be your guide. This is your opportunity to learn from them, and the gratitude that arises from learning something new can be a powerful motivator.\r\nYou never know. Most of the literature of successful networkers point out that it is usually our acquaintances to which we have weak ties that helps us find new work.\r\n3. Participate in Activities\r\nDo activities together. I have mentioned before that organizing and actively participating in conferences has been one way that I’ve met a ton of people. If there are volunteer opportunities or hackathons, these are low key ways of doing activities together. Especially when you’re working on things together in small groups, the opportunities to be curious about each other will come up.\r\nOne of the most fruitful activities I participated in was the rOpenSci unconference in 2018. I met a ton of people I am still really good friends with. I also participated in the 2019 Symposium on Statistics and Data Science. Through moderating a session on education, and a speed mentoring session, I met a ton of like-minded people. I also gained a reputation as a booster - I was as supportive as I could be of everyone in that session. CSVconf and RStudioConf were also great opportunities to meet people.\r\nThere are usually student volunteer opportunities at nearly every event, which let you not only meet people, but participate in events for free or for a reduced discount. Also, there are great groups such as R-Ladies and PyLadies which are great places to meet like-minded people.\r\n4. Practice, Practice\r\nAs an introvert, one of your strengths is that you can come prepared. Good informational interviewing takes time to learn; you need to practice at it, and especially with people you feel comfortable with. That way, if you slip up, it won’t feel as catastrophic. Being able to be gentle with yourself and laugh at your mistakes will really help you here.\r\nYou’ll begin to identify, through active listening, the deep connection points. It’s easier to learn this by practicing with someone you know.\r\nDon’t start with the cold informational interview, where you ask your neighbor about something. This is actually the highest level of difficulty, where you know nothing about someone.\r\n5. Moderate Your Expectations\r\nDon’t put all your networking eggs in one basket. One thing I used to do was identify one or two people and put all of my networking eggs in one basket, especially if they were well known. If I couldn’t meet that person, then I thought I was wasting my time. Again, everyone is interesting, so adjust your expectations.\r\nNetworking can be exhausting. Don’t push yourself too hard. Set some hard limits when networking. Don’t expect to be able to talk with everyone. Maybe talking to three people at a conference is all you can do right now. That’s fine. With practice, you’ll be able to do more.\r\nRemember, almost everyone at conferences is in the same boat as you. Be gentle on yourself, and you’ll figure it out.\r\n6. Promoting the work of others\r\nFinally, one low-key way to make a connection with someone is to promote their work in an honest way.\r\nI love Twitter for this, because you can quote retweet other people’s work and provide an honest assessment of people’s work you are passionate about. Oftentimes, just a couple of comment tweets is enough to make a connection.\r\nConclusion: You can do it\r\nIn summary, networking is a skill that you have to tailor to your own personal style. Even as an introvert, there are a lot of opportunities to connect with others in a way that feels natural to you. Bringing your most authentic and curious self to the table will help you find those ways to connect.\r\nResources\r\nMuch of what I relate above comes from hard-won experience. But the book that helped me start was Give and Take, which gave me ideas about how to meet people using my giving nature. Highly recommended.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/tidyverse_functions/",
    "title": "Underrated Tidyverse Functions",
    "description": "Learn about our assignment to teach the `tidyverse` to each other.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2020-12-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nThe Assignment\r\nSome of my favorite suggestions\r\nWow! Let’s Grab All the Tweets and Replies\r\nFunctions by frequency\r\nCleaned Tweets and Threads\r\nSource Code and Data\r\nThank You\r\n\r\n\r\nThe Assignment\r\nI’m teaching an R Programming course next term. Jessica Minnier and I are developing the Ready for R Materials into a longer and more involved course.\r\nI think one of the most important things is to teach people how to self-learn. As learning to program is a lifelong learning activity, it’s critically important to give them these meta-learning skills. So that’s the motivation behind the Tidyverse function of the Week assignment.\r\nI asked on Twitter:\r\n\r\n\r\nHi Everyone. I'm teaching an #rstats course next quarter. One assignment is to have each student write about a #tidyverse function. What it's for and an example.What are some less known #tidyverse functions that do a job you find useful?\r\n\r\n— Ted Laderas, PhD 🏳️‍🌈 (@tladeras) November 30, 2020\r\n\r\nSome of my favorite suggestions\r\nHere are some of the highlights from the thread.\r\nI loved all of these. Danielle Quinn wins the MVP award for naming so many useful functions:\r\n\r\n\r\ndplyr::uncount()tidyr::complete()tidyr::fill() / replace_na()stringr::str_detect() / str_which()lubridate::ymd_hms() and related functionsggplot2::labs() - so simple, yet under appreciated!\r\n\r\n— Danielle Quinn (she/her) (@daniellequinn88) December 1, 2020\r\n\r\nfill() was highly suggested:\r\n\r\n\r\ntidyr::fill() - extremely useful when creating a usable dataset out of a spreadsheet originally built for data entry, in which redundant informations are only reported once at the beginning of the group they refer to, rather than in every row as needed for the analysis.\r\n\r\n— Luca Foppoli (@foppoli_luca) December 1, 2020\r\n\r\nMany people suggested the window functions, including lead() and lag() and the cumulative functions:\r\n\r\n\r\nCheck out the dplyr window functions, cummin, cummax, cumany and cumall. They don't seen useful at first but they can solve really tricky aggregation problems. https://t.co/aDpXqSB2Vx\r\n\r\n— Robert Kubinec (@rmkubinec) December 1, 2020\r\n\r\nAlison Hill suggested problems(), which helps you diagnose why your data isn’t loading:\r\n\r\n\r\nOoh problems is a good function for importing rx https://t.co/P4ZR57PgOG\r\n\r\n— Alison Presmanes Hill (@apreshill) December 1, 2020\r\n\r\nI think that deframe() and enframe() are really exciting, since I do this operation all the time:\r\n\r\n\r\ntibble::deframe(), tibble::deframe()coercing a two-column df to named vector, which I prefer immensely to names(df) <- vec_of_names\r\n\r\n— E. David Aja (@PeeltothePithy) December 1, 2020\r\n\r\nunite(), separate() and separate_rows() also had their own contingent:\r\n\r\n\r\nI find myself using tidyr::unite() a lot to clean messy data - particularly useful for making unique and informative ID's for each row. coalesce() and fill() are also little known gems! :)\r\n\r\n— Guy Sutton🐝🌾🇿🇦🇿🇼 (@Guy_F_Sutton) December 1, 2020\r\n\r\nWow! Let’s Grab All the Tweets and Replies\r\nI was bowled over by all of the replies. This was an unexpectedly really fun thread, and lots of recommendations from others.\r\nI thought I would try and summarize everyone’s suggestions and compile a list of recommended functions. I used this script with some modifications to pull all the replies to my tweet. In particular, I had to request for extended tweet mode, and I extracted a few more fields from the returned JSON.\r\nThis wrote the tweet information into a CSV file.\r\nThen I started parsing the data. I wrote a couple of functions, remove_users_from_text(), which removes the users from a tweet (by looking for words that begin with @) and get_funcs(), which uses a relatively simple regular expression to try to return the function (it looks for paired parentheses () or an underscore “-” to extract the functions). It actually works pretty well, and grabs most of the functions.\r\nThen I use separate_rows() to split the multiple functions into their separate rows. This makes it easier to tally all the functions.\r\n\r\n\r\nremove_users_from_text <- function(col){\r\n  str_replace_all(col, \"\\\\@\\\\w*\", \"\")\r\n  \r\n}\r\n\r\nget_funcs <- function(col){\r\n  out <- str_extract_all(col, \"\\\\w*\\\\(\\\\)|\\\\w*_\\\\w*\")\r\n  paste(out[[1]], collapse=\", \")  \r\n}\r\n\r\nparsed_tweets <- tweets %>%\r\n  rowwise() %>%\r\n  mutate(text = remove_users_from_text(text)) %>%\r\n  mutate(funcs = get_funcs(text)) %>%\r\n  ungroup() %>%\r\n  separate_rows(funcs, sep=\", \") %>%\r\n  select(date, user, funcs, text, reply, parent_thread) %>%\r\n  distinct()\r\n\r\nwrite_csv(parsed_tweets, file = \"cleaned_tweets_incomplete.csv\")\r\n\r\nrmarkdown::paged_table(parsed_tweets[1:10,-c(5:6)])\r\n\r\n\r\n\r\n\r\n\r\nAt this point, I realized that I just needed to hand annotate the rest of the tweets, rather than wasting my time trying to parse the rest of the cases. So I pulled everything into Excel and just annotated the ones which I couldn’t pull from.\r\nFunctions by frequency\r\nHere are the function suggestions by frequency. Unsurprisingly, case_when() (which I cover in the main course), has the most number of suggestions, because it’s so useful. tidyr::pivot_wider() and tidyr::pivot_longer() are also covered in the course.\r\nThere are some others which were new to me, and a bit of a surprise, such as coalesce(), fill().\r\n\r\n\r\ncleaned_tweets <- read_csv(\"cleaned_tweets.csv\") %>% select(-parent_thread) %>%\r\n  mutate(user = paste0(\"[\",user,\"](\",reply,\")\")) %>%\r\n  select(-reply)\r\n\r\nfunctions_by_freq <- cleaned_tweets %>%\r\n  janitor::tabyl(funcs) %>%\r\n  filter(!is.na(funcs)) %>%\r\n  arrange(desc(n)) \r\n\r\nwrite_csv(functions_by_freq, \"functions_by_frequency.csv\")\r\n\r\nfunctions_by_freq %>%\r\n  rmarkdown::paged_table()\r\n\r\n\r\n\r\n\r\n\r\nCleaned Tweets and Threads\r\nHere’s all of the tweets from this thread (naysayers included). They are in somewhat order (longer threads are grouped).\r\nHere’s a link to the cleaned CSV file\r\n\r\n\r\nrmarkdown::paged_table(cleaned_tweets)\r\n\r\n\r\n\r\n\r\n\r\nSource Code and Data\r\nFeel free to use and modify.\r\nRMarkdown file used to generate this post\r\nPython Twitter Scraper (by Giovanni Mellini) - I used this because there wasn’t a ready made recipe in rtweet to extract replies - you have to use recursion to extract all of the thread replies that belong to a tweet, and this was easily modifiable.\r\nCleaned Tweets File (CSV)\r\nThank You\r\nThis post is my thank you for everyone who contributed to this thread. Thank you!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "description": "Getting Shiny, LearnR, and Mybinder.org to play together nicely.",
    "author": [],
    "date": "2020-09-15",
    "categories": [],
    "contents": "\r\nThe learnr package is a wonderful way to package your tutorials. Anyone can download a learnr package of tutorials, and use learnr::run_tutorial() to run them on their own personal system.\r\nSince learnr is based on shiny, these tutorials can also be published to a Shiny server such as shinyapps.io. However, one drawback to this is that the more popular the tutorial, the more access time you may be on the hook for, which can rapidly become expensive.\r\nEnter mybinder.org, which is a way of running reproducible analyses and tutorials. In short, you give a mybinder.org server some information about the software environment needed to run your code, and you can run code based on your repo. Mybinder.org servers are a little limited, (1 Gb RAM), but this is more than capable of running most learnr tutorials.\r\nRecently, they added Shiny as a deployable format, which means you can run shiny apps, including learnr tutorial packages off their servers.\r\nI’m starting a learnr tutorial package called tidyowl and I decided to share what I’ve learned.\r\nWhat’s the problem?\r\nHowever, there are some differences between the file structure of a learnr package and the expected file structure of a shiny-ready mybinder.org repository.\r\nCan we get both setups to work at the same time? Yes, we can.\r\nWhy would we want to run learnr tutorials off mybinder.org?\r\nThe short answer is making your material accessible to as many people as possible. learnr tutorials can be run from a phone or tablet, and running your tutorial for a lot of people doesn’t cost you any bandwidth or usage costs, as you’re using the same infrastructure that mybinder.org provides.\r\nQuick Review of learnr package structure\r\nFor a package, learnr tutorials are stored in the following folder:\r\ninst/tutorials/TUTORIAL_NAME\r\nSo, if you had a tutorial named learning_shiny it would live in\r\ninst/tutorials/learning_shiny/\r\nand the .Rmd file containing the tutorial should be named learning_shiny.Rmd as well.\r\nThe problem I encountered is that mybinder.org expects your tutorial to exist as a folder in the root of the repo. In other words, it needs to see\r\nlearning_shiny/ in the root of the repository to run.\r\nWe can fix this by adding a file called postBuild that gives instructions to run after the software environment is built. We’ll use it to copy the tutorials into the root folder.\r\nMaking your tutorial mybinder.org ready\r\nIn short, you’ll need 3 files to make your learnr tutorial mybinder.org ready: a runtime.txt, a install.R, and a postBuild file in order to make your learnr package compatible with mybinder.org. Let’s go through the steps:\r\nStep 1. Specify a runtime.txt file in your root folder. You’ll need a file called runtime.txt that contains a single line:\r\nr-3.6-2020-08-01\r\nThis gives mybinder.org the signal that the Docker image needs to have R installed. You can see that I specified a version (3.6) and a snapshot date (2020-08-01). These should be a valid version and date for the snapshot - check the MRAN pages for more info: https://mran.microsoft.com/\r\nNote: When R 4.0 and greater is available in MRAN, you should move to it. It includes RStudio Package Manager, which installs the binary images rather than installing from source code which speeds up building the Docker images by quite a bit.\r\nAnother Note: I tried to get this to work with a Dockerfile using the rocker/binder images, but I couldn’t get this image to work. If anyone has gotten this working, I’d appreciate you sharing how you did it.\r\nStep 2. Specify package dependencies using install.R in your root folder - in this file, you’ll need to specify all the packages your tutorial is dependent on using install.packages() commands. Here’s the contents of my install.R file:\r\ninstall.packages(\"learnr\")\r\ninstall.packages(\"here\")\r\ninstall.packages(\"tidyverse\")\r\nNote: getting the dependencies right in package building can be major headaches to getting your binder container to work. You may have to specify some system dependencies in your apt.txt file for certain packages. This information is available here: https://github.com/rstudio/r-system-requirements\r\nStep 3. Specify moving the tutorials in inst/tutorials/ to the repository root folder using the postBuild folder. These commands are run after the container is built and will make the tutorials accessible via Binder.\r\nFor example, for the tidyowl package, I have these mv commands in my postBuild:\r\nmv inst/tutorials/learning_tidyselect/ .\r\nmv inst/tutorials/learning_rowwise/ .\r\nYou’ll need a mv line for each tutorial that your package contains.\r\nNote: Using holepunch\r\nI believe you can also use holepunch to make setup a little easier. https://github.com/karthik/holepunch\r\nI haven’t tried it yet, but will update this when I do.\r\nBuild the Docker Image for your tutorial\r\nOkay, almost there! Now we’re going to go to mybinder.org to build your Docker image. This is the software environment that your tutorial will run off of. This image will have shiny-server and RStudio installed on it automatically, which makes debugging your package easier.\r\nWhen you’re ready, go to https://mybinder.org and put in the public location of your repository. Then click the “Launch” button.\r\nNow your container will build. Note that this will take a little while (10+ minutes), especially if you need to install something like tidyverse. Note that this can be one of the hardest steps to get going, especially if you need packages such as sf (see above for a link to system dependencies).\r\n\r\nWhen it’s done building, you’ll be at a Jupyter page. Click “New >> RStudio” to open up your image with RStudio.\r\n\r\nYou should see that your tutorial folders have been moved to the root folders. This is good confirmation that the mv statements of postBuild work. I personally like to have individual data/ folders in each tutorial, as it makes making them a little easier to deploy.\r\nTest out running the tutorial by going to the .Rmd file and running it.\r\n\r\nIf you’ve setup everything right, you should see your learnr tutorial popup. I will say that this is usually the fine tuning step that takes the longest.\r\nNote: there is a GitHub action (https://github.com/jupyterhub/repo2docker-action) to rebuild your Docker image on new commits. I’ll be looking into this in the future.\r\nSpecify the URL for running your tutorial.\r\nEach tutorial in your package will need its own URL to run.\r\nYou’ll add the following to your mybinder.org link:\r\n?urlpath=shiny/learning_tidyselect/\r\nThe urlpath is a signal to mybinder that it will need to run shiny, and you’ll put the name of your tutorial folder instead of learning_tidyselect. Note the trailing slash after learning_tidyselect.\r\nSo, my final URL for the learning_tidyselect tutorial is this (click it and try it out):\r\nhttps://mybinder.org/v2/gh/laderast/tidyowl/master?urlpath=shiny/learning_tidyselect/\r\nYou can now send this link out and nearly anyone in the world can run your learnr tutorial without installing R and not using up precious Shinyapps.io CPU time!\r\nCaveats\r\nShiny apps may suddenly disconnect - have students reload the page if that happens.\r\nImages get deleted off the mybinder.org servers within a week, so it is worth automating your container build to do so every week so that your students don’t have to wait for your container image to rebuild.\r\nAlso, progress is not saved, because the final url is different each time you run it off mybinder.org servers.\r\nAcknowledgements\r\nThanks so much to the mybinder.org team, what they do is beyond awesome.\r\nThanks to Sang Yun Oh, whose repository helped me figure more of these details out. https://github.com/syoh/learnr-tutorial\r\n\r\n\r\n\r\n",
    "preview": "articles/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/binder_repo.jpg",
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2020-9-3_crops/",
    "title": "Tidy Tuesday: Crop Production",
    "description": "Understanding crop production across the world.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2020-09-03",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\nLook at the available datasets\r\n\r\n\r\nlibrary(tidytuesdayR)\r\n#This will open up in the help window\r\ntidytuesdayR::tt_available()\r\n\r\n\r\n\r\nWhat was your dataset?\r\nLoad your dataset in with the function below. The input is the date the dataset was issued. You should be able to get this from the tt_available() function.\r\n\r\n\r\n#incoming data comes in as a list\r\ndatasets <- tidytuesdayR::tt_load(\"2020-09-01\")\r\n\r\n\r\n\r\n    Downloading file 1 of 5: `arable_land_pin.csv`\r\n    Downloading file 2 of 5: `cereal_crop_yield_vs_fertilizer_application.csv`\r\n    Downloading file 3 of 5: `cereal_yields_vs_tractor_inputs_in_agriculture.csv`\r\n    Downloading file 4 of 5: `key_crop_yields.csv`\r\n    Downloading file 5 of 5: `land_use_vs_yield_change_in_cereal_production.csv`\r\n\r\n#show the names of the individual datasets\r\nnames(datasets)\r\n\r\n\r\n[1] \"arable_land_pin\"                               \r\n[2] \"cereal_crop_yield_vs_fertilizer_application\"   \r\n[3] \"cereal_yields_vs_tractor_inputs_in_agriculture\"\r\n[4] \"key_crop_yields\"                               \r\n[5] \"land_use_vs_yield_change_in_cereal_production\" \r\n\r\nKey Crop Yields\r\n\r\n\r\nkey_crop_yields <- datasets$key_crop_yields\r\n\r\n\r\n\r\nVisdat\r\n\r\n\r\nvisdat::vis_dat(key_crop_yields)\r\n\r\n\r\n\r\n\r\nSkimr\r\n\r\n\r\nskimr::skim(key_crop_yields)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nkey_crop_yields\r\nNumber of rows\r\n13075\r\nNumber of columns\r\n14\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n2\r\nnumeric\r\n12\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nEntity\r\n0\r\n1.00\r\n4\r\n39\r\n0\r\n249\r\n0\r\nCode\r\n1919\r\n0.85\r\n3\r\n8\r\n0\r\n214\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1.00\r\n1990.37\r\n16.73\r\n1961.00\r\n1976.00\r\n1991.00\r\n2005.00\r\n2018.00\r\n▇▆▇▇▇\r\nWheat (tonnes per hectare)\r\n4974\r\n0.62\r\n2.43\r\n1.69\r\n0.00\r\n1.23\r\n1.99\r\n3.12\r\n10.67\r\n▇▅▂▁▁\r\nRice (tonnes per hectare)\r\n4604\r\n0.65\r\n3.16\r\n1.85\r\n0.20\r\n1.77\r\n2.74\r\n4.16\r\n10.68\r\n▇▇▃▁▁\r\nMaize (tonnes per hectare)\r\n2301\r\n0.82\r\n3.02\r\n3.13\r\n0.03\r\n1.14\r\n1.83\r\n3.92\r\n36.76\r\n▇▁▁▁▁\r\nSoybeans (tonnes per hectare)\r\n7114\r\n0.46\r\n1.45\r\n0.75\r\n0.00\r\n0.86\r\n1.33\r\n1.90\r\n5.95\r\n▇▇▂▁▁\r\nPotatoes (tonnes per hectare)\r\n3059\r\n0.77\r\n15.40\r\n9.29\r\n0.84\r\n8.64\r\n13.41\r\n20.05\r\n75.30\r\n▇▅▁▁▁\r\nBeans (tonnes per hectare)\r\n5066\r\n0.61\r\n1.09\r\n0.82\r\n0.03\r\n0.59\r\n0.83\r\n1.35\r\n9.18\r\n▇▁▁▁▁\r\nPeas (tonnes per hectare)\r\n6840\r\n0.48\r\n1.48\r\n1.01\r\n0.04\r\n0.72\r\n1.15\r\n1.99\r\n7.16\r\n▇▃▁▁▁\r\nCassava (tonnes per hectare)\r\n5887\r\n0.55\r\n9.34\r\n5.11\r\n1.00\r\n5.55\r\n8.67\r\n11.99\r\n38.58\r\n▇▇▁▁▁\r\nBarley (tonnes per hectare)\r\n6342\r\n0.51\r\n2.23\r\n1.50\r\n0.09\r\n1.05\r\n1.88\r\n3.02\r\n9.15\r\n▇▆▂▁▁\r\nCocoa beans (tonnes per hectare)\r\n8466\r\n0.35\r\n0.39\r\n0.28\r\n0.00\r\n0.24\r\n0.36\r\n0.49\r\n3.43\r\n▇▁▁▁▁\r\nBananas (tonnes per hectare)\r\n4166\r\n0.68\r\n15.20\r\n12.08\r\n0.66\r\n5.94\r\n11.78\r\n20.79\r\n77.59\r\n▇▃▁▁▁\r\n\r\nWhat was your question?\r\nGiven your inital exploration of the data, what was the question you wanted to answer?\r\n\r\nHow have key crop yields changed over time?\r\n\r\nWhat were your findings?\r\nPut your findings and your visualization code here.\r\n\r\n\r\nkey_crop_yields %>%\r\n  tidyr::pivot_longer(cols = contains(\"(tonnes\"), names_to=\"crop\",\r\n                      values_to=\"Yield\") %>%\r\n  ggplot() + aes(x=Year, y= Yield, group=crop, color=crop) + geom_line() + facet_wrap(~Entity)\r\n\r\n\r\n\r\n\r\nLet’s try and estimate whether a country is increasing its yield or decreaing its yield over time. I’ll use lm() to run a linear regression on each entity in the data, and use broom::tidy() to pull out the estimates of the slopes.\r\n\r\n\r\nmodel_results <- key_crop_yields %>%\r\n  tidyr::pivot_longer(cols = contains(\"(tonnes\"), names_to=\"crop\",\r\n                      values_to=\"Yield\") %>%\r\n  mutate(crop=str_replace(crop,\"\\\\(tonnes per hectare\\\\)\", \"\")) %>%\r\n  tidyr::drop_na(Yield) %>%\r\n  nest_by(Entity, crop) %>%\r\n  mutate(num_points = nrow(data)) %>%\r\n  mutate(model=list(lm(Yield ~ Year, data=data))) %>%\r\n  summarize(num_points, broom::tidy(model)) %>%\r\n  filter(term == \"Year\") %>%\r\n  arrange(Entity, desc(estimate))\r\n\r\nmodel_results\r\n\r\n\r\n# A tibble: 1,691 × 8\r\n# Groups:   Entity, crop [1,691]\r\n   Entity crop  num_points term  estimate std.error statistic  p.value\r\n   <chr>  <chr>      <int> <chr>    <dbl>     <dbl>     <dbl>    <dbl>\r\n 1 Afgha… \"Pot…         58 Year   0.102    0.0197        5.15 3.45e- 6\r\n 2 Afgha… \"Ric…         58 Year   0.0238   0.00253       9.40 4.10e-13\r\n 3 Afgha… \"Whe…         58 Year   0.0169   0.00199       8.52 1.10e-11\r\n 4 Afgha… \"Mai…         58 Year   0.00987  0.00272       3.63 6.23e- 4\r\n 5 Afgha… \"Bar…         58 Year   0.00711  0.00198       3.60 6.73e- 4\r\n 6 Africa \"Pot…         58 Year   0.121    0.00630      19.1  3.20e-26\r\n 7 Africa \"Cas…         58 Year   0.0742   0.00305      24.4  1.79e-31\r\n 8 Africa \"Ban…         58 Year   0.0724   0.00653      11.1  9.80e-16\r\n 9 Africa \"Whe…         58 Year   0.0369   0.00123      30.0  3.39e-36\r\n10 Africa \"Soy…         58 Year   0.0172   0.000931     18.4  1.89e-25\r\n# … with 1,681 more rows\r\n\r\nLooking at the United States, there are mostly increases in crop yield.\r\n\r\n\r\nmodel_results %>%\r\n  filter(Entity == \"United States\") \r\n\r\n\r\n# A tibble: 9 × 8\r\n# Groups:   Entity, crop [9]\r\n  Entity  crop  num_points term  estimate std.error statistic  p.value\r\n  <chr>   <chr>      <int> <chr>    <dbl>     <dbl>     <dbl>    <dbl>\r\n1 United… \"Pot…         58 Year   0.497    0.00795      62.6  1.61e-53\r\n2 United… \"Ban…         58 Year   0.215    0.0207       10.4  1.28e-14\r\n3 United… \"Mai…         58 Year   0.121    0.00554      21.8  5.14e-29\r\n4 United… \"Ric…         58 Year   0.0770   0.00239      32.2  7.44e-38\r\n5 United… \"Bar…         58 Year   0.0355   0.00169      21.0  2.93e-28\r\n6 United… \"Soy…         58 Year   0.0295   0.00138      21.5  1.07e-28\r\n7 United… \"Whe…         58 Year   0.0256   0.00129      19.8  5.68e-27\r\n8 United… \"Bea…         58 Year   0.0126   0.000812     15.6  5.11e-22\r\n9 United… \"Pea…         58 Year   0.00902  0.00375       2.40 1.96e- 2\r\n\r\nWe can rank the top producers by crop:\r\n\r\n\r\nranked_by_slope <- model_results %>%\r\n  ungroup() %>%\r\n  group_by(crop) %>%\r\n  summarize(Entity, crop, num_points, estimate=signif(estimate, digits = 3), rank = row_number(desc(estimate))) %>%\r\n  arrange(crop, rank)\r\n\r\nranked_by_slope\r\n\r\n\r\n# A tibble: 1,691 × 5\r\n# Groups:   crop [11]\r\n   crop       Entity          num_points estimate  rank\r\n   <chr>      <chr>                <int>    <dbl> <int>\r\n 1 \"Bananas \" South Korea              6    6.56      1\r\n 2 \"Bananas \" Indonesia               58    1.27      2\r\n 3 \"Bananas \" Peru                    14    1.25      3\r\n 4 \"Bananas \" Syria                   49    1.01      4\r\n 5 \"Bananas \" South Africa            58    0.947     5\r\n 6 \"Bananas \" Turkey                  58    0.839     6\r\n 7 \"Bananas \" Southern Africa         58    0.835     7\r\n 8 \"Bananas \" Cote d'Ivoire           58    0.749     8\r\n 9 \"Bananas \" Guatemala               58    0.723     9\r\n10 \"Bananas \" Morocco                 40    0.654    10\r\n# … with 1,681 more rows\r\n\r\nFinally, let’s do histograms by crop:\r\n\r\n\r\nranked_by_slope %>%\r\n  ggplot() +\r\n  aes(x=estimate, fill=crop) +\r\n  geom_histogram() +\r\n  geom_vline(xintercept = 0, lty =2) +\r\n  labs(title = \"Crop productivity across countries (tonnes/hectare/year)\",\r\n       subtitle = \"Positive values = increase, Negative values = decrease\") +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\") +\r\n  facet_wrap(~crop, scales = \"free\")\r\n\r\n\r\n\r\n\r\nWhat did you learn?\r\nWere there any lessons you learned? Any cool packages you want to talk about?\r\n\r\n\r\n\r\n",
    "preview": "articles/2020-9-3_crops/distill-preview.png",
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "articles/2020-08-11-avatar_last_airbender/",
    "title": "Sentiment analysis of Avatar",
    "description": "Understanding Characters through Avatar Episode Scripts.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2020-08-11",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nLook at the available datasets\r\nLoading the Data\r\nMy Research Question\r\nCharacters by Episode/Chapter\r\nBit Players\r\nCabbage merchant\r\nWho Spoke the Most?\r\nUnderstanding Each Character’s Journey\r\nSentiment Heatmap\r\nZuko has the most interesting journey\r\nAang and Zuko’s Journeys\r\nIroh is so chill and positive\r\n\r\nLook at the available datasets\r\n\r\n\r\nlibrary(tidytuesdayR)\r\n\r\n\r\n\r\nLoading the Data\r\n\r\n\r\n#incoming data comes in as a list\r\ndatasets <- tidytuesdayR::tt_load(\"2020-08-11\")\r\n\r\n\r\n\r\n    Downloading file 1 of 2: `avatar.csv`\r\n    Downloading file 2 of 2: `scene_description.csv`\r\n\r\n#show the names of the individual datasets\r\nnames(datasets)\r\n\r\n\r\n[1] \"avatar\"            \"scene_description\"\r\n\r\n\r\n\r\navatar <- datasets$avatar\r\navatar[1:5,]\r\n\r\n\r\n# A tibble: 5 × 11\r\n     id book  book_num chapter   chapter_num character  full_text     \r\n  <dbl> <chr>    <dbl> <chr>           <dbl> <chr>      <chr>         \r\n1     1 Water        1 The Boy …           1 Katara     \"Water. Earth…\r\n2     2 Water        1 The Boy …           1 Scene Des… \"As the title…\r\n3     3 Water        1 The Boy …           1 Sokka      \"It's not get…\r\n4     4 Water        1 The Boy …           1 Scene Des… \"The shot pan…\r\n5     5 Water        1 The Boy …           1 Katara     \"[Happily sur…\r\n# … with 4 more variables: character_words <chr>, writer <chr>,\r\n#   director <chr>, imdb_rating <dbl>\r\n\r\n\r\n\r\nscenes <- datasets$scene_description\r\nscenes[1:5,]\r\n\r\n\r\n# A tibble: 5 × 2\r\n     id scene_description                                             \r\n  <dbl> <chr>                                                         \r\n1     3 [Close-up of the boy as he grins confidently over his shoulde…\r\n2     5 [Happily surprised.]                                          \r\n3     6 [Close-up of Sokka; whispering.]                              \r\n4     6 [A look of bliss adorns his face. He licks his lips and wiggl…\r\n5     8 [Struggling with the water that passes right in front of her.]\r\n\r\nMy Research Question\r\nDoes the sentiment of each character change over the multiple seasons? That is, does a character become more positive or more negative as their character develops?\r\nI will attempt to summarize the sentiment of each character across each episode.\r\nUsing tidytext to unnest_tokens() - that is, split each line into 1 word per row.\r\n\r\n\r\nlibrary(tidytext)\r\nlibrary(tidyverse)\r\n\r\navatar_words <- avatar %>%\r\n  select(id, book, book_num, chapter, chapter_num, character, character_words) %>%\r\n  filter(character != \"Scene Description\") %>%\r\n  unnest_tokens(word, character_words)\r\n\r\navatar_words[1:10,]\r\n\r\n\r\n# A tibble: 10 × 7\r\n      id book  book_num chapter         chapter_num character word    \r\n   <dbl> <chr>    <dbl> <chr>                 <dbl> <chr>     <chr>   \r\n 1     1 Water        1 The Boy in the…           1 Katara    water   \r\n 2     1 Water        1 The Boy in the…           1 Katara    earth   \r\n 3     1 Water        1 The Boy in the…           1 Katara    fire    \r\n 4     1 Water        1 The Boy in the…           1 Katara    air     \r\n 5     1 Water        1 The Boy in the…           1 Katara    my      \r\n 6     1 Water        1 The Boy in the…           1 Katara    grandmo…\r\n 7     1 Water        1 The Boy in the…           1 Katara    used    \r\n 8     1 Water        1 The Boy in the…           1 Katara    to      \r\n 9     1 Water        1 The Boy in the…           1 Katara    tell    \r\n10     1 Water        1 The Boy in the…           1 Katara    me      \r\n\r\nCharacters by Episode/Chapter\r\n\r\n\r\nepisode_count <- avatar %>%\r\n  count(character, chapter) %>%\r\n  select(character, chapter) %>%\r\n  filter(character != \"Scene Description\") %>%\r\n  distinct() %>%\r\n  count(character) %>%\r\n  arrange(desc(n))\r\n\r\nepisode_count %>% \r\n  DT::datatable()\r\n\r\n\r\n\r\n\r\nBit Players\r\n\r\n\r\nepisode_count %>%\r\n  filter(n == 1) %>%\r\n  arrange(character)\r\n\r\n\r\n# A tibble: 272 × 2\r\n   character         n\r\n   <chr>         <int>\r\n 1 Aang and Zuko     1\r\n 2 Aang:             1\r\n 3 Actor Bumi        1\r\n 4 Actor Iroh        1\r\n 5 Actor Jet         1\r\n 6 Actor Ozai        1\r\n 7 Actor Sokka       1\r\n 8 Actor Toph        1\r\n 9 Actor Zuko        1\r\n10 Actress Azula     1\r\n# … with 262 more rows\r\n\r\nCabbage merchant\r\nThe cabbage merchant appears in 4 episodes, and you can see his path to resignation as Aang and company keep busting up his cabbage kiosk.\r\n\r\n\r\navatar %>%\r\n  filter(character == \"Cabbage merchant\") %>%\r\n  select(chapter, character_words) %>%\r\n  gt::gt()\r\n\r\n\r\n\r\nchapter\r\n      character_words\r\n    The King of Omashu\r\nNo! My cabbages!The King of Omashu\r\nMy cabbages!  You're gonna pay for this!The King of Omashu\r\nOff with their heads! One for each head of cabbage!The King of Omashu\r\nMy cabbages!The Waterbending Scroll\r\nMy cabbages! This place is worse than Omashu!The Serpent's Pass\r\nAhhh! My cabbages!The Tales of Ba Sing Se\r\nMy cabba-  Oh, forget it.\r\n\r\nWho Spoke the Most?\r\nSurprisingly, Sokka has the most lines.\r\n\r\n\r\nline_count <- avatar_words %>% \r\n  count(character) %>%\r\n  arrange(desc(n)) \r\n\r\nline_count[1:20,] %>%\r\n  gt::gt()\r\n\r\n\r\n\r\ncharacter\r\n      n\r\n    Sokka\r\n18293Aang\r\n17821Katara\r\n14961Zuko\r\n8972Toph\r\n5434Iroh\r\n5252Azula\r\n3299Zhao\r\n1607Jet\r\n1604Suki\r\n1221Hakoda\r\n1065Pathik\r\n1030Roku\r\n1015Ozai\r\n1002Hama\r\n955Mai\r\n844Bumi\r\n818Long Feng\r\n757Warden\r\n722Ty Lee\r\n705\r\n\r\nUnderstanding Each Character’s Journey\r\nUsing tidytext, I do a sentiment analysis of each episode (here called a chapter) to determine the overal sentiment for a character.\r\n\r\n\r\nbing <- get_sentiments(\"bing\")\r\n\r\ncharacters <- c(\"Aang\", \"Katara\", \"Zuko\", \"Toph\", \"Iroh\", \"Sokka\", \"Azula\", \"Mai\", \"Ty Lee\")\r\n\r\nsentiment_summary <- avatar_words %>%\r\n  inner_join(bing) %>%\r\n  count(book_num, chapter_num, chapter, character, sentiment) %>%\r\n  filter(character %in% characters) %>%\r\n  arrange(book_num, chapter_num) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n) %>%\r\n  mutate(positive = tidyr::replace_na(positive, 0),\r\n         negative = tidyr::replace_na(negative, 0)) %>%\r\n  mutate(sentiment = positive - negative)\r\n\r\n\r\n\r\n\r\n\r\nindex_chapters <- avatar_words %>%\r\n  select(book_num, chapter_num) %>%\r\n  distinct() %>%\r\n  mutate(index = row_number())\r\n\r\n\r\n\r\n\r\n\r\nsentiment_index <- sentiment_summary %>% \r\n  inner_join(y= index_chapters, by=c(\"book_num\", \"chapter_num\"))\r\n\r\n\r\n\r\n\r\n\r\nout_plot <- ggplot(sentiment_index) +\r\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, episode_number=chapter_num) +\r\n  geom_col(show_legend = FALSE) +\r\n  facet_wrap(~character, ncol=2) +\r\n  labs(title= \"Each Character's Sentiment Journey\", x=\"Episode Number\",\r\n       subtitle = \"mouse over each graph for more information\") + \r\n  geom_vline(xintercept = 21, lty=2) +\r\n  geom_vline(xintercept = 41, lty=2)\r\n\r\nplotly::ggplotly(out_plot)\r\n\r\n\r\n\r\n\r\nSentiment Heatmap\r\n\r\n\r\ns_index <- sentiment_index %>%\r\n  tidyr::complete(chapter_num, character)\r\n\r\nggplot(s_index) +\r\n  aes(x=index, y=character, fill=sentiment) +\r\ngeom_tile() +\r\n  scale_fill_viridis_b(na.value=\"black\") \r\n\r\n\r\n\r\n\r\nZuko has the most interesting journey\r\nZuko has many ups and downs, which may reflect his overall lack of confidence and his tendency for self-loathing.\r\n\r\n\r\nzuko <- sentiment_index %>%\r\n  filter(character==\"Zuko\")\r\n\r\nout_plot <- ggplot(zuko) +\r\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, group=character, episode_number=chapter_num) +\r\n  geom_col(show_legend = FALSE) +\r\n  facet_wrap(~character, ncol=2) +\r\n  annotate(geom=\"text\", x=27, y= -8 , label = \"Zuko Alone\\nA Turning Point\") +\r\n  annotate(geom=\"text\", x=53, y = 11, label = \"Where Zuko\\ntrains Aang\") +\r\n  labs(title= \"Zuko has lots of ups and downs\", x=\"Episode Number\",\r\n       subtitle = \"mouse over for more episode information\") +\r\n  ylim(c(-13, 13)) +\r\n  geom_vline(xintercept = 21, lty=2) +\r\n  geom_vline(xintercept = 41, lty=2)\r\n\r\n\r\nplotly::ggplotly(out_plot)\r\n\r\n\r\n\r\n\r\nAang and Zuko’s Journeys\r\nPlotting the sentiment journey of Zuko and Aang together shows that they often mirror each other, except in the last parts of Book 3.\r\n\r\n\r\nzuko_aang <- sentiment_index %>%\r\n  filter(character %in% c(\"Zuko\", \"Aang\"))\r\n\r\nout_plot <- ggplot(zuko_aang) +\r\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, episode_number=chapter_num) +\r\n  geom_col(show_legend = FALSE, alpha=0.7) +\r\n  labs(title= \"Aang and Zuko's Journeys Often Mirror Each Other\", \r\n       x=\"Episode Number\",\r\n       subtitle = \"mouse over for more episode information\") +\r\n  ylim(c(-13, 13)) +\r\n    geom_vline(xintercept = 21, lty=2) +\r\n  geom_vline(xintercept = 41, lty=2)\r\n\r\n\r\nplotly::ggplotly(out_plot)\r\n\r\n\r\n\r\n\r\nIroh is so chill and positive\r\n\r\n\r\niroh <- sentiment_index %>%\r\n  filter(character==\"Iroh\")\r\n\r\nout_plot <- ggplot(iroh) +\r\n  aes(x=index, y=sentiment, fill=character, episode=chapter, book = book_num, episode_number=chapter_num) +\r\n  geom_col(show_legend = FALSE) +\r\n  labs(title= \"Iroh is just so chill and positive\", x=\"Episode Number\",\r\n       subtitle = \"mouse over for more episode information\") +\r\n  ylim(c(-13, 13)) +\r\n    geom_vline(xintercept = 21, lty=2) +\r\n  geom_vline(xintercept = 41, lty=2)\r\n\r\nplotly::ggplotly(out_plot)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "articles/2020-08-11-avatar_last_airbender/distill-preview.png",
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "articles/2020-07-08_coffee/",
    "title": "Coffee Data Exploration",
    "description": "Understanding coffee production and consumption across the world.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2020-07-08",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nWhat was your dataset?\r\nWhat was your question?\r\nInitial Skim of Data\r\nTotal Number of Samples Per Country\r\nDistribution of total_cup_points versus processing_method\r\nCounts of Country of Origin versus Processing Method\r\nSorted Heatmap of scores by total_cup_points\r\nBi-clustered Heatmap of Scores\r\nProcessing Method: Dry / Natural\r\nProcessing Method: Washed / Wet\r\nMexico: Processing Methods\r\n\r\nLinear model of total_cup_points\r\n\r\nWhat was your dataset?\r\nLoad your dataset in with the function below. The input is the date the dataset was issued. You should be able to get this from the tt_available() function.\r\n\r\n\r\ncoffee <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv')\r\n\r\n\r\n\r\nWhat was your question?\r\nGiven your inital exploration of the data, what was the question you wanted to answer?\r\nDoes processing method affect overall coffee rating?\r\nInitial Skim of Data\r\n\r\n\r\nskimr::skim(coffee)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ncoffee\r\nNumber of rows\r\n1339\r\nNumber of columns\r\n43\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n24\r\nnumeric\r\n19\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nspecies\r\n0\r\n1.00\r\n7\r\n7\r\n0\r\n2\r\n0\r\nowner\r\n7\r\n0.99\r\n3\r\n50\r\n0\r\n315\r\n0\r\ncountry_of_origin\r\n1\r\n1.00\r\n4\r\n28\r\n0\r\n36\r\n0\r\nfarm_name\r\n359\r\n0.73\r\n1\r\n73\r\n0\r\n571\r\n0\r\nlot_number\r\n1063\r\n0.21\r\n1\r\n71\r\n0\r\n227\r\n0\r\nmill\r\n315\r\n0.76\r\n1\r\n77\r\n0\r\n460\r\n0\r\nico_number\r\n151\r\n0.89\r\n1\r\n40\r\n0\r\n847\r\n0\r\ncompany\r\n209\r\n0.84\r\n3\r\n73\r\n0\r\n281\r\n0\r\naltitude\r\n226\r\n0.83\r\n1\r\n41\r\n0\r\n396\r\n0\r\nregion\r\n59\r\n0.96\r\n2\r\n76\r\n0\r\n356\r\n0\r\nproducer\r\n231\r\n0.83\r\n1\r\n100\r\n0\r\n691\r\n0\r\nbag_weight\r\n0\r\n1.00\r\n1\r\n8\r\n0\r\n56\r\n0\r\nin_country_partner\r\n0\r\n1.00\r\n7\r\n85\r\n0\r\n27\r\n0\r\nharvest_year\r\n47\r\n0.96\r\n3\r\n24\r\n0\r\n46\r\n0\r\ngrading_date\r\n0\r\n1.00\r\n13\r\n20\r\n0\r\n567\r\n0\r\nowner_1\r\n7\r\n0.99\r\n3\r\n50\r\n0\r\n319\r\n0\r\nvariety\r\n226\r\n0.83\r\n4\r\n21\r\n0\r\n29\r\n0\r\nprocessing_method\r\n170\r\n0.87\r\n5\r\n25\r\n0\r\n5\r\n0\r\ncolor\r\n218\r\n0.84\r\n4\r\n12\r\n0\r\n4\r\n0\r\nexpiration\r\n0\r\n1.00\r\n13\r\n20\r\n0\r\n566\r\n0\r\ncertification_body\r\n0\r\n1.00\r\n7\r\n85\r\n0\r\n26\r\n0\r\ncertification_address\r\n0\r\n1.00\r\n40\r\n40\r\n0\r\n32\r\n0\r\ncertification_contact\r\n0\r\n1.00\r\n40\r\n40\r\n0\r\n29\r\n0\r\nunit_of_measurement\r\n0\r\n1.00\r\n1\r\n2\r\n0\r\n2\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ntotal_cup_points\r\n0\r\n1.00\r\n82.09\r\n3.50\r\n0\r\n81.08\r\n82.50\r\n83.67\r\n90.58\r\n▁▁▁▁▇\r\nnumber_of_bags\r\n0\r\n1.00\r\n154.18\r\n129.99\r\n0\r\n14.00\r\n175.00\r\n275.00\r\n1062.00\r\n▇▇▁▁▁\r\naroma\r\n0\r\n1.00\r\n7.57\r\n0.38\r\n0\r\n7.42\r\n7.58\r\n7.75\r\n8.75\r\n▁▁▁▁▇\r\nflavor\r\n0\r\n1.00\r\n7.52\r\n0.40\r\n0\r\n7.33\r\n7.58\r\n7.75\r\n8.83\r\n▁▁▁▁▇\r\naftertaste\r\n0\r\n1.00\r\n7.40\r\n0.40\r\n0\r\n7.25\r\n7.42\r\n7.58\r\n8.67\r\n▁▁▁▁▇\r\nacidity\r\n0\r\n1.00\r\n7.54\r\n0.38\r\n0\r\n7.33\r\n7.58\r\n7.75\r\n8.75\r\n▁▁▁▁▇\r\nbody\r\n0\r\n1.00\r\n7.52\r\n0.37\r\n0\r\n7.33\r\n7.50\r\n7.67\r\n8.58\r\n▁▁▁▁▇\r\nbalance\r\n0\r\n1.00\r\n7.52\r\n0.41\r\n0\r\n7.33\r\n7.50\r\n7.75\r\n8.75\r\n▁▁▁▁▇\r\nuniformity\r\n0\r\n1.00\r\n9.83\r\n0.55\r\n0\r\n10.00\r\n10.00\r\n10.00\r\n10.00\r\n▁▁▁▁▇\r\nclean_cup\r\n0\r\n1.00\r\n9.84\r\n0.76\r\n0\r\n10.00\r\n10.00\r\n10.00\r\n10.00\r\n▁▁▁▁▇\r\nsweetness\r\n0\r\n1.00\r\n9.86\r\n0.62\r\n0\r\n10.00\r\n10.00\r\n10.00\r\n10.00\r\n▁▁▁▁▇\r\ncupper_points\r\n0\r\n1.00\r\n7.50\r\n0.47\r\n0\r\n7.25\r\n7.50\r\n7.75\r\n10.00\r\n▁▁▁▇▁\r\nmoisture\r\n0\r\n1.00\r\n0.09\r\n0.05\r\n0\r\n0.09\r\n0.11\r\n0.12\r\n0.28\r\n▃▇▅▁▁\r\ncategory_one_defects\r\n0\r\n1.00\r\n0.48\r\n2.55\r\n0\r\n0.00\r\n0.00\r\n0.00\r\n63.00\r\n▇▁▁▁▁\r\nquakers\r\n1\r\n1.00\r\n0.17\r\n0.83\r\n0\r\n0.00\r\n0.00\r\n0.00\r\n11.00\r\n▇▁▁▁▁\r\ncategory_two_defects\r\n0\r\n1.00\r\n3.56\r\n5.31\r\n0\r\n0.00\r\n2.00\r\n4.00\r\n55.00\r\n▇▁▁▁▁\r\naltitude_low_meters\r\n230\r\n0.83\r\n1750.71\r\n8669.44\r\n1\r\n1100.00\r\n1310.64\r\n1600.00\r\n190164.00\r\n▇▁▁▁▁\r\naltitude_high_meters\r\n230\r\n0.83\r\n1799.35\r\n8668.81\r\n1\r\n1100.00\r\n1350.00\r\n1650.00\r\n190164.00\r\n▇▁▁▁▁\r\naltitude_mean_meters\r\n230\r\n0.83\r\n1775.03\r\n8668.63\r\n1\r\n1100.00\r\n1310.64\r\n1600.00\r\n190164.00\r\n▇▁▁▁▁\r\n\r\nTotal Number of Samples Per Country\r\n\r\n\r\ncoffee %>%\r\n  janitor::tabyl(country_of_origin) %>%\r\n  arrange(desc(n)) %>%\r\n  gt::gt()\r\n\r\n\r\n\r\ncountry_of_origin\r\n      n\r\n      percent\r\n      valid_percent\r\n    Mexico\r\n236\r\n0.176250934\r\n0.1763826607Colombia\r\n183\r\n0.136669156\r\n0.1367713004Guatemala\r\n181\r\n0.135175504\r\n0.1352765321Brazil\r\n132\r\n0.098581031\r\n0.0986547085Taiwan\r\n75\r\n0.056011949\r\n0.0560538117United States (Hawaii)\r\n73\r\n0.054518297\r\n0.0545590433Honduras\r\n53\r\n0.039581777\r\n0.0396113602Costa Rica\r\n51\r\n0.038088125\r\n0.0381165919Ethiopia\r\n44\r\n0.032860344\r\n0.0328849028Tanzania, United Republic Of\r\n40\r\n0.029873040\r\n0.0298953662Uganda\r\n36\r\n0.026885736\r\n0.0269058296Thailand\r\n32\r\n0.023898432\r\n0.0239162930Nicaragua\r\n26\r\n0.019417476\r\n0.0194319880Kenya\r\n25\r\n0.018670650\r\n0.0186846039El Salvador\r\n21\r\n0.015683346\r\n0.0156950673Indonesia\r\n20\r\n0.014936520\r\n0.0149476831China\r\n16\r\n0.011949216\r\n0.0119581465India\r\n14\r\n0.010455564\r\n0.0104633782Malawi\r\n11\r\n0.008215086\r\n0.0082212257Peru\r\n10\r\n0.007468260\r\n0.0074738416United States\r\n10\r\n0.007468260\r\n0.0074738416Myanmar\r\n8\r\n0.005974608\r\n0.0059790732Vietnam\r\n8\r\n0.005974608\r\n0.0059790732Haiti\r\n6\r\n0.004480956\r\n0.0044843049Philippines\r\n5\r\n0.003734130\r\n0.0037369208Panama\r\n4\r\n0.002987304\r\n0.0029895366United States (Puerto Rico)\r\n4\r\n0.002987304\r\n0.0029895366Ecuador\r\n3\r\n0.002240478\r\n0.0022421525Laos\r\n3\r\n0.002240478\r\n0.0022421525Burundi\r\n2\r\n0.001493652\r\n0.0014947683Cote d?Ivoire\r\n1\r\n0.000746826\r\n0.0007473842Japan\r\n1\r\n0.000746826\r\n0.0007473842Mauritius\r\n1\r\n0.000746826\r\n0.0007473842Papua New Guinea\r\n1\r\n0.000746826\r\n0.0007473842Rwanda\r\n1\r\n0.000746826\r\n0.0007473842Zambia\r\n1\r\n0.000746826\r\n0.0007473842NA\r\n1\r\n0.000746826\r\nNA\r\n\r\nDistribution of total_cup_points versus processing_method\r\n\r\n\r\nggplot(coffee) + \r\n  aes(y=total_cup_points, x=processing_method, fill=processing_method) +\r\n  geom_boxplot() +\r\n  theme(axis.text.x = element_text(angle=90, hjust = 1)) +\r\n  coord_flip() \r\n\r\n\r\n\r\n\r\nCounts of Country of Origin versus Processing Method\r\n\r\n\r\ncoffee %>%\r\n  mutate(country_of_origin= fct_rev(country_of_origin)) %>%\r\n  ggplot() +\r\n  aes(y=country_of_origin, x=processing_method, \r\n      color=processing_method) +\r\n  geom_count() +\r\n  theme(axis.text.x = element_text(angle=90))\r\n\r\n\r\n\r\n\r\nHere’s a sortable table of the above table\r\n\r\n\r\nlibrary(reactable)\r\ncoffee %>%\r\n  janitor::tabyl(country_of_origin, processing_method) %>%\r\n  reactable::reactable()\r\n\r\n\r\n\r\n\r\nSorted Heatmap of scores by total_cup_points\r\n\r\n\r\ncoffee %>% mutate(sample_id = rownames(coffee)) %>%\r\n  select(sample_id, country_of_origin, total_cup_points, aroma, flavor, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points)%>%\r\n  pivot_longer(cols = c(aroma, flavor, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points), names_to=\"type\", values_to=\"score\") %>%\r\n  mutate(sample_id = fct_reorder(sample_id, total_cup_points)) %>%\r\n  ggplot() +\r\n  aes(y=sample_id, x=type, fill=score) +\r\n    geom_tile()\r\n\r\n\r\n\r\n\r\nBi-clustered Heatmap of Scores\r\n\r\n\r\nlibrary(heatmaply)\r\n\r\ncoffee %>% mutate(sample_id = rownames(coffee)) %>%\r\n  select(aroma, flavor, acidity, body, balance, uniformity, clean_cup, sweetness, cupper_points) %>% heatmaply()\r\n\r\n\r\n\r\n\r\nProcessing Method: Dry / Natural\r\n\r\n\r\ncoffee %>%\r\n  filter(processing_method == \"Natural / Dry\") %>%\r\n  mutate(country_of_origin = fct_reorder(country_of_origin, total_cup_points, median)) %>%\r\nggplot() + \r\n  aes(y=total_cup_points, x=country_of_origin, fill=country_of_origin) +\r\n  geom_boxplot() +\r\n  theme(axis.text.x = element_text(angle=90), legend.position = \"none\") +\r\n  coord_flip() +\r\n  labs(title=\"Tanzania leads with ratings in Natural/Dry\")\r\n\r\n\r\n\r\n\r\nProcessing Method: Washed / Wet\r\n\r\n\r\ncoffee %>%\r\n  filter(processing_method == \"Washed / Wet\") %>%\r\n  mutate(country_of_origin = fct_reorder(country_of_origin, total_cup_points, median)) %>%\r\nggplot() + \r\n  aes(y=total_cup_points, x=country_of_origin, fill=country_of_origin) +\r\n  geom_boxplot() +\r\n  theme(axis.text.x = element_text(angle=90), legend.position = \"none\") +\r\n  coord_flip() +\r\n  labs(title=\"US leads in Ratings in Washed/Wet\")\r\n\r\n\r\n\r\n\r\nMexico: Processing Methods\r\n\r\n\r\ncoffee %>%\r\n  filter(country_of_origin == \"Mexico\") %>%\r\n  mutate(processing_method = fct_reorder(processing_method, total_cup_points, median)) %>%\r\n  ggplot() +\r\n  aes(y=total_cup_points, x=processing_method,  fill=processing_method) +\r\n  geom_boxplot(color=\"black\") +\r\n  coord_flip()\r\n\r\n\r\n\r\n\r\nLinear model of total_cup_points\r\n\r\n\r\nbroom::tidy(lm(total_cup_points ~\r\n                 country_of_origin +\r\n                 category_one_defects, data=coffee)) %>%\r\n  filter(p.value < 0.05) %>%\r\n  arrange(p.value)\r\n\r\n\r\n# A tibble: 9 × 5\r\n  term                       estimate std.error statistic      p.value\r\n  <chr>                         <dbl>     <dbl>     <dbl>        <dbl>\r\n1 (Intercept)                  82.4      0.286     288.   0           \r\n2 country_of_originHonduras    -3.06     0.535      -5.72 0.0000000133\r\n3 country_of_originEthiopia     3.10     0.573       5.41 0.0000000757\r\n4 category_one_defects         -0.163    0.0371     -4.40 0.0000116   \r\n5 country_of_originMexico      -1.40     0.359      -3.90 0.000100    \r\n6 country_of_originHaiti       -5.00     1.37       -3.64 0.000284    \r\n7 country_of_originNicaragua   -1.92     0.706      -2.73 0.00649     \r\n8 country_of_originKenya        1.89     0.718       2.63 0.00859     \r\n9 country_of_originColombia     0.741    0.376       1.97 0.0488      \r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2020-05-05_animal-crossing/",
    "title": "Animal Crossing",
    "description": "Looking at the unique animal personalities in Animal Crossing.",
    "author": [
      {
        "name": "Ted Laderas",
        "url": {}
      }
    ],
    "date": "2020-05-05",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nWhat was your dataset?\r\nVillagers\r\nPersonalities by Species\r\n\r\nItems\r\nMost Expensive Items\r\nReproducing the above table\r\nMost Expensive Furniture\r\nMost Expensive Hats\r\nMost Expensive Fossils\r\nPriceless Items by Category\r\n\r\nWhat was your dataset?\r\nLoad your dataset in with the function below. The input is the date the dataset was issued. You should be able to get this from the tt_available() function.\r\n\r\n\r\ncritic <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv')\r\nuser_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')\r\nitems <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/items.csv')\r\nvillagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')\r\n\r\n\r\n\r\nVillagers\r\n\r\n\r\nskimr::skim(villagers)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nvillagers\r\nNumber of rows\r\n391\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n10\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nid\r\n0\r\n1.00\r\n2\r\n8\r\n0\r\n391\r\n0\r\nname\r\n0\r\n1.00\r\n2\r\n8\r\n0\r\n391\r\n0\r\ngender\r\n0\r\n1.00\r\n4\r\n6\r\n0\r\n2\r\n0\r\nspecies\r\n0\r\n1.00\r\n3\r\n9\r\n0\r\n35\r\n0\r\nbirthday\r\n0\r\n1.00\r\n3\r\n5\r\n0\r\n361\r\n0\r\npersonality\r\n0\r\n1.00\r\n4\r\n6\r\n0\r\n8\r\n0\r\nsong\r\n11\r\n0.97\r\n7\r\n16\r\n0\r\n92\r\n0\r\nphrase\r\n0\r\n1.00\r\n2\r\n10\r\n0\r\n388\r\n0\r\nfull_id\r\n0\r\n1.00\r\n11\r\n17\r\n0\r\n391\r\n0\r\nurl\r\n0\r\n1.00\r\n60\r\n66\r\n0\r\n391\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nrow_n\r\n0\r\n1\r\n239.9\r\n140.7\r\n2\r\n117.5\r\n240\r\n363.5\r\n483\r\n▇▇▇▇▇\r\n\r\nPersonalities by Species\r\n\r\n\r\nspecies_count <- villagers %>%\r\n  group_by(species)  %>%\r\n  summarize(species_count = n()) %>%\r\n  arrange(species_count)\r\n\r\ndatatable(species_count)\r\n\r\n\r\n\r\n\r\nlevel_order <- villagers %>%\r\n  group_by(species) %>% count() %>%\r\n  arrange(desc(n)) %>%\r\n  pull(species)\r\n\r\nvillagers %>%\r\n  mutate(species=factor(species, levels=level_order)) %>%\r\n  ggplot() + aes(x=species, y=personality, color=personality) %>%\r\n  geom_count() + \r\n   theme_light() + theme(legend.position = \"none\") +\r\n  theme(axis.text.x = element_text(angle = 90)) \r\n\r\n\r\n\r\n\r\n\r\n\r\nvillagers %>% select(name, species, personality, url) %>%\r\n  mutate(combo = paste(species, personality)) %>% select(name, combo, url) -> villager_index\r\n\r\nunique_combos <- villagers %>%\r\n  group_by(species, personality) %>% summarize(n=n()) %>%\r\n  filter(n == 1) %>% mutate(combo=paste(species, personality)) %>%\r\n  inner_join(y=villager_index, by=c(\"combo\")) %>% ungroup()\r\n\r\nout_image <- unique_combos %>%\r\n  ggplot() + aes(x=species, y=personality, image=url, name=name) +\r\n  geom_count() +\r\n  geom_raster(fill=\"white\", color=\"black\") +\r\n  geom_image(asp=1.2, size=0.03) + \r\n   theme_minimal() + theme(legend.position = \"none\") +\r\n  theme(axis.text.x = element_text(angle = 90)) + labs(title=\"There can be only one\", subtitle = \"Unique Personality/Species combos in Animal Crossing\")\r\n\r\nout_image\r\n\r\n\r\n\r\nggsave(plot=out_image, filename = \"unique_animal_personalities.pdf\", width=10, height = 5)\r\n\r\n\r\n\r\n\r\n\r\npers_vil <- villagers %>% \r\n  group_by(personality, species) %>%\r\n  summarize(count=n()) %>%\r\n  #filter(count==1) %>%\r\n  arrange(species) \r\n\r\npers_vil %>%\r\n  arrange(desc(count)) %>%\r\n  datatable()\r\n\r\n\r\n\r\n\r\nItems\r\n\r\n\r\nskimr::skim(items)\r\n\r\n\r\nTable 2: Data summary\r\nName\r\nitems\r\nNumber of rows\r\n4565\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n10\r\nlogical\r\n2\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nid\r\n0\r\n1.00\r\n3\r\n29\r\n0\r\n4200\r\n0\r\nname\r\n0\r\n1.00\r\n3\r\n29\r\n0\r\n4200\r\n0\r\ncategory\r\n0\r\n1.00\r\n4\r\n11\r\n0\r\n21\r\n0\r\nsell_currency\r\n36\r\n0.99\r\n5\r\n5\r\n0\r\n1\r\n0\r\nbuy_currency\r\n1014\r\n0.78\r\n5\r\n5\r\n0\r\n2\r\n0\r\nsources\r\n3663\r\n0.20\r\n3\r\n71\r\n0\r\n125\r\n0\r\nrecipe_id\r\n3977\r\n0.13\r\n4\r\n20\r\n0\r\n102\r\n0\r\ngames_id\r\n0\r\n1.00\r\n2\r\n2\r\n0\r\n1\r\n0\r\nid_full\r\n1528\r\n0.67\r\n8\r\n34\r\n0\r\n2704\r\n0\r\nimage_url\r\n1528\r\n0.67\r\n56\r\n82\r\n0\r\n2672\r\n0\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\norderable\r\n2775\r\n0.39\r\n0.45\r\nFAL: 976, TRU: 814\r\ncustomizable\r\n3992\r\n0.13\r\n0.45\r\nFAL: 316, TRU: 257\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nnum_id\r\n0\r\n1.00\r\n3661.62\r\n2187.95\r\n12\r\n1722\r\n3569\r\n5607\r\n7443\r\n▇▇▇▇▇\r\nsell_value\r\n36\r\n0.99\r\n2261.38\r\n11313.23\r\n5\r\n240\r\n390\r\n1000\r\n300000\r\n▇▁▁▁▁\r\nbuy_value\r\n1014\r\n0.78\r\n6959.65\r\n34326.09\r\n40\r\n870\r\n1300\r\n2700\r\n1200000\r\n▇▁▁▁▁\r\nrecipe\r\n3977\r\n0.13\r\n4.71\r\n6.49\r\n1\r\n2\r\n3\r\n6\r\n90\r\n▇▁▁▁▁\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nitems %>% ggplot() +\r\n  aes(x=category, y=buy_value) +\r\n  geom_boxplot() +\r\n  ylim(c(0,75000)) +\r\n  theme(axis.text.x  = element_text(angle = 90))\r\n\r\n\r\n\r\n\r\nMost Expensive Items\r\nThis was the original code in my tweet:\r\n\r\n\r\nlibrary(gt)\r\n\r\n items %>% \r\n  top_n(10, buy_value) %>%\r\n  arrange(desc(buy_value)) %>%\r\n  select(name, sell_value, buy_value, category, image=image_url) %>%\r\n  gt() %>%\r\n   text_transform(\r\n    locations = cells_body(vars(image)),\r\n    fn = function(x) {\r\n      web_image(\r\n        url = x,\r\n        height = 50\r\n      )\r\n    }\r\n  )\r\n\r\n\r\n\r\nname\r\n      sell_value\r\n      buy_value\r\n      category\r\n      image\r\n    Royal Crown\r\n300000\r\n1200000\r\nHats\r\nCrown\r\n250000\r\n1000000\r\nHats\r\nGold Armor\r\n80000\r\n320000\r\nDresses\r\nGolden Casket\r\n80000\r\n320000\r\nFurniture\r\nGrand Piano\r\n65000\r\n260000\r\nFurniture\r\nGolden Toilet\r\n60000\r\n240000\r\nFurniture\r\nBlue Steel Staircase\r\nNA\r\n228000\r\nFurniture\r\nIron Bridge\r\nNA\r\n228000\r\nFurniture\r\nRed Steel Staircase\r\nNA\r\n228000\r\nFurniture\r\nRed Zen Bridge\r\nNA\r\n228000\r\nFurniture\r\nZen Bridge\r\nNA\r\n228000\r\nFurniture\r\n\r\n\r\nHere’s a function.\r\n\r\n\r\nmost_expensive <- function(category_name=NULL, price_category=buy_value){\r\n  \r\n  if(!is.null(category_name)){\r\n    items <- items %>%\r\n      filter(category == category_name)\r\n  }\r\n  \r\n  items %>% \r\n  top_n(10, {{price_category}}) %>%\r\n  arrange(desc({{price_category}})) %>%\r\n  select(name, sell_value, buy_value, category, image=image_url) %>%\r\n  gt() %>%\r\n   text_transform(\r\n    locations = cells_body(vars(image)),\r\n    fn = function(x) {\r\n      web_image(\r\n        url = x,\r\n        height = 50\r\n      )\r\n    }\r\n  )\r\n  \r\n}\r\n\r\n\r\n\r\nReproducing the above table\r\n\r\n\r\nmost_expensive()\r\n\r\n\r\n\r\nname\r\n      sell_value\r\n      buy_value\r\n      category\r\n      image\r\n    Royal Crown\r\n300000\r\n1200000\r\nHats\r\nCrown\r\n250000\r\n1000000\r\nHats\r\nGold Armor\r\n80000\r\n320000\r\nDresses\r\nGolden Casket\r\n80000\r\n320000\r\nFurniture\r\nGrand Piano\r\n65000\r\n260000\r\nFurniture\r\nGolden Toilet\r\n60000\r\n240000\r\nFurniture\r\nBlue Steel Staircase\r\nNA\r\n228000\r\nFurniture\r\nIron Bridge\r\nNA\r\n228000\r\nFurniture\r\nRed Steel Staircase\r\nNA\r\n228000\r\nFurniture\r\nRed Zen Bridge\r\nNA\r\n228000\r\nFurniture\r\nZen Bridge\r\nNA\r\n228000\r\nFurniture\r\n\r\n\r\nMost Expensive Furniture\r\n\r\n\r\nmost_expensive(\"Furniture\")\r\n\r\n\r\n\r\nname\r\n      sell_value\r\n      buy_value\r\n      category\r\n      image\r\n    Golden Casket\r\n80000\r\n320000\r\nFurniture\r\nGrand Piano\r\n65000\r\n260000\r\nFurniture\r\nGolden Toilet\r\n60000\r\n240000\r\nFurniture\r\nBlue Steel Staircase\r\nNA\r\n228000\r\nFurniture\r\nIron Bridge\r\nNA\r\n228000\r\nFurniture\r\nRed Steel Staircase\r\nNA\r\n228000\r\nFurniture\r\nRed Zen Bridge\r\nNA\r\n228000\r\nFurniture\r\nZen Bridge\r\nNA\r\n228000\r\nFurniture\r\nElaborate Kimono Stand\r\n55000\r\n220000\r\nFurniture\r\nGolden Seat\r\n50000\r\n200000\r\nFurniture\r\n\r\n\r\nMost Expensive Hats\r\n\r\n\r\nlibrary(gt)\r\nmost_expensive(\"Hats\")\r\n\r\n\r\n\r\nname\r\n      sell_value\r\n      buy_value\r\n      category\r\n      image\r\n    Royal Crown\r\n300000\r\n1200000\r\nHats\r\nCrown\r\n250000\r\n1000000\r\nHats\r\nGold Helmet\r\n50000\r\n200000\r\nHats\r\nBlue Rose Crown\r\n12000\r\n48000\r\nHats\r\nGold Rose Crown\r\n12000\r\n48000\r\nHats\r\nSnowperson Head\r\n7000\r\n28000\r\nHats\r\nKnight's Helmet\r\n3750\r\n15000\r\nHats\r\nDark Cosmos Crown\r\n3360\r\n13440\r\nHats\r\nChic Rose Crown\r\n2880\r\n11520\r\nHats\r\nPurple Hyacinth Crown\r\n2880\r\n11520\r\nHats\r\nPurple Pansy Crown\r\n2880\r\n11520\r\nHats\r\nPurple Windflower Crown\r\n2880\r\n11520\r\nHats\r\nSimple Mum Crown\r\n2880\r\n11520\r\nHats\r\n\r\n\r\nMost Expensive Fossils\r\n\r\n\r\nmost_expensive(\"Fossils\", sell_value)\r\n\r\n\r\n\r\nname\r\n      sell_value\r\n      buy_value\r\n      category\r\n      image\r\n    Brachio Skull\r\n6000\r\nNA\r\nFossils\r\nT. Rex Skull\r\n6000\r\nNA\r\nFossils\r\nBrachio Chest\r\n5500\r\nNA\r\nFossils\r\nBrachio Tail\r\n5500\r\nNA\r\nFossils\r\nDimetrodon Skull\r\n5500\r\nNA\r\nFossils\r\nRight Megalo Side\r\n5500\r\nNA\r\nFossils\r\nT. Rex Torso\r\n5500\r\nNA\r\nFossils\r\nTricera Skull\r\n5500\r\nNA\r\nFossils\r\nBrachio Pelvis\r\n5000\r\nNA\r\nFossils\r\nDimetrodon Torso\r\n5000\r\nNA\r\nFossils\r\nDiplo Skull\r\n5000\r\nNA\r\nFossils\r\nDiplo Tail\r\n5000\r\nNA\r\nFossils\r\nLeft Quetzal Wing\r\n5000\r\nNA\r\nFossils\r\nRight Quetzal Wing\r\n5000\r\nNA\r\nFossils\r\nStego Skull\r\n5000\r\nNA\r\nFossils\r\nT. Rex Tail\r\n5000\r\nNA\r\nFossils\r\nTricera Torso\r\n5000\r\nNA\r\nFossils\r\n\r\n\r\nPriceless Items by Category\r\n\r\n\r\nitems %>%\r\n  filter(is.na(buy_value)) %>%\r\n  ggplot(aes(x=category)) + geom_bar() +\r\n  theme(axis.text.x = element_text(angle=90))\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(ggalluvial)\r\n\r\npers_vil %>% filter(species %in% c(\"cat\", \"rabbit\", \"dog\")) %>%\r\nggplot(\r\n       aes(y = count,\r\n           axis1 = personality, axis2 = species)) +\r\n           geom_alluvium(aes(fill = count),\r\n                width = 0, knot.pos = 0, reverse = FALSE) +\r\n  guides(fill = FALSE) +\r\n  geom_stratum(width = 1/8, reverse = FALSE) +\r\n    geom_text(stat = \"stratum\", infer.label = TRUE, reverse = FALSE) \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2019-11-15-my-experience-with-rstudio-instructor-training/",
    "title": "Notes on the RStudio Instructor Training Experience",
    "description": "Notes on the RStudio Instructor Training and certification exams.",
    "author": [],
    "date": "2019-11-15",
    "categories": [],
    "contents": "\r\n\r\nFull Disclosure: my department paid for the training and two of the certification exams. I did the Shiny exam for free with the stipulation that I would provide feedback on the exam itself.\r\n\r\nI recently became a certified RStudio Instructor in both Shiny and the Tidyverse. I thought I would write a little about the experience. I haven’t really had any formal pedagogical training, and having some of the state of the art and evidence-based practices were really helpful in extending my approaches to teaching.\r\nInstructor Training\r\nThe instructor training is online, but delivered via Zoom. The session themselves are 4 hours apiece and at set times, with a small cohort overall (I think mine was about 14). Greg Wilson delivered a short set of slides for each activity, and depending on the activity, we were sent out into smaller groups to discuss the topic. I liked how the course was paced - I felt like having activities almost every half hour definitely kept me awake.\r\nThe training for me (I am on PST) was from 8 AM to 12 PM two days in a row, and my cohort was from all over the US/Canada.\r\nHere are the parts of the instructor training that I found especially useful. Note that since Greg also helped develop the Carpentries instructor training, there is some overlap between the two training programs (in fact, if you’ve taught a Carpentries course and done the Carpentries training, you can waive the instructor training altogether).\r\nBuilding concept maps. If you can, you should map out the concepts you want to teach, along with how these concepts are related. This concept map is important in helping your students build the mental models that will guide them towards mastery. I did a concept map of Tidy data for my example. Concept maps themselves are a bit tricky to build, but I think they help clarify your understanding of what you are teaching, and thus your lessons will be more understandable.\r\nThinking about cognitive overload. This was a call to think about limiting the number of concepts and connections covered per unit. Humans have a short term memory of about 5 +/- 2 things at a time. This is an extremely helpful guideline to consider when developing training units, as your course objectives and training should keep these in mind. Also, there are different kinds of cognitive overload, and some of them (like extraneous details), you can remove to help with the other kinds. There are lots of ways to teach coding, such as Parsons Problems (where students have to put lines of code in order), or faded examples (fill in the blank style coding assignments), that can focus on particular aspects of coding. (As an unrelated topic, I have been thinking about reading code as being an essential skill and I try to teach students to learn how to read code.)\r\nTeaching needs to be dynamic. As instructors, part of our value lies in tailoring our training for our students and their previous knowledge. Unlike YouTube videos, we can assess what our students already know, and focus on the concepts that they don’t. In order to do this, though, we need to have ways of assessing what our students know. Formative assessments (assessments during a lesson) are essential in figuring out whether you can proceed, or whether you need to spend more time.\r\nProviding feedback. We were to teach each other a short lesson of our choosing, which was pretty fun. After a test run with a larger group, my partner and I got to teach each other - she taught me about making jam (and the food safety issues) and I taught her about making cornbread. (It was close to lunchtime when we did this, I think we were both hungry.) The exercise was super useful in helping us to not be afraid of giving feedback and to be aware of what kinds of feedback to give.\r\nWhat demotivates students. This was an important section, and a bit emotionally difficult. I have been guilty of some of the teaching behavior that demotivates students, such as taking over their keyboard, instead of letting them fix it. If we are to be inclusive and welcoming to everyone, we need to be aware of these (sometimes subtle) behaviors that can discourage and demotivate all of our students.\r\nOther tips from other instructors. I really enjoyed hearing other instructor’s experiences and I had a lot of respect for my cohort. There were lots of little tips we shared with each other that were super helpful, such as the types of mics to use. I also thought that it was helpful when we shared our struggles with each other. It made me feel much more as part of a group of instructors, which was very helpful for me.\r\nInstructor Certification Exam\r\nThe instructor certification covered the pedagogical techniques we discussed in the instructor training. This exam, like the others, was open book, open note, open internet, but not open person (no lifeline, no talking with others).\r\nAs part of the exam, we were required to develop and deliver a lesson. I ended up reworking one of my SQL lessons in my Analytics course. I found it a really useful exercise in rethinking my examples discussing left and inner joins, especially in my table examples. You can see my lesson here: SQL Joins in R.\r\nThis part of the exam is only about 10 minutes long, but you should be prepared, especially in figuring out how you will assess the learning of the students.\r\nTidyverse Certification Exam\r\nAfter I had done my instructor certification, I had to do the technical qualifications. This was also done online, with Greg delivering the exam over Zoom. Most of the exam had to be done via RStudio on my computer. This exam, like the others, was open book, open note, open internet, but not open person (no lifeline, no talking with others).\r\nWhat did the exam cover? Roughly, it covered almost all of R for Data Science and was very task-focused. There was a focus on debugging, data transformation, markdown and visualization. The tasks were mostly problem driven, on the order of recreating a figure, or parametrizing a workflow. It was emphasized that we should solve the problems the way we usually work and talk out our problems aloud. For one of the problems, I actually had to search and teach myself on the fly. This was actually a good way to confirm that my conceptual models of the tidyverse were helpful.\r\nOne tip: 90 minutes is a long amount of time for an exam, especially where you are actively thinking and talking out loud. You should have something to eat by your side so you can replenish your blood glucose. It is also slightly unnerving to have someone watch you actively solve problems and program. If you get flustered, take a breath. There’s not usually a single way to solve the problems.\r\nShiny Certification Exam\r\nA few weeks later, I took the Shiny certification. In terms of coverage, there is not currently a good text to study for the Shiny Certification exam (Mastering Shiny is probably going to be the reference to study when it is finished). In order to prepare, I went over the RStudio articles about Shiny, and looked over the RStudio Shiny Gallery to bone up on the different programming techniques used in Shiny, such as UI elements, reactives, the observe/update pattern, and modules.\r\nAgain, the exam covered debugging, including fixing applications, understanding control logic between ui and server, and building an app from scratch. I would say much of the exam covers how well established your own mental model of Shiny progamming is.\r\nOne thing to remember: as the technology and code changes, the certification exams will change and you may have to get re-certified. So my experience of these exams may be different than your experience.\r\nOverall Experience\r\nWould I recommend the training? I thought it was extremely useful, especially in terms of learning more about evidence based methods in effective education. As someone who is thinking about starting a consulting group focusing on training, having the certifications are extremely valuable to me and my future career. I look forward to teaching and training more people in R.\r\nFor More Info\r\nPlease consult the RStudio Trainer Website for info on how to register for the instructor training.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/",
    "title": "Package Building: How `DESCRIPTION`, `NAMESPACE`, `roxygen`, and `devtools::document` work together",
    "description": "Some thoughts the package building process and how `devtools::document()` is at the center of it..",
    "author": [],
    "date": "2019-02-12",
    "categories": [
      "R"
    ],
    "contents": "\r\nAs part of my new year’s resolution to learn new things about R, I’m trying to plug some holes in my R knowledge by writing more vignettes to explain them to myself this year.\r\nThis week I finally think I understand more about namespaces in R and why you should use them in your R package.\r\nNamespaces: Why Bother?\r\nIn short, we need namespaces because of the ambiguity of function names. Think of how many packages have a filter() function! How does R know which function from which package you’re talking about? One nightmare case is the Bioconductor exprs() method and the rlang exprs() function. These functions do very, very different things (one of them extracts an expression matrix from a Bioconductor ExpressionSet, and the other one is used for quoting multiple expressions).\r\nWhat happens when you call library on both of these packages? The worst case scenario is that you mean to call one package function and R executes the other one. This is called a namespace collision, and unfortunately it can break your code.\r\nEnter the namespace, which allows us to be package and function specific. If I want to use the function filter from dplyr, I write it as dplyr::filter. The dplyr:: part is the namespace of the dplyr package.\r\nDESCRIPTION: The gatekeeper for calling packages\r\nThe DESCRIPTION file is one of the gatekeepers in your package. Ever wonder how install.packages knows how to install the packages your package depends on? It’s actually because of a field in the DESCRIPTION file. Here’s the DESCRIPTION file for my burro package:\r\nPackage: burro\r\nType: Package\r\nTitle: Shiny App Package for setting up a data exploration session (\"burro\"w into the data)\r\nVersion: 0.1.0\r\nAuthors@R: as.person(c(\r\n    \"Ted Laderas <tedladeras@gmail.com> [aut, cre]\",\r\n    \"Jessica Minnier <minnier@ohsu.edu> [ctb]\",\r\n    \"Gabrielle Choonoo <choonoo@ohsu.edu> [ctb]\"\r\n  ))\r\nMaintainer: Ted Laderas <ted.laderas@gmail.com>\r\nDescription: Allows the teacher to deploy a simple data exploration app for exploring a dataset (mostly for teaching purposes).\r\nLicense: MIT LICENSE\r\nEncoding: UTF-8\r\nLazyData: true\r\nImports:\r\n    dplyr,\r\n    shinydashboard,\r\n    ggplot2,\r\n    visdat,\r\n    skimr,\r\n    naniar,\r\n    data.table,\r\n    magrittr,\r\n    glue,\r\n    usethis,\r\n    here,\r\n    viridis,\r\n    DT\r\nDepends:\r\n    shiny\r\nRoxygen: list(markdown = TRUE)\r\nRoxygenNote: 6.1.0\r\nSuggests:\r\n    testthat\r\nLook at the Imports: field. You can see a list of all of the packages that burro utilizes. In ye olde days of R, we used the Depends: field. Nowadays we use the Imports: fields. Here’s a Stack Overflow post explaining why. The main reason is that Imports: requires the package to have a namespace.\r\nModifying the DESCRIPTION file by hand is possible, but I don’t recommend it. Instead, you can use the usethis package to modify it. For example, if I want to use dplyr in my package I can do this in the console, while I am building it.\r\nusethis::use_package(\"dplyr\")\r\nThis will add dplyr to the Imports: field of your DESCRIPTION file.\r\nAddition (Thanks Hao Ye, for the suggestion): If the function is in a development version (i.e., hosted on GitHub), you can use usethis::use_dev_package() to add it to your DESCRIPTION file. It will add an additional field called Remotes: to your package:\r\n\r\n\r\nusethis::use_dev_package(\"tidyverse/dplyr\")\r\n\r\n\r\n\r\nFor more info about using remotes, check out the vignette: https://remotes.r-lib.org/articles/dependencies.html\r\nUsing namespaces to call functions from other packages\r\nNow that we have specified the package in our DESCRIPTION file, we can now call any function in dplyr by adding a dplyr:: before the function. So if we wanted to call mutate() in our package function we can do this:\r\nmutate_iris <- function(iris){\r\n    dplyr::mutate(iris, sepal_sum = Sepal.Length + Sepal.Width)\r\n}\r\nIn many cases, calling a function by specifying its namespace is good practice. For one, there are many functions called filter(): I can think of at least the ones that are in base and dplyr. Using the namespace makes it unambiguous to both R and other developers which filter() function you’re talking about.\r\nHow do I call functions from other packages? Using roxygen docstrings\r\nUgh, I’ve already written a bunch of code and I don’t want to add the namespaces before all of the functions from other packages! How can I avoid this?\r\nThis is where roxygen and devtools::document() come in.\r\nroxygen docstrings are responsible for at least three things in your package: 1) producing the documentation (.Rd) files, but also: 2) specifying what package namespaces you want to utilize, or import in your function, and 3) whether you want to export that function (i.e., make it accessible publicly).\r\nHow do they accomplish 2)? When you call devtools::document() to build the documentation, they scan for multiple fields, such as @import and @importFrom in the roxygen doc strings. Then devtools::document() actually modifies the NAMESPACE file in your package.\r\nUsing @importFrom: When you only need one function\r\nSay you just wanted to use filter from dplyr, but didn’t want to write dplyr::filter before all of your functions. You can just import the filter() function by including the following docstring:\r\n#' @importFrom dplyr filter\r\nAnd then you can just use filter() like normal in your code:\r\n#' @importFrom dplyr filter               #This is where you add the @importFrom\r\nuse_filter <- function(df, cutoff=0.5) {\r\n\r\n  filter(df, value < cutoff)\r\n\r\n}\r\nUsing @import: When you need a lot of functions from a package\r\nWhat if you had a lot of functions from one package, such as shiny, that you want to use? Do you need to add an @importFrom for each of these functions? Nope. You can just use one @import field for the whole package:\r\n#` @import shiny\r\nAnd then code like usual:\r\n#` @import shiny            # This is where you add the @import\r\nshinyUI <- function() {\r\n    selectInput(\"\")\r\n}\r\nImporting multiple packages: just don’t do it (UPDATED).\r\nJust a note to not import multiple packages using @import. As Hadley Wickham has noted, you have no control over the development of the packages you import. Just because there are no function collisions right now between the packages doesn’t mean that one of the developers may add a function down the line that might collide. So, if you need to use multiple packages, @import one package and use @importsFrom or namespaces to refer to functions in the other packages.\r\nExported versus Internal functions\r\nRemember I mentioned ‘@export’ above? Specifying in a docstring for your function exports it. Specifying it means that ‘devtools::document()’ will add an Export directive for the function in the NAMESPACE file. That means you can access it using ‘::’. Going back to our use_filter example:\r\n#' @export                              #This is where you add the @export\r\n#' @importFrom dplyr filter\r\nuse_filter <- function(df, cutoff=0.5) {\r\n\r\n  filter(df, value < cutoff)\r\n\r\n}\r\nIf our package name is mypackage, then this function will be accessible if we use library(mypackage) or mypackage::use_filter().\r\nWhy is this important? You may write some internal functions that are useful in your package, but they aren’t necessarily ones you want your users to use in their daily use. @export allows you to control which functions you make publicly accessible in your package.\r\nFor example, try typing dplyr:: in RStudio and hit the tab key. You’ll see the usual dplyr verbs pop up. But these are only the exported functions. Now try dplyr::: and hit the tab key. You’ll see a list of functions that pop up that’s much longer - these are all the functions, including the internal functions.\r\nSo, if there’s a cool bit of internal code you want to use in a package, you can use ‘:::’ to specify it. Just be aware that oftentimes, internal functions may change a lot as code gets refactored, so code that utilizes them may be refactored.\r\nRemember to run devtools::document()\r\nOnce you’ve written code and want to test it in your package, remember to run devtools::document() before you reinstall your package for testing. Otherwise, the NAMESPACE file won’t be modified, and your code won’t work.\r\nYou can also modify your Project Options for your package and check the Tools >> Project Options >> Build Tools >> Generate Documentation with Roxygen box. Next to it, there is also a Configure button that lets you select the option to run devtools::document() whenever you build a package for testing.\r\nGo forth and Package!\r\n\r\nTL;DR:\r\nUse usethis::use_package() to add packages to your DESCRIPTION file (while in the console) after you’ve created your package skeleton using use_package().\r\nUse namespaces (such as mypackage::usefilter()) to refer to external functions where possible to avoid collisions in function names when you are writing code.\r\nUse @imports and @importsFrom judiciously in your roxygen documentation for a function if you need to use many extenral package functions within a function. Use only 1 @import statement in a function, use namespaces/@importsFrom for the other packages.\r\nExpose functions you want to be made public using @export in your roxygen documentation.\r\nRemember to run devtools::document() in the console after you modify/add these fields to the roxygen documentation when you build for testing, or set it up in Tools >> Project Options.\r\nI hope this helps you to understand exactly the relationships between all of the components that are responsible for accessing namespaces in your package. I was confused about this for years, so writing this has helped me understand these relationships.\r\nFurther Reading\r\nIf you want to know how R searches for a function across its environments and why namespaces are a good thing, this is an excellent writeup: http://blog.obeautifulcode.com/R/How-R-Searches-And-Finds-Stuff/\r\nMore about the usethis workflow: https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/\r\nMore about the DESCRIPTION file: http://r-pkgs.had.co.nz/description.html\r\nMore about namespaces: http://r-pkgs.had.co.nz/namespace.html\r\nMore about roxygen: http://r-pkgs.had.co.nz/man.html#roxygen-comments\r\nAn alternative workflow to modify the DESCRIPTION file: You can use attachment::att_to_description() to scan code and add packages to the file after coding (thanks, Sébastien!).\r\nAcknowledgements\r\nThanks to Hao Ye, Sébastien Rochette, Michael Chirico, Tim Hesterberg, and Hadley Wickham for their comments and questions. I’ve incorporated your suggestions.\r\nNote\r\nI want this to be clear and correct. Please email me if there are any mistakes I’ve made.\r\n\r\n\r\n\r\n",
    "preview": "articles/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/imports-workflow.png",
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {},
    "preview_width": 477,
    "preview_height": 767
  },
  {
    "path": "articles/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/",
    "title": "Reframing Impostor Syndrome as the Road to Mastery",
    "description": "Some thoughts about impostor syndrome.",
    "author": [],
    "date": "2018-12-13",
    "categories": [],
    "contents": "\r\nAt some point in your life in Data Science, you will probably struggle with impostor syndrome. We all do - in fact, even though I have used R and have done bioinformatics and data science for more than 15 years, I still struggle with this feeling. As a beginner, the mountain you must climb to master skills in data science seems like a long and impossible one.\r\nCaitlin Hudon, in her post about dealing with impostor syndrome has this to say about countering impostor syndrome:\r\n\r\nThe way that I’ve dealt with imposter syndrome is this: I’ve accepted that I will never be able to learn everything there is to know in data science — I will never know every algorithm, every technology, every cool package, or even every language — and that’s okay. The great thing about being in such a diverse field is that nobody will know all of these things (and that’s okay too!).\r\n\r\nI think it’s important to try and reframe the feelings of impostor syndrome into something more positive. I think having self-compassion about the difficulties of the learning process can help.\r\nGeorge Leonard’s Mastery is a short book that I think can help provide the antidote to these feelings of fraud and inadequacy. I feel that beginners and learners would feel much better if their instructors would own up to their own personal shortcomings as learners. That is, instead of trying to project the image of the all knowledgable guru, instructors should show themselves as humble, lifelong learners as well.\r\nIn Mastery, Leonard talks about our unrealistic expectations and how these expectations can get in the way of actually learning and mastering a craft. We are conditioned by ads, movies, and social media that mastering a craft is a never ending set of ever rising climaxes (cue the training montage), that we can make continuous and steady progress by working hard enough.\r\nMastering a craft takes practice, and lots of it. We must learn to be contented to practice when we are on a plateau and are not making visible progress. As Leonard notes,\r\n\r\nThe Path to Mastery is practice.\r\n\r\nLeonard outlines 5 principles that can sustain us in our road to mastery and away from impostor syndrome: Instruction, Practice, Surrender, Intentionality, and The Edge. I’m trying to map common feelings of impostor syndrome and show how these principles can counteract these feelings.\r\nInstruction. Leonard emphasizes the importance in finding good instructors and good mentorship that will help us to grow. Finding good instructors can actually be difficult and finding someone who remembers what it was like to be learning something is important. Avoid those instructors who say things like “it should now be obvious” or are disparaging when you don’t understand something.\r\n\r\nHe or she is not necessarily the one that gives the most polished lectures, but rather the one who has discovered how to involve each student actively in the process of learning.\r\n\r\nWhat you can do today: look at twitter and other forums for your community of learners and support those who give good instruction. Realize that not all teachers are good teachers; leave them and seek better ones if necessary.\r\nPractice. Practice for practices’ sake. Deliberate practice where you slowly build up your understanding and perceptions is important to your growth.\r\n\r\nWhere in our upbringing, our schooling, our career are we taught to value, to enjoy, even to love the plateau, the long stretch of diligent effort with no seeming progress?\r\n\r\nWhat you can do today: Join communities of practice such as Tidy Tuesday and share your learning with others. Tidy Tuesday is extremely friendly and encouraging for beginners. Learn together and grow together. These are safe communities to share knowledge.\r\nIntentionality. This goes hand in hand with practice. Deliberate practice requires visualizing your process and guiding yourself gently.\r\n\r\nIntentionality fuels the master’s journey. Every master is a master of vision.\r\n\r\nWhat you can do today: find small exercises and projects that help you reinforce what you’ve learned so far. Find people’s code and vignettes and modify them until you understand what they’ve done and how they structured their work.\r\nSurrender. At some point, you will have to give up your own social position as an expert to grow as a learner. When this happens, you must be willing to risk that standing to progress further. Leonard talks about a karate master learning aikido who was not willing to start from scratch, which impeded his learning. For many of us academics, being willing to abandon the comfort of what we have learned is especially difficult. We feel like we are risking our own social standing and reputation.\r\n\r\nFor the master, surrender means there are no experts. There are only Learners.\r\n\r\nWhat you can do today: be humble when faced with new concepts (for many impostor syndrome sufferers this is not the hard part). Recognize when you need to grow and when you have to leave old concepts behind.\r\nThe Edge. This is where things are undefined and scary. Still, part of the journey to mastery is a willingness to push your thoughts to beyond the horizon of what you thought was possible.\r\n\r\nThe trick here is not only to test the edges of the envelope, but also to walk the fine line between endless, goalless practice and those alluring roles that appear along the way.\r\n\r\nWhat you can do today: Identify some goals that are just beyond your current skillset and be willing to push your learning to that point.\r\nLeonard maintains that true mastery is not due to innate talent. True mastery is due to tenacity and perserverance in the face of difficult learning. In fact, he suggests that learning things too easily means that you might lack perserverance when the going gets rough and your progress slows. He maintains that someone who perseveres will “have learned whatever [they] are practicing to the marrow of [their] bones.”\r\nEncouraging Mastery as a Community\r\nI think Caitlin’s prescriptions for community wide suggestions for reducing impostor syndrome are wonderful. Especially the advice to “Get comfortable with I don’t know”. Normalizing “I don’t know” within a community is incredibly important to making a psychologically safe learning environment.\r\nTo encourage learners, I think that creating a community of practice and helpfulness is vitally important to give new learners the support they need. When communities take responsibility for the learning of their members, something magic happens. Learning no longer feels lonely and there is no shame when you don’t immediately grasp a concept. Patience becomes the norm and people become more confident.\r\nFor me, this is the true value of schools and universities. To get the most out of online learning, you need to participate within a community that encourages you to learn further. Be on the pathway to mastery by participate within learning communities.\r\nFurther Reading\r\nMastery: https://www.goodreads.com/book/show/81940.Mastery\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2018-08-07-shiny-and-tidyeval/",
    "title": "Shiny and Tidyeval Part 1",
    "description": "How do you incorporate tidyeval and shiny together?",
    "author": [],
    "date": "2018-08-07",
    "categories": [
      "shiny"
    ],
    "contents": "\r\n\r\nNote: sometimes I write these posts to teach myself a better way to do things in R.\r\n\r\nI have been avoiding tidyeval somewhat, because I seem to have a bit of a learning block about it. I’m going to try to write some posts that help me understand what’s going on with Tidy Evaluation.\r\nUsing sym() in a Shiny App\r\nOne fairly simple Shiny Application might be selecting a column of the dataset and then doing something with it, such as using it in a select() or filter() statement. Say we had a simple app to produce histograms, and we wanted to change the column that is being displayed on the histogram.\r\nTry this app out by running the following command. The code is here.\r\n\r\n\r\nrunGist(\"https://gist.github.com/laderast/a5205554324306e642b2df9f80ed6409\", display.mode=\"showcase\")\r\n\r\n\r\n\r\nOur input is a select input called numeric_var, which returns a single column name as a character In our server logic, we’ve built a reactive called selected_data, which returns the selected column as a vector using pull().\r\n\r\n\r\n  selected_data <- reactive({\r\n    ## input$numeric_var is a character, so we cast it to symbol\r\n    var_name <- sym(input$numeric_var)\r\n\r\n    ## Now we evaluate it with !!\r\n    out_col <- iris %>% pull(!!var_name)\r\n  })\r\n\r\n\r\n\r\nThe question is: how do we pass the input value into pull()? We first have to use rlang::sym() to pass our character in as a symbol that we’re calling var_name. But the issue is that our reactive doesn’t know which environment to look in.\r\nWe want our reactive to look for the column name within the environment of the iris tibble. This is where the !! (bang-bang) comes in. It says, ‘look for the value’ within the tibble.\r\nUsing syms() in a Shiny App\r\nWhat if wanted to pass in multiple variables from a select box? We’ll need to wrap our input with syms(), which takes a list.\r\nLet’s do a slightly different version where we’re visualizing a box plot and we want to select multiple columns to display in our dataset from a selectInput where we’ve specified the multiple=TRUE argument.\r\nOur setup is similar, but different. Because we have multiple values, we have to use syms() to wrap the input from input$numeric_vars. Then we can evaluate it with !!! (the triple bang).\r\n\r\n\r\n  selected_data <- reactive({\r\n    ## input$numeric_var is a character vector, so we cast it to symbol\r\n    var_list <- syms(input$numeric_vars)\r\n\r\n    ## Now we evaluate it with !!!\r\n    out_col <- iris %>% select(!!!var_list)\r\n  })\r\n\r\n\r\n\r\nTry this app out. The Code is here.\r\n\r\n\r\nrunGist(\"https://gist.github.com/laderast/952120ac46d1f27c2d2dba5bd1ab5d10\", display.mode=\"showcase\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2018-08-02-turning-down-the-noise/",
    "title": "Turning down the noise",
    "description": "More information about managing depression and anxiety.",
    "author": [],
    "date": "2018-08-02",
    "categories": [],
    "contents": "\r\nI’m still in the process of recovering from my current bout of depression and anxiety. I’d like to talk about what is currently helping me moderate my anxiety. I have been practicing mindfulness and meditation for the past three years and I’m beginning to realize how necessary it is in our information dense age. Many of my symptoms of anxiety are really from an information overglut.\r\nI’m currently on way too many projects and am teaching as well. Everything wants my attention. We need to decide dates for a visit, etc. Booking travel, grading students, etc. The noise of academic life can be overwhelming and can prevent me from working effectively. A book I’m reading, Real Happiness at Work by Sharon Salzberg, talks about Attention Deficit Trait disorder:\r\n\r\nAttention deficit trait (ADT) is workplace-induced attention deficit caused by the constant, relentless input of information, these days usually enabled by our high-tech devices, smartphones, and computers.\r\n\r\nThis is especially prevalent for people like me who are on multiple grants and have many collaborators. I’m okay with putting out the occasional fire or dealing with an emergency deadline; if I believe in the project, I can muster the energy. However, the problem is when I have multiple fires to deal with from multiple people. The task switching leads to stress and leads to an inability to prioritize. This is where I’ve been the last few months.\r\nAnd this is when the voices of doubt begin to fuel my anxiety. On top of the enormous task list, there’s the feelings of failure and disappointment because I can’t get simple things done. In my head the voices reach a frenzy, a cacophony, a noise. And then I can’t think straight, prioritize, work on one thing.\r\nWhat is the solution to ADT and the noise of life? Mindfulness and unitasking. As Sharon Salzberg notes:\r\n\r\n… while it’s unrealistic to try to stop the number and variety of incoming demands, in our technologically advanced world it is possible to modulate how much information we’re taking in, and how many tasks we are doing at once. When we slow down and concentrate on doing just what is before us to be done now, we become the masters of our own environment rather than its frantic slaves.\r\n\r\nThis idea (being a master of my environment) appeals to me in the midst of my academic anxiety. Practicing mindfulness (through daily meditation) helps me to focus on the here and now. We are wired to ruminated about the past (things we wish we’d done better) or the future (oh crap, I need to prepare upcoming stuff). Meditation is all about building that focus and attention on what’s before us. If we can’t do it all, we can at least work on what’s right in front of us.\r\nI’ve also been reading Anne Lamott’s Bird by Bird, a book reflecting on the hows and whys of writing and living your life while doing it. It’s a sobering view of why we write and how to keep on in the face of numerous adversities. There’s one passage that really resonated with me in the chapter called “Shitty First Drafts” in dealing with the incessant chatter of life:\r\n\r\nClose your eyes and get quiet for a minute, until the chatter starts up. Then isolate one of the voices and imagine the person speaking as a mouse. Pick it up by the tail and drop it into a mason jar. Then isolate another voice, pick it up by the tail, drop it in the jar. And so on. Drop any high maintenance parental units, drop in any contractors, lawyers, colleagues, children, anyone else who is whining in your head. Then put the lid on, and watch all these mouse people clawing at the glass, jabbering away, trying to make you feel like shit because you won’t do what they want - won’t give them more money, won’t be more successful, won’t see them more often. Then imagine there is a volume control button on the bottle. Turn it all the way up for a minute, and listen to the stream of angry, neglected, guilt-mongering voices. Then turn it all the way down and watch the frantic mice lunge at the glass, trying to get to you. Leave it down, and get back to your shitty first draft.\r\n\r\nThis is another idea in mindfulness: welcoming and acknowledging our worries and negative voices. If we ignore or refuse these feelings, they just become stronger and louder in the din and the noise. I have all sorts of these feelings: I don’t work hard enough, I’m a failure, I’m letting down people who depend on me, Everyone is out there working on cooler things than me. Now I’m trying to take an effort to welcome these feelings into my mental space, saying “okay, I hear you and acknowledge you, so you don’t have to be yelling anymore”. And surprisingly, they do quiet down. Acknowledging the feelings of failure, thanking them for their contribution to the discussion and showing them the door is important. As Kelly Boys says in The Blind Spot Effect:\r\n\r\nIf you approach life with a willingness to be with what you encounter without getting lost in it, it moves gracefully within your experience.\r\n\r\nAnd that helps lower the volume and free your attention.\r\nIf you are feeling generalized anxiety or attention deficit trait, I encourage you to look into mindfulness as a way of turning down the noise.\r\nResources\r\nThese are some of the resources that I’ve used when writing this post.\r\nThe Mindful Geek: Secular Meditation for Smart Skeptics by Michael Taft. This is a great book for people who are interested in Mindfulness and the psychological and neuroscience research about why mindfulness works in reducing depression and anxiety. I started here. Michael Taft calls meditation “a technology for hacking the human wetware to improve your life”.\r\nReal Happiness at Work: Meditations for Accomplishment, Achievement, and Peace by Sharon Salzberg. This is a nice followup to Mindful Geek, talking about how we can regain control in our workplace.\r\nThe Blind Spot Effect: How to Stop Missing What’s Right in Front of You by Kelly Boys. I am really liking this book so far. It’s about how meditation and mindfulness can help us find our cognitive blind spots and move beyond them.\r\nBird by Bird: Some Instructions on Writing and Life by Anne Lamott. I recently started reading this on vacation. Writing can be a lonely and solitary life, and I find many of her suggestions about living to apply equally well to research.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2018-05-24-group-by-summarize-not-just-for-numeric-values/",
    "title": "group_by/summarize: Not just for numeric values!",
    "description": "Learn some more about the many uses of `group_by()`/`summarize()`.",
    "author": [],
    "date": "2018-05-24",
    "categories": [
      "tidyverse"
    ],
    "contents": "\r\nEven though I’ve been using the tidyverse for a couple of years, there’s always a couple new applications of tidyverse verbs.\r\nThis one, in retrospect, is pretty simple. I had a one to many table that I wanted to collapse, tidy-style. Let’s look at the diamonds dataset:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>%\r\n  head() %>%\r\n  knitr::kable()\r\n\r\n\r\ncolor\r\ncut\r\nE\r\nIdeal\r\nE\r\nPremium\r\nE\r\nGood\r\nI\r\nPremium\r\nJ\r\nGood\r\nJ\r\nVery Good\r\n\r\nWhat if we wanted to collapse all the entries for each color into a single line? There’s 7 different colors, so we can use a combination of group_by on color and use the paste() function within summarize() to get what we want, which I’ve called all_colors here. By specifying the collapse argument, we can specify the delimiter within that column:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>% \r\n  group_by(color) %>% \r\n  summarize(all_colors=\r\n              paste(cut, collapse=\";\"))\r\n\r\n\r\n# A tibble: 7 x 2\r\n  color all_colors                                                    \r\n* <ord> <chr>                                                         \r\n1 D     Very Good;Very Good;Very Good;Good;Good;Premium;Premium;Ideal~\r\n2 E     Ideal;Premium;Good;Fair;Premium;Premium;Very Good;Very Good;V~\r\n3 F     Premium;Very Good;Very Good;Very Good;Good;Premium;Very Good;~\r\n4 G     Very Good;Ideal;Ideal;Very Good;Premium;Premium;Ideal;Very Go~\r\n5 H     Very Good;Very Good;Very Good;Good;Good;Very Good;Good;Very G~\r\n6 I     Premium;Very Good;Ideal;Good;Premium;Ideal;Ideal;Ideal;Ideal;~\r\n7 J     Good;Very Good;Good;Ideal;Ideal;Good;Good;Very Good;Very Good~\r\n\r\nThanks to Ken Butler, who pointed out that the tidyverse way (via stringr) is to use str_c instead:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>% \r\n  group_by(color) %>% \r\n  summarize(all_colors=\r\n              stringr::str_c(cut, collapse=\";\")) \r\n\r\n\r\n# A tibble: 7 x 2\r\n  color all_colors                                                    \r\n* <ord> <chr>                                                         \r\n1 D     Very Good;Very Good;Very Good;Good;Good;Premium;Premium;Ideal~\r\n2 E     Ideal;Premium;Good;Fair;Premium;Premium;Very Good;Very Good;V~\r\n3 F     Premium;Very Good;Very Good;Very Good;Good;Premium;Very Good;~\r\n4 G     Very Good;Ideal;Ideal;Very Good;Premium;Premium;Ideal;Very Go~\r\n5 H     Very Good;Very Good;Very Good;Good;Good;Very Good;Good;Very G~\r\n6 I     Premium;Very Good;Ideal;Good;Premium;Ideal;Ideal;Ideal;Ideal;~\r\n7 J     Good;Very Good;Good;Ideal;Ideal;Good;Good;Very Good;Very Good~\r\n\r\nFinally, if we wanted to just get the unique values of the cuts in a single line, we can use unique:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>% \r\n  group_by(color) %>% \r\n  summarize(all_colors=\r\n              paste(unique(cut), collapse=\";\")) \r\n\r\n\r\n# A tibble: 7 x 2\r\n  color all_colors                       \r\n* <ord> <chr>                            \r\n1 D     Very Good;Good;Premium;Ideal;Fair\r\n2 E     Ideal;Premium;Good;Fair;Very Good\r\n3 F     Premium;Very Good;Good;Fair;Ideal\r\n4 G     Very Good;Ideal;Premium;Good;Fair\r\n5 H     Very Good;Good;Premium;Fair;Ideal\r\n6 I     Premium;Very Good;Ideal;Good;Fair\r\n7 J     Good;Very Good;Ideal;Premium;Fair\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students/",
    "title": "What We learned teaching Python to Neuroscience Students",
    "description": "Our team-taught class introducing Neuroscience Graduate Program students to Python.",
    "author": [],
    "date": "2018-01-17",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nWell, the week of teaching our Python Bootcamp for Neuroscientists is over. I had the pleasure of working with a great group of students, professors and instructors in developing the material, and had a great time teaching complete beginners to programming and Python.\r\nWe had the overall goal of introducting 21 Neuroscience Graduate Program students at OHSU to the basics of programming in Python using data that they were interested in: electrophysiology data, and confocal microscopy data. The course was designed to be a 1 credit course to encourage students to persist and finish it.\r\nThe format of the class was spread over 5 days (2.5 hours a day) and had the following schedule:\r\nIntroduction to basic data types in Python\r\nIntroduction to for loops and Pandas DataFrames\r\nUsing Pandas to analyse electrophysiology data\r\nUsing NumPy to analyse confocal microscopy data\r\nEvaluation of students; installing Python/Juypter; wrap-up with questions.\r\nI’ll just write up some random thoughts about our experiences about the course. We are definitely planning to give the course again next year, given the enthusiastic reception.\r\nThings that really worked well\r\nAvoid the first day blues of installing Python by using JupyterHub. I think one of the major pain points for beginners is installing software before they can even learn. Instead of making them install Python the first day, we had them sign into an AWS server that had JuypterHub deployed. JupyterHub is a multi-user server for Juypter Notebooks which had the right version of Python and our need the dependencies installed. So our students just needed a laptop and a web browser to access our lessons. We could update the notebooks by pulling changes from our course repo.\r\nStephen David, my fellow instructor, figured a lot of the difficult deployment details out. He has put together some handy instructions about deploying JuypterHub to AWS and keeping the accounts updated via a GitHub repo in case other people are interested in using our bootcamp materials.\r\nMake the atmosphere welcoming to beginners. In order to do so, we used many great tips from Software/Data Carpentry: modeling resilience by using live coding (and making mistakes along the way), using post-it notes for students to signal when they need help or are finished, and having plenty of TAs per student (at least 4 students/TA or instructor). We tried to emphasize that learning programming is an ongoing process, and that even we still have to Google errors on Stack Overflow. Showing that you can make mistakes and still recover is a big part of that.\r\nPlan some early wins and make the exercises as interactive as possible. For the most part, we tried to avoid lecturing too long and break up the session with interactive exercises. I also really don’t like workshops where the trainer/teacher moves on no matter whether people understand the material or not. By using the post-its to signal when they were done, we were able to more appropriately pace the workshop. We also planned on stopping points if we couldn’t get through the day’s materials.\r\nEmphasize working together and building a community. From the beginning, we emphasized that everyone needed to work together. I always emphasize the chain of help: 1) First your programming partner, 2) then the TA help. Discussing and working on issues together fosters a sense of community. I think there will be a group of students who will really want to learn more because of this.\r\nGetting feedback along the way. I still feel like being a teacher is about 75% preparation and 25% improvisation. You need to be flexible enough to come up with examples on the fly, and you need to evaluate whether students are getting the material along the way. The exercises we tried to sprinkle throughout the notebooks helped us understand where people were stumbling.\r\nPlanning follow-up sessions. Through BioData Club, we’re planning some follow-up sessions. Through DataCamp in the Classroom, I also got our students premium access. We also pointed students out to other Python-based courses at OHSU.\r\nSome things we could improve on\r\nI believe that given our time frame, we couldn’t really have anticipated many of these issues. We did our best to deal with them in the moment, however.\r\nDescribing the difference between Jupyter Notebooks and Python. At the beginning, we glossed over what a Jupyter Notebook was and really didn’t describe its relation to Python. I think next time we will open with describing the relationship between Jupyter and Python with a diagram, and revisit it on the last day.\r\nAnticipate the JupyterHub server requirements better. On Day 3, we had a large dataset that basically hosed the server because 21 students were trying to open it up at once. We managed to recover by getting another AWS server and dividing the students among the two, but we could have stress tested that day a little more. Lesson learned.\r\nGoing slowly enough. I am a very excitable teacher, to the point of which sometimes I go a little too fast. I have to confess that I may have sped through some of the material a little too fast. As a result, some of the students didn’t quite get what functions like enumerate() were for and the concept of unpacking a list. Luckily, Brad Buran covered these on Day 4 and the students felt comfortable enough to finish the programming test on the final day.\r\nSetting student expectations. It’s vital to show the students that they can learn programming, but also what’s possible if they do. One of the days was a big leap from the previous day, but we did mention that it’s really to show them what’s possible if they continued to learn about programming.\r\nWould we do it again?\r\nI would definitely say yes! We had to waitlist some students who really wanted to take it, and our overall feedback about the course was really positive. I hope that we can have more TAs, and have the future data workshops be more student driven.\r\nAcknowlegements\r\nThis was a collaboration between the Neuroscience Graduate Program (NGP) and the Department of Medical Informatics and Clinical Epidemiology (DMICE).\r\nThe NGP students involved in designing and testing the material were\r\nDaniela Saderi\r\nLucille Moore\r\nCharles Heller\r\nZack Schwartz\r\nFaculty/Instructors involved were:\r\nBrad Buran (Research Instructor)\r\nStephen David (NGP Assistant Professor)\r\nLisa Karstens (DMICE Assistant Professor)\r\nMichael Mooney (DMICE Assistant Professor)\r\nTed Laderas (DMICE Assistant Professor)\r\nThanks very much to Gary Westbrook (Director of the NGP program), Shannon McWeeney (Head of the Division of Bioinformatics and Computational Biology within DMICE), and Bill Hersh (Head of DMICE).\r\n\r\n\r\n\r\n",
    "preview": "articles/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students/py_class.jpg",
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "articles/2018-01-05-so-you-ve-accidentally-checked-in-a-large-file-into-git/",
    "title": "So You've Accidentally Checked in a Large File Into Git",
    "description": "Don't panic. I'll show you how to scrub your Git history and get rid of it.",
    "author": [],
    "date": "2018-01-05",
    "categories": [
      "git",
      "git hell"
    ],
    "contents": "\r\nNote: after posting this, I heard back from Roberto Tyley, the creator of the BFG. I’d like to note that the BFG actually does its job really well. I was mostly really frustrated about how Git/GitHub doesn’t prevent a user from doing something that’s hard to undo. So my frustration is really about that, not really about the BFG. This post has been edited to reflect that.\r\nGreg Wilson first said it, but I’ve come to agree. Git is an aggressively antisocial piece of software. Git is a piece of software that can make developers with any amount of experience feel dumb.\r\nRecently, I accidentally checked a large file (greater than 100 Megs) into my local repo. When I tried to push to GitHub, of course, it refused it (I know about git large file storage, but I don’t have any).\r\nSo my local repo was screwed up. Of course, I did what seemed like the rational thing and deleted the file from my repo and recommitted. More than once. This is a mistake I’ve done more than once. So you need to scrub your git history with BFG so that GitHub will accept your lowly commits again.\r\nThe BFG documentation specifies how to fix a remote repo. I would say that this situation is much less common than the local situation. So I just decided to share how I got the BFG to work for the local repo situation.\r\nI usually install the BFG through homebrew, using brew install bfg. When you install it this way, you can just run BFG with bfg. You can download it from the website, but you’ll have to call java -jar bfg[VERSION].jar to run it.\r\nSay you’ve accidently checked in a large file into your current repo. The first thing to do is to clone your local repo:\r\ngit clone --mirror local_repo\r\nThis will create another folder called local_repo.git that you will do all the BFG magic on. This local_rep.git is what is called a bare repo. I want to remove any files larger than 100 Megs, so I do this:\r\nbfg -b 100M local_repo.git\r\nIf this doesn’t return an error, you can move on. However, I got the dreaded error:\r\nWarning : no large blobs matching criteria found in packfiles - \r\ndoes the repo need to be packed?\r\nAugh. Ok, some googling later I found that I needed to pack my orignal repo:\r\ncd local_repo\r\ngit repack\r\nOkay, we need to get rid of our cloned repo and redo the last few steps.\r\nrm -rf local_repo.git\r\ngit clone --mirror local_repo\r\nbfg -b 100M local_repo.git\r\nThen comes some git commands that no one has bothered to explain to me (UPDATE: I forgot to add changing directories into local_repo.git - sorry about that!).\r\ncd local_repo.git\r\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive\r\ngit push\r\nUh oh, I get a remote: error: refusing to update checked out branch: refs/heads/master error! More ugh.\r\nHere’s the trick. Since you cloned a local repo, you need to set the origin of your current repo (local_repo.git) to the GitHub remote. Still in our local_repo.git directory, first we remove the current origin, and then add back our remote.\r\ngit remote rm origin\r\ngit remote add origin https://github.com/laderast/remote_repo\r\nFinally, after much gnashing of the teeth, we can\r\ngit push\r\nDon’t forget to remove your now dirty local_repo, and the mirrored copy, and then pull a fresh copy down!\r\n##remove both original local repo and altered bare repo\r\nrm -rf local_repo\r\nrm -rf local_repo.git\r\n##clone a fresh copy from GitHub\r\ngit clone https://github.com/laderast/remote_repo\r\nThis may be obvious to the 10 people in the world who have read all of the git documentation, but I am not one of them. I’m stuck with git, unfortunately. I’m writing this post to remind me of what to do when I innocently do something like commit a large file to my repo.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-06-06T10:29:48+02:00",
    "input_file": {}
  }
]

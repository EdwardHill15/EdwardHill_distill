[
  {
    "path": "posts/2021-03-19-an-introduction-to-distill/",
    "title": "An Introduction to {distill} for websites",
    "description": "How you too can learn to {distill}.",
    "author": [],
    "date": "2021-03-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nWorkshop Video\r\nLearning Objectives\r\nBefore You Start\r\nTaking a tour of the Project\r\nHow does {distill} work?\r\nWhat is Netlify?\r\nCustomize your about links\r\nAdding a photo\r\nBuilding your website using the Build Tab\r\nPreviewing your website\r\nTry out different postcards themes\r\nAdd Your Rendered .html files to the articles/ folder\r\nCustomize the Menu\r\nGetting Your Website Online\r\nUpdating Your Website\r\nCustomize Your Domain\r\nStyling your website\r\nCreating New Websites\r\nPutting your site code on GitHub\r\nAcknowledgements\r\n\r\nWant to learn how to build a website using {distill}? This is a short blog post explaining the basics of Distill.\r\nhttps://github.com/laderast/distill_website/\r\nWorkshop Video\r\n\r\n\r\nLearning Objectives\r\nLearn the basic components of a distill website\r\nBuild and preview your website using the Build tab\r\nCustomize the website with your bio and your picture\r\nAdd links and entries to your Navbar Menu\r\nAdd knitted html files as article links\r\nGet your website live using Netlify Drop\r\nUpdate your website by claiming it and registering it\r\nChange your domain name to a better one.\r\nBefore You Start\r\nInstall R/RStudio Desktop to your Computer.\r\nInstall the following packages in R:\r\ninstall.packages(c(\"tidyverse\", \"postcards\", \"distill\", \"usethis\"))\r\nCreate a website project in your home directory using usethis.\r\nusethis::use_course(url=\"laderast/distill_website\",destdir=\".\")\r\nTake a look at the website before we get started:\r\nhttps://distill-example.netlify.app/\r\nGather up the follwing materials:\r\nLinks to Social Media (twitter, linkedin, etc)\r\nBio\r\nHeadshot\r\nKnitted .html files you want to share\r\nSign up for a netlify account and log in: https://netlify.com\r\nTaking a tour of the Project\r\nThese are the main files for the project.\r\nindex.Rmd - This is the main website page.\r\nabout.Rmd - This is a nice looking about page built using the {postcards} package.\r\nimage - A folder. Any images you put in here can be accessed by image/ted.jpg in your pages.\r\narticles/ - A folder. This is a nice place to park your articles. For right now, it’s probably easier to have self-contained articles (single html files)\r\n_site.yml - Customize this to change menus and links\r\n_site/ folder - this contains your rendered website - you’ll drop this folder into Netlify Drop and it will serve it.\r\ntheme.css - this is where you can set appearance options, such as font, font-size, and colors.\r\nHow does {distill} work?\r\n{distill} is what is called a static site generator. It takes Markdown and Rmarkdown and converts them to . .html files.\r\nMuch like any RMarkdown file, {distill} uses {knitr} and pandoc to build your website files that are contained in an RStudio Project. It knits your .Rmd files, converting them to .html files to a folder. The default name of this folder is called _site and it contains all of them files you need to upload to make a website.\r\nWhat is Netlify?\r\nNetlify is what is called a hosting service. This is a network of computers called web servers that are accessible via web addresses that will serve your website files when they are requested by a web browser.\r\nThe amazing thing about Netlify is that it is mostly free and it is very fast, no matter where you are (they have web servers almost everywhere).\r\nWe’ll use Netlify Drop to get our website files up and accessible as quickly as possible.\r\nCustomize your about links\r\nTake a look at about.Rmd and start filling out the front matter with your own links:\r\nlinks:\r\n  - label: LinkedIn\r\n    url: \"https://www.linkedin.com/in/ted-laderas-0714a92/\"\r\n  - label: Twitter\r\n    url: \"https://twitter.com/tladeras\"\r\n  - label: Portfolio\r\n    url: \"index.html\"\r\n  - label: Email\r\n    url: \"mailto:email@email.com\"\r\nAdding a photo\r\nAdd your photo to the images folder. Change the line in about.Rmd:\r\nimage: \"image/ted.jpg\"\r\nto the name of your file. For example, if your file is named jane.jpg and you put it in images:\r\nimage: \"image/jane.jpg\"\r\nBuilding your website using the Build Tab\r\nIn the top right panel in RStudio (next to Environment and History), there is a “Build” Tab.\r\nPress the Build Website to run knitr, which will knit your website to the _site folder. This is where your rendered content lives.\r\nBuild Website TabPreviewing your website\r\nOpen the _site folder and click on the index.html file (make sure you’re viewing in web browser)\r\nThis is your main link to the website (the entry point). For example, if I was hosting my website at https://laderast.github.io/, this would be the first page that I would see.\r\n\r\nYour about page is available as about.html in the _site folder.\r\nClick away and make sure that everything works (links in menu, etc). If not, update the _site.yml and build it again.\r\nTry out different postcards themes\r\nThe postcards package has the following built in themes:\r\njolla\r\njolla_blue\r\nonofre\r\ntrestles - which your current site uses\r\nChange this line in your about.Rmd file to the theme of your interest and start building again:\r\noutput:\r\n  postcards::trestles\r\nAdd Your Rendered .html files to the articles/ folder\r\nYou can now add your articles to the articles/ folder.\r\nThere are a couple example articles here. Add your own files here.\r\nIn general, you’ll put knitted html articles here. Distill does not rebuild articles, it leaves that up to you.\r\nThe relative path to access articles is like this:\r\narticles/crops.html\r\nYou’ll use this when adding links to your menu.\r\nCustomize the Menu\r\nThe menu lives in the _site.yml file:\r\nnavbar:\r\n  right:\r\n    - text: \"Home\"\r\n      href: index.html\r\n    - text: \"About\"\r\n      href: about.html\r\n    - text: \"Articles\"\r\n      menu:\r\n          -  text: \"dplyr::slice()\"\r\n             href: articles/slice.html\r\n          -  text: \"Crop Yields\"\r\n             href: articles/crops.html\r\nAdd another menu entry under articles, or modify the above entries to have a link to your articles.\r\nBuild your website again and preview it to make sure the links work.\r\nGetting Your Website Online\r\nWe’ll take the _site folder with our generated website and drop this entire folder into Netlify Drop.\r\nhttps://drop.netlify.com\r\n\r\nUpdating Your Website\r\nThe first thing you want to do is claim your site and register for a Netlify account. That ties your newly created website to your account so you can update it.\r\nWhen you update your website with the Build Website button, you’ll drag the _site folder onto the deploy zone. This is under the deploy tab:\r\n\r\n\r\nMore info here: https://docs.netlify.com/site-deploys/create-deploys/#drag-and-drop\r\nCustomize Your Domain\r\nThat crazy name is the address of your site. To change it, you can click on the Domain Settings button:\r\nDomain Settings ButtonIn the following page, click the Options >> Edit Site Name button. You can change the first part of the domain, such as \"myportfolio.netlify.app).\r\nEdit Site Name ButtonStyling your website\r\nNote that this only applies to the main distill website and not the about.html, since that is styled separately.\r\nIn _site.yml, try uncommenting this line and seeing how the site changes.\r\n#theme: theme.css\r\nYou can modify the appearance of your website by altering the theme.css file. Much more info about this here:\r\nhttps://rstudio.github.io/distill/website.html#theming\r\nCreating New Websites\r\nIf you want to start from scratch, I highly recommend the Distill tutorial here:\r\nhttps://rstudio.github.io/distill/website.html\r\nYou may want to setup your webpage as a blog, which lets you add posts by date:\r\nhttps://rstudio.github.io/distill/blog.html\r\nPutting your site code on GitHub\r\nThis is beyond the scope of this tutorial, but you can put your site code up on GitHub as well. This has the following advantages:\r\nLets others contribute to your website\r\nCan host on GitHub Pages as well\r\nLets others reuse your code for their own website\r\nIf you’re interested in this, I really recommend Happy Git with R as a way to get started.\r\nAcknowledgements\r\nThank you to RStudio for the {distill} package. It is so great!\r\nPortions of this tutorial are adapted from: https://rstudio.github.io/distill/website.html and from https://rstudio.com/resources/webinars/sharing-on-short-notice-how-to-get-your-materials-online-with-r-markdown/\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T20:43:49-07:00",
    "input_file": "an-introduction-to-distill.utf8.md"
  },
  {
    "path": "posts/2021-03-18-updated-blog/",
    "title": "Updated Blog",
    "description": "Reasons why I moved over to {distill} for my blog.",
    "author": [],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\r\nI’ve imported most of my blog posts over from my Hugo/Blogdown blog over to this new {distill} site.\r\nWhy? {distill} is much more lightweight and easier to maintain. If I want, I can drag over articles from tidytuesday explorations or such and easily link to them. The {blogdown} workflow was really becoming a pain, especially in maintaining different Hugo versions, and honestly, the page was getting much too busy.\r\nSo, really, theoretically, you should see more content from me soon.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T20:31:17-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-14-setting-boundaries-and-saying-no/",
    "title": "Setting Boundaries and Saying No",
    "description": "How saying no can be the best self-care.",
    "author": [],
    "date": "2020-10-14",
    "categories": [
      "self-care"
    ],
    "contents": "\r\nI hate saying no. But as I go on in life, I realize that I can’t be all things to all people. I want to help people, but I also have to have enough energy to get through the day.\r\nIt’s easy to go through life doing what everyone expects of you and have nothing to show for it. In fact, this is often the curse of hyper-competent people. People tend to see you as a gear that fits within their own vision and expect you to do things for them.\r\nI want to outline an approach that will make it easier to say no. It starts with understanding yourself.\r\nWhat are your values? Who are you fighting for?\r\nIn order to say no, you have to have clarity on your values and goals. What do you see your life as?\r\nHere are some of my values:\r\nMake doing data science inclusive, accessible, and fun\r\nIntroduce people to data science who hadn’t seen it before\r\nIntroduce useful and applicable data science skills that learners can provide\r\nTeach others about lifelong learning and participate and foster learning communities in Data Science\r\nAn equally useful question to ask yourself is who are you fighting for? In my case:\r\nUnderrepresented Minorities\r\nLGBTQ folks\r\nMyself and my husband\r\nI will say that as I get older, I have achieved greater clarity about what matters to me, and what doesn’t matter to me. This clarity helps me understand what is and what isn’t important to me.\r\nDon’t respond to requests right away\r\nIf you don’t want to add to your workload, I beg you to not give an answer right away. You need to check in with your feelings.\r\nHere is a boilerplate response if you need something to work off of:\r\n\r\n“Thanks for thinking of me with this opportunity. However, I am extremely busy at the moment. I need to evaluate my current workload in order to tell you yes or no.”\r\n\r\nThis buys you time to think and understand about how you feel about the request. Also, it gives you time to compose a thoughtful reply.\r\nAsk Yourself: Is this in line with my values/goals?\r\nTake a deep breath, and ask yourself how you feel about the request.\r\nDoes it help further your goals or the people you’re fighting for? Will it upset your current work/life balance? Are you willing to pay that price?\r\nIf the answer is no to all of these, then the answer is no. No matter how noble their goals, if they don’t fit with yours and your current workload, you should probably say no. You can leave the door open for future collaborations, though.\r\nThe first few times, it may be agonizing to do this. But consulting your feelings about something will become second nature to you.\r\nGently Say No. Don’t Feel Sorry.\r\nHere’s the thing: it is way easier to say no when the request doesn’t align with your values or it doesn’t benefit who you’re fighting for. It gives your response much more force.\r\nYou may need to do a little verbal/email jujitsu here, because some people don’t hear no. Use absolutes for right now, but keep the door open if you see yourself possibly working together in the future.\r\nMore boilerplate:\r\n\r\n“I really appreciate you thinking of me. However, I cannot help with your effort at the moment. I am trying to maintain work/life balance, and this would upset that. \\[I hope you'll consider me for future efforts.\\]”\r\n\r\nYou don’t need to provide more personal reasons than the above, especially if the requestor is a stranger.\r\nBoundaries Keep You Sane\r\nSaying no doesn’t feel good. But the more you do it, the more you can achieve work/life balance.\r\nYou can’t be everything to everyone, and you will end up dissappointing people. But you should feel okay about that, especially if their efforts are outside your own goals.\r\nI have to admit that I’m still learning how to do this. I’ve written this as much for myself as for others.\r\nFurther Reading\r\nParts of this article are adapted from Unfuck Your Boundaries by Faith Harper. Her advice is no-nonsense, but it does require some soul searching to really utilize her advice.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T20:14:22-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/",
    "title": "Getting LearnR tutorials to run on mybinder.org",
    "description": "Getting Shiny, LearnR, and Mybinder.org to play together nicely.",
    "author": [],
    "date": "2020-09-15",
    "categories": [],
    "contents": "\r\nThe learnr package is a wonderful way to package your tutorials. Anyone can download a learnr package of tutorials, and use learnr::run_tutorial() to run them on their own personal system.\r\nSince learnr is based on shiny, these tutorials can also be published to a Shiny server such as shinyapps.io. However, one drawback to this is that the more popular the tutorial, the more access time you may be on the hook for, which can rapidly become expensive.\r\nEnter mybinder.org, which is a way of running reproducible analyses and tutorials. In short, you give a mybinder.org server some information about the software environment needed to run your code, and you can run code based on your repo. Mybinder.org servers are a little limited, (1 Gb RAM), but this is more than capable of running most learnr tutorials.\r\nRecently, they added Shiny as a deployable format, which means you can run shiny apps, including learnr tutorial packages off their servers.\r\nI’m starting a learnr tutorial package called tidyowl and I decided to share what I’ve learned.\r\nWhat’s the problem?\r\nHowever, there are some differences between the file structure of a learnr package and the expected file structure of a shiny-ready mybinder.org repository.\r\nCan we get both setups to work at the same time? Yes, we can.\r\nWhy would we want to run learnr tutorials off mybinder.org?\r\nThe short answer is making your material accessible to as many people as possible. learnr tutorials can be run from a phone or tablet, and running your tutorial for a lot of people doesn’t cost you any bandwidth or usage costs, as you’re using the same infrastructure that mybinder.org provides.\r\nQuick Review of learnr package structure\r\nFor a package, learnr tutorials are stored in the following folder:\r\ninst/tutorials/TUTORIAL_NAME\r\nSo, if you had a tutorial named learning_shiny it would live in\r\ninst/tutorials/learning_shiny/\r\nand the .Rmd file containing the tutorial should be named learning_shiny.Rmd as well.\r\nThe problem I encountered is that mybinder.org expects your tutorial to exist as a folder in the root of the repo. In other words, it needs to see\r\nlearning_shiny/ in the root of the repository to run.\r\nWe can fix this by adding a file called postBuild that gives instructions to run after the software environment is built. We’ll use it to copy the tutorials into the root folder.\r\nMaking your tutorial mybinder.org ready\r\nIn short, you’ll need 3 files to make your learnr tutorial mybinder.org ready: a runtime.txt, a install.R, and a postBuild file in order to make your learnr package compatible with mybinder.org. Let’s go through the steps:\r\nStep 1. Specify a runtime.txt file in your root folder. You’ll need a file called runtime.txt that contains a single line:\r\nr-3.6-2020-08-01\r\nThis gives mybinder.org the signal that the Docker image needs to have R installed. You can see that I specified a version (3.6) and a snapshot date (2020-08-01). These should be a valid version and date for the snapshot - check the MRAN pages for more info: https://mran.microsoft.com/\r\nNote: When R 4.0 and greater is available in MRAN, you should move to it. It includes RStudio Package Manager, which installs the binary images rather than installing from source code which speeds up building the Docker images by quite a bit.\r\nAnother Note: I tried to get this to work with a Dockerfile using the rocker/binder images, but I couldn’t get this image to work. If anyone has gotten this working, I’d appreciate you sharing how you did it.\r\nStep 2. Specify package dependencies using install.R in your root folder - in this file, you’ll need to specify all the packages your tutorial is dependent on using install.packages() commands. Here’s the contents of my install.R file:\r\ninstall.packages(\"learnr\")\r\ninstall.packages(\"here\")\r\ninstall.packages(\"tidyverse\")\r\nNote: getting the dependencies right in package building can be major headaches to getting your binder container to work. You may have to specify some system dependencies in your apt.txt file for certain packages. This information is available here: https://github.com/rstudio/r-system-requirements\r\nStep 3. Specify moving the tutorials in inst/tutorials/ to the repository root folder using the postBuild folder. These commands are run after the container is built and will make the tutorials accessible via Binder.\r\nFor example, for the tidyowl package, I have these mv commands in my postBuild:\r\nmv inst/tutorials/learning_tidyselect/ .\r\nmv inst/tutorials/learning_rowwise/ .\r\nYou’ll need a mv line for each tutorial that your package contains.\r\nNote: Using holepunch\r\nI believe you can also use holepunch to make setup a little easier. https://github.com/karthik/holepunch\r\nI haven’t tried it yet, but will update this when I do.\r\nBuild the Docker Image for your tutorial\r\nOkay, almost there! Now we’re going to go to mybinder.org to build your Docker image. This is the software environment that your tutorial will run off of. This image will have shiny-server and RStudio installed on it automatically, which makes debugging your package easier.\r\nWhen you’re ready, go to https://mybinder.org and put in the public location of your repository. Then click the “Launch” button.\r\nNow your container will build. Note that this will take a little while (10+ minutes), especially if you need to install something like tidyverse. Note that this can be one of the hardest steps to get going, especially if you need packages such as sf (see above for a link to system dependencies).\r\n\r\nWhen it’s done building, you’ll be at a Jupyter page. Click “New >> RStudio” to open up your image with RStudio.\r\n\r\nYou should see that your tutorial folders have been moved to the root folders. This is good confirmation that the mv statements of postBuild work. I personally like to have individual data/ folders in each tutorial, as it makes making them a little easier to deploy.\r\nTest out running the tutorial by going to the .Rmd file and running it.\r\n\r\nIf you’ve setup everything right, you should see your learnr tutorial popup. I will say that this is usually the fine tuning step that takes the longest.\r\nNote: there is a GitHub action (https://github.com/jupyterhub/repo2docker-action) to rebuild your Docker image on new commits. I’ll be looking into this in the future.\r\nSpecify the URL for running your tutorial.\r\nEach tutorial in your package will need its own URL to run.\r\nYou’ll add the following to your mybinder.org link:\r\n?urlpath=shiny/learning_tidyselect/\r\nThe urlpath is a signal to mybinder that it will need to run shiny, and you’ll put the name of your tutorial folder instead of learning_tidyselect. Note the trailing slash after learning_tidyselect.\r\nSo, my final URL for the learning_tidyselect tutorial is this (click it and try it out):\r\nhttps://mybinder.org/v2/gh/laderast/tidyowl/master?urlpath=shiny/learning_tidyselect/\r\nYou can now send this link out and nearly anyone in the world can run your learnr tutorial without installing R and not using up precious Shinyapps.io CPU time!\r\nCaveats\r\nShiny apps may suddenly disconnect - have students reload the page if that happens.\r\nImages get deleted off the mybinder.org servers within a week, so it is worth automating your container build to do so every week so that your students don’t have to wait for your container image to rebuild.\r\nAlso, progress is not saved, because the final url is different each time you run it off mybinder.org servers.\r\nAcknowledgements\r\nThanks so much to the mybinder.org team, what they do is beyond awesome.\r\nThanks to Sang Yun Oh, whose repository helped me figure more of these details out. https://github.com/syoh/learnr-tutorial\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-09-15-getting-learnr-tutorials-to-run-on-mybinder-org/binder_repo.jpg",
    "last_modified": "2021-03-19T20:13:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-15-my-experience-with-rstudio-instructor-training/",
    "title": "Notes on the RStudio Instructor Training Experience",
    "description": "Notes on the RStudio Instructor Training and certification exams.",
    "author": [],
    "date": "2019-11-15",
    "categories": [],
    "contents": "\r\n\r\nFull Disclosure: my department paid for the training and two of the certification exams. I did the Shiny exam for free with the stipulation that I would provide feedback on the exam itself.\r\n\r\nI recently became a certified RStudio Instructor in both Shiny and the Tidyverse. I thought I would write a little about the experience. I haven’t really had any formal pedagogical training, and having some of the state of the art and evidence-based practices were really helpful in extending my approaches to teaching.\r\nInstructor Training\r\nThe instructor training is online, but delivered via Zoom. The session themselves are 4 hours apiece and at set times, with a small cohort overall (I think mine was about 14). Greg Wilson delivered a short set of slides for each activity, and depending on the activity, we were sent out into smaller groups to discuss the topic. I liked how the course was paced - I felt like having activities almost every half hour definitely kept me awake.\r\nThe training for me (I am on PST) was from 8 AM to 12 PM two days in a row, and my cohort was from all over the US/Canada.\r\nHere are the parts of the instructor training that I found especially useful. Note that since Greg also helped develop the Carpentries instructor training, there is some overlap between the two training programs (in fact, if you’ve taught a Carpentries course and done the Carpentries training, you can waive the instructor training altogether).\r\nBuilding concept maps. If you can, you should map out the concepts you want to teach, along with how these concepts are related. This concept map is important in helping your students build the mental models that will guide them towards mastery. I did a concept map of Tidy data for my example. Concept maps themselves are a bit tricky to build, but I think they help clarify your understanding of what you are teaching, and thus your lessons will be more understandable.\r\nThinking about cognitive overload. This was a call to think about limiting the number of concepts and connections covered per unit. Humans have a short term memory of about 5 +/- 2 things at a time. This is an extremely helpful guideline to consider when developing training units, as your course objectives and training should keep these in mind. Also, there are different kinds of cognitive overload, and some of them (like extraneous details), you can remove to help with the other kinds. There are lots of ways to teach coding, such as Parsons Problems (where students have to put lines of code in order), or faded examples (fill in the blank style coding assignments), that can focus on particular aspects of coding. (As an unrelated topic, I have been thinking about reading code as being an essential skill and I try to teach students to learn how to read code.)\r\nTeaching needs to be dynamic. As instructors, part of our value lies in tailoring our training for our students and their previous knowledge. Unlike YouTube videos, we can assess what our students already know, and focus on the concepts that they don’t. In order to do this, though, we need to have ways of assessing what our students know. Formative assessments (assessments during a lesson) are essential in figuring out whether you can proceed, or whether you need to spend more time.\r\nProviding feedback. We were to teach each other a short lesson of our choosing, which was pretty fun. After a test run with a larger group, my partner and I got to teach each other - she taught me about making jam (and the food safety issues) and I taught her about making cornbread. (It was close to lunchtime when we did this, I think we were both hungry.) The exercise was super useful in helping us to not be afraid of giving feedback and to be aware of what kinds of feedback to give.\r\nWhat demotivates students. This was an important section, and a bit emotionally difficult. I have been guilty of some of the teaching behavior that demotivates students, such as taking over their keyboard, instead of letting them fix it. If we are to be inclusive and welcoming to everyone, we need to be aware of these (sometimes subtle) behaviors that can discourage and demotivate all of our students.\r\nOther tips from other instructors. I really enjoyed hearing other instructor’s experiences and I had a lot of respect for my cohort. There were lots of little tips we shared with each other that were super helpful, such as the types of mics to use. I also thought that it was helpful when we shared our struggles with each other. It made me feel much more as part of a group of instructors, which was very helpful for me.\r\nInstructor Certification Exam\r\nThe instructor certification covered the pedagogical techniques we discussed in the instructor training. This exam, like the others, was open book, open note, open internet, but not open person (no lifeline, no talking with others).\r\nAs part of the exam, we were required to develop and deliver a lesson. I ended up reworking one of my SQL lessons in my Analytics course. I found it a really useful exercise in rethinking my examples discussing left and inner joins, especially in my table examples. You can see my lesson here: SQL Joins in R.\r\nThis part of the exam is only about 10 minutes long, but you should be prepared, especially in figuring out how you will assess the learning of the students.\r\nTidyverse Certification Exam\r\nAfter I had done my instructor certification, I had to do the technical qualifications. This was also done online, with Greg delivering the exam over Zoom. Most of the exam had to be done via RStudio on my computer. This exam, like the others, was open book, open note, open internet, but not open person (no lifeline, no talking with others).\r\nWhat did the exam cover? Roughly, it covered almost all of R for Data Science and was very task-focused. There was a focus on debugging, data transformation, markdown and visualization. The tasks were mostly problem driven, on the order of recreating a figure, or parametrizing a workflow. It was emphasized that we should solve the problems the way we usually work and talk out our problems aloud. For one of the problems, I actually had to search and teach myself on the fly. This was actually a good way to confirm that my conceptual models of the tidyverse were helpful.\r\nOne tip: 90 minutes is a long amount of time for an exam, especially where you are actively thinking and talking out loud. You should have something to eat by your side so you can replenish your blood glucose. It is also slightly unnerving to have someone watch you actively solve problems and program. If you get flustered, take a breath. There’s not usually a single way to solve the problems.\r\nShiny Certification Exam\r\nA few weeks later, I took the Shiny certification. In terms of coverage, there is not currently a good text to study for the Shiny Certification exam (Mastering Shiny is probably going to be the reference to study when it is finished). In order to prepare, I went over the RStudio articles about Shiny, and looked over the RStudio Shiny Gallery to bone up on the different programming techniques used in Shiny, such as UI elements, reactives, the observe/update pattern, and modules.\r\nAgain, the exam covered debugging, including fixing applications, understanding control logic between ui and server, and building an app from scratch. I would say much of the exam covers how well established your own mental model of Shiny progamming is.\r\nOne thing to remember: as the technology and code changes, the certification exams will change and you may have to get re-certified. So my experience of these exams may be different than your experience.\r\nOverall Experience\r\nWould I recommend the training? I thought it was extremely useful, especially in terms of learning more about evidence based methods in effective education. As someone who is thinking about starting a consulting group focusing on training, having the certifications are extremely valuable to me and my future career. I look forward to teaching and training more people in R.\r\nFor More Info\r\nPlease consult the RStudio Trainer Website for info on how to register for the instructor training.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T20:10:57-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-09-23-kindness-at-the-cost-of-yourself/",
    "title": "Kindness at the Cost of Yourself",
    "description": "Being overdrawn at the bank of kindness and self-care.",
    "author": [],
    "date": "2019-09-23",
    "categories": [
      "self-care"
    ],
    "contents": "\r\n\r\nIn healthy cultures, people rise by elevating others and fall by undermining others. In toxic cultures, people are forced to choose between helping others and achieving success. Choose the workplace where success comes from making others successful. - Adam Grant\r\n\r\nWhen I read Adam Grant’s Give and Take, it was like a light went off. In this book, Grant gives profiles of highly successful people who are givers - people who have changed their field by making things better for others. I immediately connected to this idea of giving. Here was a way to network with people that really worked for me. Giving and connecting people are what I do best. I like to help people achieve their goals if I know someone else who can help them.\r\nAnd no doubt, embracing my identity as a giver has helped change my career trajectory - in the last few years, I co-founded Cascadia R, produced the R-Bootcamp with Jessica Minnier, and have met so many people in the field of education and open science that I want to work with and who have inspired me.\r\nHowever, as I’ve noted before, I tend to be giving to the point of being self sacrificing, which is not good. I’m reminded of the end of Kafka’s Metamorphosis when (spoilers) as Gregor Samsa dies in his bug form. As he dies, he worries about the agency of his family. Will they be able to support themselves? In the end, the family that he was worried about finds jobs, support themselves and thrive in the end. Without him. One interpretation of this story: no one is so irreplaceable in this world.\r\nThis Spring, I probably gave a little too much to my Health Analytics class, to the point of being self-sacrificing. This shouldn’t be a reflection of the class - they were great and really whip smart - I just tend to give a little too much. As a result, I was pretty drained by the time I got to Summer quarter. I’d definitely overdrawn on giving - I was pretty wiped out through the summer. Being overdrawn actually meant I didn’t have enough energy to teach as well as I would have liked - and I felt worse for this.\r\nThe lesson I’ve learned this summer: If you find yourself giving to the point of self-sacrifice, you need to ask yourself why you are sacrificing so much of yourself. Excessive self-sacrifice seems to occur when your self-esteem is tied to the perception of yourself by other people. Meaning, you should question why you are self-sacrificing. Is it because you feel like you need the approval of others? No one’s approval is worth overworking yourself over.\r\nSo, self-sacrifice is ultimately bullshit. Ironically, being self-sacrificing can lead to feelings of helplessness and resentment from the people you are sacrificing yourself for. Additionally, being highly proactive and championing change can actually have negative effects if your actions are perceived as a threat to your organizational culture. There’s a reason why change management is important. No one person can be a revolution - it takes others to do so. It shouldn’t fall to just being on you.\r\nYou need to take care of yourself and protect yourself from the takers. As Faith Harper writes in Unf#ck Your Adulting, “taking care of yourself does the world a favor”. Taking care of yourself means that you can do good in the world sustainably, and continue to do it, rather than burning out.\r\nUnderstand that the notion of true giving is not rooted in self-sacrifice. Giving is in the spirit of generosity; in contrast, self-sacrifice is more akin depleting one’s own resources. As a giver, your generosity should be recognized and not exploited.\r\nOne thing to be aware of as a giver is passion exploitation, where people and organizations will take advantage of you because of your passion for a subject as an opportunity to pay you much less than you’re worth. Such an attitude is prevalent in creative industries, where one is encouraged to work for free or “exposure” (I don’t really do soundtrack work any more because of this). As if being a poor artist or creative was a privilege. Such situations will drain your passion and your energy and burn you out with nothing to show for it. Again, you have to keep something for yourself.\r\nIf you want to continue to do good in the world, you must take care of yourself. This is especially the case in a toxic work culture. You have to build reserves - never give more of yourself than you can. Especially do not do this in a work setting. It is amazing how self-sacrifice can become the status-quo and what is expected of workers. No one is well off because of that. Toxic work cultures encourage this: crunch time as the norm at game studios, overwork at Japanese Black Companies, even in the non-profit sector.\r\nIn short, I believe in kindness and giving. But the kindness and giving must be sustainable. You must defend yourself from those who would take more than they give, and avoid being exploited by those who see your passion as something exploitable. Identifying these people and organizations and saying no is key to taking care of yourself while making things better for others. Don’t feel guilty about this. Don’t make self-sacrifice part of your identity or job description.\r\nResources\r\nGive and Take - this is the book that opened my eyes to the power of being a social giver.\r\nUnfuck Your Adulting - this book by Faith Harper is full of good advice about navigating situations, emotions, and taking responsibility for what you can take responsibility for.\r\nHow to be a giver, not a self-sacrificer - talks about giving in the spirit of generosity, and not self-sacrifice. Again, keep something for yourself.\r\nWhen to take initiative at work, and when not to\r\nAvoiding Burnout: Tips for self-care - again, very good tips on taking care of yourself, because no one else will.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T20:10:03-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-03-burning-out-and-recovering-slowly/",
    "title": "Burning Out and Recovering (slowly)",
    "description": "How I'm recovering from burnout.",
    "author": [],
    "date": "2019-07-03",
    "categories": [],
    "contents": "\r\nLast year about this time, I had a meltdown, which is a symptom of burnout. I’d like to explain a little bit about my burnout in the hopes that other people can avoid it.\r\nI want to help people, and I’m a people pleaser. I’m a giver, and sometimes I give too much, to the point I have nothing left. I have to be aware when people exploit this. One collaboration, unfortunately took advantage of my giving nature.\r\nI was on a contracted project where I used some of the time to build an open source project to enable our research. With our collaborators, I showed them my work and they seemed excited. Unfortunately, this “excitement” turned into unreasonable demands. I was literally trying to do a full time development position while balancing the demands of other projects, including teaching. I started feeling like a machine, where each of my collaborators just wanted me to output figures and analyses.\r\nThe worst I’ve ever felt is when I have been busting my ass for things I don’t believe in, or have been stretched across too many projects. Unlike a lot of faculty at OHSU, I don’t have my own R01. I am a collaborator, and I work hard at it.\r\nThe wake-up call for me was persistent thoughts of suicide. I realized that I didn’t care that much about anything and I knew that things had to change.\r\nRegaining Balance\r\nThe past year has been about finding my way back. I’ve been trying to do the following to regain my balance:\r\nWorking 4 days/week instead of the usual 5. On my “day off”, I try my best to contribute to open source projects, work on music, and do other things that recharge me. This has enabled me to say no more often.\r\nUnderstanding my values and restructuring my work to fit it. I’m much more focused on education, outreach, and mentoring these days. If a project comes up, I have to ask myself whether it is in line with my values. I’d like my research to include educational and data exploration components, such as Shiny.\r\nNot putting all my eggs in the work basket. I’m an artist in addition to being a data scientist. I play the cello, compose, and do photography. I make more time for these.\r\nRealizing that excellence does not require self sacrifice. I try to stop working on things obsessively, and only within the time I’m paid.\r\nMindfulness work. I’ve been using the Calm app to help me reorient me and not immediately react to adversities I encounter.\r\nGiving up on perfection. I’m learning to slowly abandon perfection on projects I don’t care that much about (more on this later), and instead turn my alerts off, or only respond to them within sane business hours.\r\nAm I Recovered?\r\nI’d like to say I’m fully recovered, but I’m not. I’m not sure I’ll ever be 100% recovered, if recovered means the insane productivity levels I was showing before. I have been slowly asking myself whether projects expect insane levels of work for the amount of time I can dedicate to them.\r\nThankfully, I have had really great support from my department and from the university overall throughout. I have had support from the communities I help manage (BioData Club and Portland R User Group), and all the great people involved in open science that I have met (rOpenSci, Mozilla, The Carpentries).\r\nI would say that this support has been wonderful, and I feel like the journey back from research burnout has been ongoing. I wish there was a magic bullet for burnout, but working one’s way out of mental and emotional exhaustion ultimately takes a lot of time.\r\nResources\r\nIf you are burned out, please seek mental health assistance if you need help.\r\nThe Telltale Signs of Burnout - How to recognize when you’re burned out.\r\nMayo Clinic on Burnout - More information on Burnout.\r\nResponding to Burnout. How can we recognize burnout in others?\r\nWorkLife Podcast - This podcast by Adam Grant (organizational psychologist) has been full of ideas on how to make what I work on more meaningful. Highly recommended.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:07:15-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/",
    "title": "Package Building: How `DESCRIPTION`, `NAMESPACE`, `roxygen`, and `devtools::document` work together",
    "description": "Some thoughts the package building process and how `devtools::document()` is at the center of it..",
    "author": [],
    "date": "2019-02-12",
    "categories": [
      "R"
    ],
    "contents": "\r\nAs part of my new year’s resolution to learn new things about R, I’m trying to plug some holes in my R knowledge by writing more vignettes to explain them to myself this year.\r\nThis week I finally think I understand more about namespaces in R and why you should use them in your R package.\r\nNamespaces: Why Bother?\r\nIn short, we need namespaces because of the ambiguity of function names. Think of how many packages have a filter() function! How does R know which function from which package you’re talking about? One nightmare case is the Bioconductor exprs() method and the rlang exprs() function. These functions do very, very different things (one of them extracts an expression matrix from a Bioconductor ExpressionSet, and the other one is used for quoting multiple expressions).\r\nWhat happens when you call library on both of these packages? The worst case scenario is that you mean to call one package function and R executes the other one. This is called a namespace collision, and unfortunately it can break your code.\r\nEnter the namespace, which allows us to be package and function specific. If I want to use the function filter from dplyr, I write it as dplyr::filter. The dplyr:: part is the namespace of the dplyr package.\r\nDESCRIPTION: The gatekeeper for calling packages\r\nThe DESCRIPTION file is one of the gatekeepers in your package. Ever wonder how install.packages knows how to install the packages your package depends on? It’s actually because of a field in the DESCRIPTION file. Here’s the DESCRIPTION file for my burro package:\r\nPackage: burro\r\nType: Package\r\nTitle: Shiny App Package for setting up a data exploration session (\"burro\"w into the data)\r\nVersion: 0.1.0\r\nAuthors@R: as.person(c(\r\n    \"Ted Laderas <tedladeras@gmail.com> [aut, cre]\",\r\n    \"Jessica Minnier <minnier@ohsu.edu> [ctb]\",\r\n    \"Gabrielle Choonoo <choonoo@ohsu.edu> [ctb]\"\r\n  ))\r\nMaintainer: Ted Laderas <ted.laderas@gmail.com>\r\nDescription: Allows the teacher to deploy a simple data exploration app for exploring a dataset (mostly for teaching purposes).\r\nLicense: MIT LICENSE\r\nEncoding: UTF-8\r\nLazyData: true\r\nImports:\r\n    dplyr,\r\n    shinydashboard,\r\n    ggplot2,\r\n    visdat,\r\n    skimr,\r\n    naniar,\r\n    data.table,\r\n    magrittr,\r\n    glue,\r\n    usethis,\r\n    here,\r\n    viridis,\r\n    DT\r\nDepends:\r\n    shiny\r\nRoxygen: list(markdown = TRUE)\r\nRoxygenNote: 6.1.0\r\nSuggests:\r\n    testthat\r\nLook at the Imports: field. You can see a list of all of the packages that burro utilizes. In ye olde days of R, we used the Depends: field. Nowadays we use the Imports: fields. Here’s a Stack Overflow post explaining why. The main reason is that Imports: requires the package to have a namespace.\r\nModifying the DESCRIPTION file by hand is possible, but I don’t recommend it. Instead, you can use the usethis package to modify it. For example, if I want to use dplyr in my package I can do this in the console, while I am building it.\r\nusethis::use_package(\"dplyr\")\r\nThis will add dplyr to the Imports: field of your DESCRIPTION file.\r\nAddition (Thanks Hao Ye, for the suggestion): If the function is in a development version (i.e., hosted on GitHub), you can use usethis::use_dev_package() to add it to your DESCRIPTION file. It will add an additional field called Remotes: to your package:\r\n\r\n\r\nusethis::use_dev_package(\"tidyverse/dplyr\")\r\n\r\n\r\n\r\nFor more info about using remotes, check out the vignette: https://remotes.r-lib.org/articles/dependencies.html\r\nUsing namespaces to call functions from other packages\r\nNow that we have specified the package in our DESCRIPTION file, we can now call any function in dplyr by adding a dplyr:: before the function. So if we wanted to call mutate() in our package function we can do this:\r\nmutate_iris <- function(iris){\r\n    dplyr::mutate(iris, sepal_sum = Sepal.Length + Sepal.Width)\r\n}\r\nIn many cases, calling a function by specifying its namespace is good practice. For one, there are many functions called filter(): I can think of at least the ones that are in base and dplyr. Using the namespace makes it unambiguous to both R and other developers which filter() function you’re talking about.\r\nHow do I call functions from other packages? Using roxygen docstrings\r\nUgh, I’ve already written a bunch of code and I don’t want to add the namespaces before all of the functions from other packages! How can I avoid this?\r\nThis is where roxygen and devtools::document() come in.\r\nroxygen docstrings are responsible for at least three things in your package: 1) producing the documentation (.Rd) files, but also: 2) specifying what package namespaces you want to utilize, or import in your function, and 3) whether you want to export that function (i.e., make it accessible publicly).\r\nHow do they accomplish 2)? When you call devtools::document() to build the documentation, they scan for multiple fields, such as @import and @importFrom in the roxygen doc strings. Then devtools::document() actually modifies the NAMESPACE file in your package.\r\nUsing @importFrom: When you only need one function\r\nSay you just wanted to use filter from dplyr, but didn’t want to write dplyr::filter before all of your functions. You can just import the filter() function by including the following docstring:\r\n#' @importFrom dplyr filter\r\nAnd then you can just use filter() like normal in your code:\r\n#' @importFrom dplyr filter               #This is where you add the @importFrom\r\nuse_filter <- function(df, cutoff=0.5) {\r\n\r\n  filter(df, value < cutoff)\r\n\r\n}\r\nUsing @import: When you need a lot of functions from a package\r\nWhat if you had a lot of functions from one package, such as shiny, that you want to use? Do you need to add an @importFrom for each of these functions? Nope. You can just use one @import field for the whole package:\r\n#` @import shiny\r\nAnd then code like usual:\r\n#` @import shiny            # This is where you add the @import\r\nshinyUI <- function() {\r\n    selectInput(\"\")\r\n}\r\nImporting multiple packages: just don’t do it (UPDATED).\r\nJust a note to not import multiple packages using @import. As Hadley Wickham has noted, you have no control over the development of the packages you import. Just because there are no function collisions right now between the packages doesn’t mean that one of the developers may add a function down the line that might collide. So, if you need to use multiple packages, @import one package and use @importsFrom or namespaces to refer to functions in the other packages.\r\nExported versus Internal functions\r\nRemember I mentioned ‘@export’ above? Specifying in a docstring for your function exports it. Specifying it means that ‘devtools::document()’ will add an Export directive for the function in the NAMESPACE file. That means you can access it using ‘::’. Going back to our use_filter example:\r\n#' @export                              #This is where you add the @export\r\n#' @importFrom dplyr filter\r\nuse_filter <- function(df, cutoff=0.5) {\r\n\r\n  filter(df, value < cutoff)\r\n\r\n}\r\nIf our package name is mypackage, then this function will be accessible if we use library(mypackage) or mypackage::use_filter().\r\nWhy is this important? You may write some internal functions that are useful in your package, but they aren’t necessarily ones you want your users to use in their daily use. @export allows you to control which functions you make publicly accessible in your package.\r\nFor example, try typing dplyr:: in RStudio and hit the tab key. You’ll see the usual dplyr verbs pop up. But these are only the exported functions. Now try dplyr::: and hit the tab key. You’ll see a list of functions that pop up that’s much longer - these are all the functions, including the internal functions.\r\nSo, if there’s a cool bit of internal code you want to use in a package, you can use ‘:::’ to specify it. Just be aware that oftentimes, internal functions may change a lot as code gets refactored, so code that utilizes them may be refactored.\r\nRemember to run devtools::document()\r\nOnce you’ve written code and want to test it in your package, remember to run devtools::document() before you reinstall your package for testing. Otherwise, the NAMESPACE file won’t be modified, and your code won’t work.\r\nYou can also modify your Project Options for your package and check the Tools >> Project Options >> Build Tools >> Generate Documentation with Roxygen box. Next to it, there is also a Configure button that lets you select the option to run devtools::document() whenever you build a package for testing.\r\nGo forth and Package!\r\n\r\nTL;DR:\r\nUse usethis::use_package() to add packages to your DESCRIPTION file (while in the console) after you’ve created your package skeleton using use_package().\r\nUse namespaces (such as mypackage::usefilter()) to refer to external functions where possible to avoid collisions in function names when you are writing code.\r\nUse @imports and @importsFrom judiciously in your roxygen documentation for a function if you need to use many extenral package functions within a function. Use only 1 @import statement in a function, use namespaces/@importsFrom for the other packages.\r\nExpose functions you want to be made public using @export in your roxygen documentation.\r\nRemember to run devtools::document() in the console after you modify/add these fields to the roxygen documentation when you build for testing, or set it up in Tools >> Project Options.\r\nI hope this helps you to understand exactly the relationships between all of the components that are responsible for accessing namespaces in your package. I was confused about this for years, so writing this has helped me understand these relationships.\r\nFurther Reading\r\nIf you want to know how R searches for a function across its environments and why namespaces are a good thing, this is an excellent writeup: http://blog.obeautifulcode.com/R/How-R-Searches-And-Finds-Stuff/\r\nMore about the usethis workflow: https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/\r\nMore about the DESCRIPTION file: http://r-pkgs.had.co.nz/description.html\r\nMore about namespaces: http://r-pkgs.had.co.nz/namespace.html\r\nMore about roxygen: http://r-pkgs.had.co.nz/man.html#roxygen-comments\r\nAn alternative workflow to modify the DESCRIPTION file: You can use attachment::att_to_description() to scan code and add packages to the file after coding (thanks, Sébastien!).\r\nAcknowledgements\r\nThanks to Hao Ye, Sébastien Rochette, Michael Chirico, Tim Hesterberg, and Hadley Wickham for their comments and questions. I’ve incorporated your suggestions.\r\nNote\r\nI want this to be clear and correct. Please email me if there are any mistakes I’ve made.\r\n\r\n\r\n\r\n",
    "preview": "posts/2019-02-12-package-building-how-description-namespace-roxygen-and-devtools-document-work-together/imports-workflow.png",
    "last_modified": "2021-03-19T20:07:58-07:00",
    "input_file": {},
    "preview_width": 477,
    "preview_height": 767
  },
  {
    "path": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/",
    "title": "Rstudio Conf 2019: Education and Organizations",
    "description": "Some notes about RStudioConf 2019.",
    "author": [],
    "date": "2019-01-24",
    "categories": [
      "conference"
    ],
    "contents": "\r\n\r\n\r\nknitr::include_graphics(\"poster_laderast.png\")\r\n\r\n\r\n\r\n\r\nWell, RStudio Conf 2019 has come and gone. I attended the main conference, starting with the poster session on wednesday and stayed through the tidyverse developer day on Saturday.\r\nTo say that the conference was inspiring was an understatement. So many talented people working on such interesting and inspiring packages! It made me excited again about doing data science and teaching data science.\r\nThis post is going to highlight the interesting talks about education and organizational management at the conference. There were lots of thought provoking talks and great resources about education. Here’s a link to the other resources, posters, and talks (thanks, Karl Broman!). There are a lot of new neat whiz bang features in RStudio, which I won’t cover, but are covered very well by others at Karl’s link above.\r\nUPDATE: I’m adding links to the videos as subheadings that you can click on. I won’t embed these directly because there is often related material with the video.\r\nAvailable Teaching Resources\r\nThere was much sharing of educational resources. I’ll talk about a few.\r\nCarl Howe’s Talk: The Next Million R Users\r\nMel Gregory: RStudio for Education\r\nCarl Howe (Director of Education for RStudio) presented a number of resources that RStudio is providing for educational purposes. Their goal is to train the next million R users. Here are some of the RStudio based educational resources that might be useful for everyone.\r\nRStudio Cloud is now available free for people who teach courses, who can send them a course syllabus to gain access. RStudio Cloud includes resources to make coursework publically or privately available, allows instructors to install default packages, and gives students an immediate way to start playing with R/RStudio. We are definitely going to use this in teaching our BMI 569/669 Data Analytics course. Mel Gregory’s talk was about the nuts and bolts of using RStudio Cloud in a classroom situation. Highly recommended to get familiar with the basics.\r\nMine Çetinkaya-Rundel also has a Data Science In a Box course available that can be forked and used by anyone. There is a great discussion of how to use the course and the tech stack (RStudio Cloud, GitHub, and Slack) needed to make the course runnable.\r\nThere are also a number of LearnR Tutorials built into RStudio Cloud called RStudio Primers that cover a lot of basic RStudio operations for the self-directed learners.\r\nJessica Minnier and I presented at the poster session about our LearnR tutorials, DSIExplore and dataLiteracyTutorial and our burro package, which lets users explore a new dataset with a simplified Shiny App. There’s even a function that lets you build a shiny app from a dataset that you can publish to a Shiny server such as shinyapps.io or RStudio Connect for sharing and having a data scavenger hunt together. Or as Angela Bassa calls it, an EDA Party. Whoo!\r\nIrene Stevens’ Talk: Teaching Data Science With Puzzles\r\nI really loved Irene Stevens’ talk about Teaching Data Science Using Puzzles. Irene and Jenny Bryan put together a simple project-based framework that lets students download simple data wrangling puzzles per week, lets them submit their answer to an answer server and get feedback, and when they solve it, it creates a minimal reproducible example (reprex) to share with their puzzle community. Very Slick!\r\nKelly Bodwin: Introductory Statistics with R\r\nKelly Bodwin gave a presentation of how she used Shiny apps as an intermediate step as part of an approach to teaching introductory statistics. Kelly is of the opinion that coding is good even for non majors, and her Shiny apps provide an intermediate step between a more click-based app and coding, asking the students to input their variables, their hypothesis, and highlights the output that results. I really liked this approach. I also really loved this slide:\r\n\r\nDiversity and Inclusiveness Make for Better Data Science\r\nJesse Mostipak: R4DS online learning community: Improvements to self-taught data science & the critical need for diversity, equity, and inclusion in data science education\r\nI really enjoyed Jesse Mostipak’s talk about her data science journey starting the R For Data Science (R4DS) learning community. She especially emphasized that data science was her path out of lower incomes and that she thought it needs to be more accessible to all. She talked about the barriers to going from “Using your computer for Netflix” to “Using your computer for data science” as real, and what she learned as a community manager for the R4DS learning community. Finally, she ended with a call for data scientists to share their mistakes, in order to model that making mistakes is ok.\r\nCaitlin Hudon: Learning from Eight Years of Data Science Mistakes\r\nSpeaking of mistakes, Caitlin Hudon gave a talk called “Learning from Eight Years of Data Science Mistakes”, which was really helpful. One of her messages was that “Mistakes count as experience”, and learning from past mistakes is extremely helpful for the whole team. However, it must be safe for team members to share their mistakes, and so creating the right culture of respect and teaching is important. This includes documentation, and including pseudocode next to the actual code to talk non-data scientists through is important. Likewise, making sure that you are solving the correct business problem requires communication. I really liked her discussion of the Rhetorical Triangle as a way to frame communications: Speaker, Audience, and Context.\r\nTracy Teal: Teaching R using inclusive pedagogy: Practices and lessons learned from over 700 Carpentries workshops\r\nTracy Teal from Data Carpentries also talked about their efforts to Teaching R using inclusive pedagogy, including fostering a growth mindset. Their instructor training is a wonderful 2-day crash course in teaching, and I’m going to propose that it be offered to our graduate students to prep them for teaching. (Full disclosure: I’ve contributed to these course materials.) I also appreciated how candid she was about how much further they need to go to increase the diversity of participants.\r\nDiversity and Inclusiveness was also a theme of Angela Bassa’s talk about Team Data Science. She pointed out that data science teams need to be diverse to serve audiences who are highly diverse.\r\nMaking Teaching Resources for Community College Instructors\r\nMary Rudis: Catching the R wave: How R and RStudio are revolutionizing statistics education in community colleges (and beyond)\r\nA really interesting talk for me was Mary Rudis’ talk about her work teaching data science at the community college level. As head of the math department at Penn State Harrisburg, Mary has done a lot of work developing the first data science certification at the community college level. There are a lot more people attending community college than 4 year colleges, and we need to make paths to data science accessible for these students.\r\nA lot more work needs to be done to make data science accessible at this level. Mary has pointed out a number of efforts that need help, including StatPREP and the American Mathematical Association of Two Year Colleges. She also ended with a call for Shiny Developers to help Community College instructors develop tools they can use to teach data science.\r\nOrganizational Considerations in Data Science\r\nJoe Cheng: Shiny in Production\r\nTonya Filz: The Resilient R Champion\r\nOrganizational considerations were also a really interesting part of RStudio Conf for me. Joe Cheng’s keynote on Shiny in Production spent some time talking about overcoming resistance to Shiny in your organization, including talking with Data Engineers and IT Security. His point was that oftentimes, resistance comes from the feelings of territorialism and how to overcome them. Tonya Filz also gave a talk about being a Resilient R Champion at your organization, stressing these issues and giving us case studies/examples to show leaders at your organization.\r\nHilary Parker: Cultivating creativity in Data work\r\nHilary Parker’s talk about Creativity and Data Science in the Organization, about applying system design principles to data science. Rather than thinking of ourselves as having a traditional role at the end of the analysis, Hilary suggested that we partipate in all parts of the process, including data collection. Her group suggested a change in an app, which resulted in a brand new data stream for them to analyse.\r\nAngela Bassa: Data science as a team sport\r\nAngela Bassa’s Talk about Team Data Science talked about the organizational aspects of making a data team, including specializing roles, making sure that knowledge and expertise was distributed, and managing burnout by making sure that no one team member is overloaded. She stressed resilience within a team by making sure that people are taking care of themselves, having redundancy in the team. Process is also important, by making sure data is well documented, by allotting time. She also stressed the importance of young and new data scientists to the team, because they aren’t biased with previous knowledge. I also thought she had excellent arguments about the cost of not expanding your data team - potential users who would be lost.\r\nEncouraging a Growth Mindset\r\nPanel: Growth & change of careers, organizations and responsibility in data science\r\nI really enjoyed the final panel discussion, which was about encouraging a Growth Mindset for Data Scientists in their careers. Karthik Ram, Tracy Teal, Angela Bassa, and Hilary Parker all fielded questions about growing as a data scientist. One of my favorite remarks was from Tracy Teal, who mused about the possibility of a Leadership Carpentry, teaching potential leaders essential skills such as mentoring, faciliation skills, and valuing team contributions. Angela Bassa also had a great quote that “leadership is programming people”. She also pointed out that the role of Data Scientists is to “Question Dogma” and pointed out the importance of saying “I don’t know”.\r\nDavid Robinson: The Unreasonable Effectiveness of Public Work\r\nMaking your work publicThe growth mindset was also a theme of Dave Robinson’s Keynote, which was about making your data science work public at a number of levels. He outlined a number of different types of public contributions, such as short form (tweets, discussions about data), long form (blogs), code contributions, and even writing a book.\r\nDirect Explicit Instruction in Education\r\nFelienne Hermans: Explicit Direct Instruction in Education\r\nFelienne Hermans gave a wonderful keynote about teaching computer programming. I won’t crib her story, because her telling it was a delightful journey and I encourage you to watch it for yourself at the link above. Part of her point was that programming pedagogy is dominated by the “tyranny of fun”, rather than focusing on teaching fundamentals. One of the problems with programming education is that we haven’t defined what these fundamentals are, compared to the pedagogy of reading, which talks about teaching with fundamentals such as phonics.\r\nWe expect students to explore things without teaching them enough, and that actually hurts their progress. Instead, we should also be teaching fundamentals, such as code reading out loud and rote memorization of programming patterns. By utilizing these fundamentals, she has found improved learning outcomes. As she says, motivation comes from seeing progress, not necessarily from exploration alone. Her results on using direct instruction have showed improved outcomes in terms of motivation.\r\nI have to admit that this talk really opened my eyes to the blind spots we have in programming education. Looking forward to incorporating these ideas in my coursework.\r\nSo Many People to Teach, So Many Ways to Teach!\r\nSo that’s my summary about the education and organizational talks/posters at RStudio Conf. Hopefully it gives you an idea of what resources, opportunities, and pedagogy is out there.\r\n\r\n\r\n\r\n",
    "preview": "posts/2019-01-24-rstudio-conf-2019-education-and-organizations/poster_laderast.png",
    "last_modified": "2021-03-19T20:04:16-07:00",
    "input_file": {},
    "preview_width": 1067,
    "preview_height": 801
  },
  {
    "path": "posts/2018-12-13-reframing-impostor-syndrome-as-being-on-the-road-to-mastery/",
    "title": "Reframing Impostor Syndrome as the Road to Mastery",
    "description": "Some thoughts about impostor syndrome.",
    "author": [],
    "date": "2018-12-13",
    "categories": [],
    "contents": "\r\nAt some point in your life in Data Science, you will probably struggle with impostor syndrome. We all do - in fact, even though I have used R and have done bioinformatics and data science for more than 15 years, I still struggle with this feeling. As a beginner, the mountain you must climb to master skills in data science seems like a long and impossible one.\r\nCaitlin Hudon, in her post about dealing with impostor syndrome has this to say about countering impostor syndrome:\r\n\r\nThe way that I’ve dealt with imposter syndrome is this: I’ve accepted that I will never be able to learn everything there is to know in data science — I will never know every algorithm, every technology, every cool package, or even every language — and that’s okay. The great thing about being in such a diverse field is that nobody will know all of these things (and that’s okay too!).\r\n\r\nI think it’s important to try and reframe the feelings of impostor syndrome into something more positive. I think having self-compassion about the difficulties of the learning process can help.\r\nGeorge Leonard’s Mastery is a short book that I think can help provide the antidote to these feelings of fraud and inadequacy. I feel that beginners and learners would feel much better if their instructors would own up to their own personal shortcomings as learners. That is, instead of trying to project the image of the all knowledgable guru, instructors should show themselves as humble, lifelong learners as well.\r\nIn Mastery, Leonard talks about our unrealistic expectations and how these expectations can get in the way of actually learning and mastering a craft. We are conditioned by ads, movies, and social media that mastering a craft is a never ending set of ever rising climaxes (cue the training montage), that we can make continuous and steady progress by working hard enough.\r\nMastering a craft takes practice, and lots of it. We must learn to be contented to practice when we are on a plateau and are not making visible progress. As Leonard notes,\r\n\r\nThe Path to Mastery is practice.\r\n\r\nLeonard outlines 5 principles that can sustain us in our road to mastery and away from impostor syndrome: Instruction, Practice, Surrender, Intentionality, and The Edge. I’m trying to map common feelings of impostor syndrome and show how these principles can counteract these feelings.\r\nInstruction. Leonard emphasizes the importance in finding good instructors and good mentorship that will help us to grow. Finding good instructors can actually be difficult and finding someone who remembers what it was like to be learning something is important. Avoid those instructors who say things like “it should now be obvious” or are disparaging when you don’t understand something.\r\n\r\nHe or she is not necessarily the one that gives the most polished lectures, but rather the one who has discovered how to involve each student actively in the process of learning.\r\n\r\nWhat you can do today: look at twitter and other forums for your community of learners and support those who give good instruction. Realize that not all teachers are good teachers; leave them and seek better ones if necessary.\r\nPractice. Practice for practices’ sake. Deliberate practice where you slowly build up your understanding and perceptions is important to your growth.\r\n\r\nWhere in our upbringing, our schooling, our career are we taught to value, to enjoy, even to love the plateau, the long stretch of diligent effort with no seeming progress?\r\n\r\nWhat you can do today: Join communities of practice such as Tidy Tuesday and share your learning with others. Tidy Tuesday is extremely friendly and encouraging for beginners. Learn together and grow together. These are safe communities to share knowledge.\r\nIntentionality. This goes hand in hand with practice. Deliberate practice requires visualizing your process and guiding yourself gently.\r\n\r\nIntentionality fuels the master’s journey. Every master is a master of vision.\r\n\r\nWhat you can do today: find small exercises and projects that help you reinforce what you’ve learned so far. Find people’s code and vignettes and modify them until you understand what they’ve done and how they structured their work.\r\nSurrender. At some point, you will have to give up your own social position as an expert to grow as a learner. When this happens, you must be willing to risk that standing to progress further. Leonard talks about a karate master learning aikido who was not willing to start from scratch, which impeded his learning. For many of us academics, being willing to abandon the comfort of what we have learned is especially difficult. We feel like we are risking our own social standing and reputation.\r\n\r\nFor the master, surrender means there are no experts. There are only Learners.\r\n\r\nWhat you can do today: be humble when faced with new concepts (for many impostor syndrome sufferers this is not the hard part). Recognize when you need to grow and when you have to leave old concepts behind.\r\nThe Edge. This is where things are undefined and scary. Still, part of the journey to mastery is a willingness to push your thoughts to beyond the horizon of what you thought was possible.\r\n\r\nThe trick here is not only to test the edges of the envelope, but also to walk the fine line between endless, goalless practice and those alluring roles that appear along the way.\r\n\r\nWhat you can do today: Identify some goals that are just beyond your current skillset and be willing to push your learning to that point.\r\nLeonard maintains that true mastery is not due to innate talent. True mastery is due to tenacity and perserverance in the face of difficult learning. In fact, he suggests that learning things too easily means that you might lack perserverance when the going gets rough and your progress slows. He maintains that someone who perseveres will “have learned whatever [they] are practicing to the marrow of [their] bones.”\r\nEncouraging Mastery as a Community\r\nI think Caitlin’s prescriptions for community wide suggestions for reducing impostor syndrome are wonderful. Especially the advice to “Get comfortable with I don’t know”. Normalizing “I don’t know” within a community is incredibly important to making a psychologically safe learning environment.\r\nTo encourage learners, I think that creating a community of practice and helpfulness is vitally important to give new learners the support they need. When communities take responsibility for the learning of their members, something magic happens. Learning no longer feels lonely and there is no shame when you don’t immediately grasp a concept. Patience becomes the norm and people become more confident.\r\nFor me, this is the true value of schools and universities. To get the most out of online learning, you need to participate within a community that encourages you to learn further. Be on the pathway to mastery by participate within learning communities.\r\nFurther Reading\r\nMastery: https://www.goodreads.com/book/show/81940.Mastery\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:58:26-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-15-clinical-data-wrangling/",
    "title": "Things we learned teaching clinical data wrangling",
    "description": "More information about managing depression and anxiety.",
    "author": [
      {
        "name": "Eilis Boudreau, Ted Laderas, and Nicole Weiskopf",
        "url": {}
      }
    ],
    "date": "2018-10-15",
    "categories": [
      "visualization",
      "R"
    ],
    "contents": "\r\n\r\n\r\n\r\nWell, we just finished our clinical data wrangling workshop. This was a 12 hour workshop (spread over 4 days) where students got to work with a real research dataset (the Sleep Heart Health Study data). This is a workshop that we developed as part of an National Library of Medicine T15 training supplement in Data Science. The following is a short report describing the workshop and its outcomes.\r\nIntended Audience\r\nWe designed the workshop for our incoming informatics students (both clinical and biological majors) in order to introduce them to the difficulties of working with clinical data.\r\nLearning Objectives\r\nThese were our learning objectives for the workshop:\r\nUnderstand the biology of sleep and sleep apnea and how the biology informs the covariates measured in the Sleep Heart Health Study\r\nUnderstand why clinical data is useful and also why it’s difficult to work with\r\nLearn Exploratory Data Analysis techniques and use them to inform model building.\r\nLearn to assess logistic regression models using simple diagnostics.\r\nThe Dataset\r\nWe used the Sleep Heart Health Study dataset from the National Sleep Research Resource. This is a dataset of approximately 5800 patients that have over 3000 covariates. We limited our students to a smaller number of covariates (17), including our outcome of interest, cardiovascular disease.\r\nWorkshop Format\r\nWe designed the workshop to be a mix of didactic lectures and active learning exercises. Where possible, we had students work in groups to answer questions about the data. These activities included a data scavenger hunt using our EDA exploration app, and a logistic modeling exercise.\r\nDay 1 Outline\r\nIntroduction, logistics, and groups assigned (30 minutes)\r\nBiology of Sleep and Cardiovascular Disease (40 minutes, format: in-person lecture)\r\nBreak Time (15 minutes)\r\nThe Value of Clinical Data (15 minutes, in-person lecture)\r\nClinical Data Quality (40 minutes, in-person lecture)\r\nLunch (90 minutes, with optional R setup session)\r\nExploring the SHHS Dataset (60 minutes, format: Data Scavenger Hunt w/ Shiny App, each team gets a task and has to show the class how to find the information)\r\nApplying the Clinical Wrangling Process: Diabetes (45 minutes, format: in-person lecture)\r\nLogistic Regression Model Basics (60 minutes, format: walkthrough of R Notebook)\r\nDay 2\r\nQuestion/Answer session about Logistic Regression and Modeling (50 minutes)\r\nAssignment about race variable (assigned to groups, take-home assignment)\r\nDay 3\r\nDiscussion about race as a covariate, sharing of findings\r\nOverview of hypertension and how it relates to cardiovascular disease and sleep apnea\r\nTemplate/R Notebook given for final presentation in groups (in-class lab time, template is structured as a series of decisions.)\r\nDay 4\r\nGroup presentations about covariate decisions and resulting model (1 hour, present final version of R notebook). At each decision stage, teams must decide on whether or not to include covariates or not given what they have found from exploring the data and justify their decision using EDA visualizations.\r\nLessons Learned\r\nOverall, we believe the workshop went well, as it encouraged discussion about data and its appropriateness among the students. Students were engaged overall and asked lots of questions.\r\nThe final reports for each group were generated from a R Notebook. All three groups showed a thoughtful narrative and justification for each of the covariates included in the model.\r\nInteractive visualization removes barriers to understanding issues in data. Ted developed a Shiny App that allowed the students to visually browse and understand the data. Along with the EDA scavenger hunt (see below), this served as a good introduction for students to get their feet wet with the SHHS dataset.\r\nOur diverse backgrounds helped make the workshop accessible. Nicole Weiskopf has a background in data quality of clinical data, Eilis Boudreau does sleep study work, and I’m a bit of a mongrel.\r\nSecuring the cooperation of the data holders made the workshop possible. The dataset comes from the National Sleep Study Resource. Eilis knows Susan Redline, who heads that group and pitched the idea (over two sessions) to her group. Susan’s group was very enthusiastic and helpful, especially in helping the students get their data use agreements in so they could access the dataset.\r\nGroup work is learning work. We assigned each student to a group, and gave each group questions to answer and teach the class about the dataset. By pointing them to specific aspects of the data, we opened the door to discussion.\r\nEDA scavenger hunt. We had the students learn data exploration by giving them a scavenger hunt to look at the relationship between variables. Each group was then required to talk about their findings and which visualization helped them discover that relationship. For example, there is a relationship between age and race in our dataset; the “Other” category of race has a lower median age than the other two categories, “White” and “Black”.\r\nDidactic Teaching is also important. Nicole and Eilis covered both the biology of sleep apnea and the difficulty of understanding the implications of clinical data. Without this background, students would not be able to make informed decisions about their final model.\r\nGuide the Students, but don’t force discussion. This was important. We think the students need to connect the dots to really understand the issues. The final product (a logistic regression model predicting cardiovascular disease with an R Notebook) had steps and choices. But the choices for each group of students was different.\r\nA Code of conduct is necessary and important. We are big believers in psychological safety. If people don’t feel safe in the classroom environment (and let’s face it, grad school classrooms rarely are), they will be less likely to learn and contribute.\r\nData restrictions made deploying difficult. The activity materials were deployed as an RStudio project. However, we couldn’t share the data within a GitHub repo. As OHSU’s approved vendor is Box, we setup a box folder containing the material to be shared with students.\r\nWe were grateful for the incoming informatics students’ enthusiasm and patience as we got this workshop going. Also thanks to the NLM T15 Supplement in Data Science, without which we would not have gotten the opportunity to conceptualize, put together, and deliver this workshop. Thanks again to Susan Redline and the National Sleep Research Resource group, especially Dan Mobley who helped us with the last-minute data use agreements.\r\nLink to Workshop: https://github.com/laderast/clinical_data_wrangling\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-10-15-clinical-data-wrangling/appview.jpg",
    "last_modified": "2021-03-19T19:56:56-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-07-shiny-and-tidyeval/",
    "title": "Shiny and Tidyeval Part 1",
    "description": "How do you incorporate tidyeval and shiny together?",
    "author": [],
    "date": "2018-08-07",
    "categories": [
      "shiny"
    ],
    "contents": "\r\n\r\nNote: sometimes I write these posts to teach myself a better way to do things in R.\r\n\r\nI have been avoiding tidyeval somewhat, because I seem to have a bit of a learning block about it. I’m going to try to write some posts that help me understand what’s going on with Tidy Evaluation.\r\nUsing sym() in a Shiny App\r\nOne fairly simple Shiny Application might be selecting a column of the dataset and then doing something with it, such as using it in a select() or filter() statement. Say we had a simple app to produce histograms, and we wanted to change the column that is being displayed on the histogram.\r\nTry this app out by running the following command. The code is here.\r\n\r\n\r\nrunGist(\"https://gist.github.com/laderast/a5205554324306e642b2df9f80ed6409\", display.mode=\"showcase\")\r\n\r\n\r\n\r\nOur input is a select input called numeric_var, which returns a single column name as a character In our server logic, we’ve built a reactive called selected_data, which returns the selected column as a vector using pull().\r\n\r\n\r\n  selected_data <- reactive({\r\n    ## input$numeric_var is a character, so we cast it to symbol\r\n    var_name <- sym(input$numeric_var)\r\n\r\n    ## Now we evaluate it with !!\r\n    out_col <- iris %>% pull(!!var_name)\r\n  })\r\n\r\n\r\n\r\nThe question is: how do we pass the input value into pull()? We first have to use rlang::sym() to pass our character in as a symbol that we’re calling var_name. But the issue is that our reactive doesn’t know which environment to look in.\r\nWe want our reactive to look for the column name within the environment of the iris tibble. This is where the !! (bang-bang) comes in. It says, ‘look for the value’ within the tibble.\r\nUsing syms() in a Shiny App\r\nWhat if wanted to pass in multiple variables from a select box? We’ll need to wrap our input with syms(), which takes a list.\r\nLet’s do a slightly different version where we’re visualizing a box plot and we want to select multiple columns to display in our dataset from a selectInput where we’ve specified the multiple=TRUE argument.\r\nOur setup is similar, but different. Because we have multiple values, we have to use syms() to wrap the input from input$numeric_vars. Then we can evaluate it with !!! (the triple bang).\r\n\r\n\r\n  selected_data <- reactive({\r\n    ## input$numeric_var is a character vector, so we cast it to symbol\r\n    var_list <- syms(input$numeric_vars)\r\n\r\n    ## Now we evaluate it with !!!\r\n    out_col <- iris %>% select(!!!var_list)\r\n  })\r\n\r\n\r\n\r\nTry this app out. The Code is here.\r\n\r\n\r\nrunGist(\"https://gist.github.com/laderast/952120ac46d1f27c2d2dba5bd1ab5d10\", display.mode=\"showcase\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:54:30-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-02-turning-down-the-noise/",
    "title": "Turning down the noise",
    "description": "More information about managing depression and anxiety.",
    "author": [],
    "date": "2018-08-02",
    "categories": [],
    "contents": "\r\nI’m still in the process of recovering from my current bout of depression and anxiety. I’d like to talk about what is currently helping me moderate my anxiety. I have been practicing mindfulness and meditation for the past three years and I’m beginning to realize how necessary it is in our information dense age. Many of my symptoms of anxiety are really from an information overglut.\r\nI’m currently on way too many projects and am teaching as well. Everything wants my attention. We need to decide dates for a visit, etc. Booking travel, grading students, etc. The noise of academic life can be overwhelming and can prevent me from working effectively. A book I’m reading, Real Happiness at Work by Sharon Salzberg, talks about Attention Deficit Trait disorder:\r\n\r\nAttention deficit trait (ADT) is workplace-induced attention deficit caused by the constant, relentless input of information, these days usually enabled by our high-tech devices, smartphones, and computers.\r\n\r\nThis is especially prevalent for people like me who are on multiple grants and have many collaborators. I’m okay with putting out the occasional fire or dealing with an emergency deadline; if I believe in the project, I can muster the energy. However, the problem is when I have multiple fires to deal with from multiple people. The task switching leads to stress and leads to an inability to prioritize. This is where I’ve been the last few months.\r\nAnd this is when the voices of doubt begin to fuel my anxiety. On top of the enormous task list, there’s the feelings of failure and disappointment because I can’t get simple things done. In my head the voices reach a frenzy, a cacophony, a noise. And then I can’t think straight, prioritize, work on one thing.\r\nWhat is the solution to ADT and the noise of life? Mindfulness and unitasking. As Sharon Salzberg notes:\r\n\r\n… while it’s unrealistic to try to stop the number and variety of incoming demands, in our technologically advanced world it is possible to modulate how much information we’re taking in, and how many tasks we are doing at once. When we slow down and concentrate on doing just what is before us to be done now, we become the masters of our own environment rather than its frantic slaves.\r\n\r\nThis idea (being a master of my environment) appeals to me in the midst of my academic anxiety. Practicing mindfulness (through daily meditation) helps me to focus on the here and now. We are wired to ruminated about the past (things we wish we’d done better) or the future (oh crap, I need to prepare upcoming stuff). Meditation is all about building that focus and attention on what’s before us. If we can’t do it all, we can at least work on what’s right in front of us.\r\nI’ve also been reading Anne Lamott’s Bird by Bird, a book reflecting on the hows and whys of writing and living your life while doing it. It’s a sobering view of why we write and how to keep on in the face of numerous adversities. There’s one passage that really resonated with me in the chapter called “Shitty First Drafts” in dealing with the incessant chatter of life:\r\n\r\nClose your eyes and get quiet for a minute, until the chatter starts up. Then isolate one of the voices and imagine the person speaking as a mouse. Pick it up by the tail and drop it into a mason jar. Then isolate another voice, pick it up by the tail, drop it in the jar. And so on. Drop any high maintenance parental units, drop in any contractors, lawyers, colleagues, children, anyone else who is whining in your head. Then put the lid on, and watch all these mouse people clawing at the glass, jabbering away, trying to make you feel like shit because you won’t do what they want - won’t give them more money, won’t be more successful, won’t see them more often. Then imagine there is a volume control button on the bottle. Turn it all the way up for a minute, and listen to the stream of angry, neglected, guilt-mongering voices. Then turn it all the way down and watch the frantic mice lunge at the glass, trying to get to you. Leave it down, and get back to your shitty first draft.\r\n\r\nThis is another idea in mindfulness: welcoming and acknowledging our worries and negative voices. If we ignore or refuse these feelings, they just become stronger and louder in the din and the noise. I have all sorts of these feelings: I don’t work hard enough, I’m a failure, I’m letting down people who depend on me, Everyone is out there working on cooler things than me. Now I’m trying to take an effort to welcome these feelings into my mental space, saying “okay, I hear you and acknowledge you, so you don’t have to be yelling anymore”. And surprisingly, they do quiet down. Acknowledging the feelings of failure, thanking them for their contribution to the discussion and showing them the door is important. As Kelly Boys says in The Blind Spot Effect:\r\n\r\nIf you approach life with a willingness to be with what you encounter without getting lost in it, it moves gracefully within your experience.\r\n\r\nAnd that helps lower the volume and free your attention.\r\nIf you are feeling generalized anxiety or attention deficit trait, I encourage you to look into mindfulness as a way of turning down the noise.\r\nResources\r\nThese are some of the resources that I’ve used when writing this post.\r\nThe Mindful Geek: Secular Meditation for Smart Skeptics by Michael Taft. This is a great book for people who are interested in Mindfulness and the psychological and neuroscience research about why mindfulness works in reducing depression and anxiety. I started here. Michael Taft calls meditation “a technology for hacking the human wetware to improve your life”.\r\nReal Happiness at Work: Meditations for Accomplishment, Achievement, and Peace by Sharon Salzberg. This is a nice followup to Mindful Geek, talking about how we can regain control in our workplace.\r\nThe Blind Spot Effect: How to Stop Missing What’s Right in Front of You by Kelly Boys. I am really liking this book so far. It’s about how meditation and mindfulness can help us find our cognitive blind spots and move beyond them.\r\nBird by Bird: Some Instructions on Writing and Life by Anne Lamott. I recently started reading this on vacation. Writing can be a lonely and solitary life, and I find many of her suggestions about living to apply equally well to research.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:52:26-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-25-asking-for-help-to-get-better/",
    "title": "Asking for help to get better",
    "description": "Learning more about managing burnout.",
    "author": [],
    "date": "2018-06-25",
    "categories": [],
    "contents": "\r\nI want to thank everyone who has reached out to me after I wrote my post on struggling with my depression and self-care. I am incredibly grateful for everyone’s concern about me. I wrote that at a low point in my life because I had to. I was suffering too long in silence, and I needed to do something. Writing that post was incredibly scary. I am still worried that it may be used against me somehow down the line when I am reviewed for tenure. But the truth is that I’m not a productivity machine; I can be extremely dependable in very uncertain situations, but to do that I need support.\r\nI wanted to start the conversation because mental health is not something we talk about much in academia. There is still a lot of bias and stigma against those with mental illness, that they are “weak”, or that they should “get out of academia”. This stigma prevents a lot of students and postdocs from seeking the treatment they need. I just want to say this to everyone who is struggles with mental illness: you aren’t weak, and you aren’t alone. As far as I am concerned, each of you that manages to get through your day despite of your illness should be considered a hero. Because I know how hard it can be to even get out of bed some days. And I would like for PIs to consider that your overall neglect and/or bad advising can contribute to your student’s decline in mental health.\r\nThank you to all my students, my colleagues, alumni, and everyone in my department (including my department head and division head) who are supporting me. Your kindness and sharing your stories about your struggles really has meant a lot to me. I’ve come to realize that I have an amazing support network and that I just need to ask for help. I am slowly figuring out what that help is. I’m going to 0.8 time for a while to give me more breathing room. I’m exhausted and I just need some time to replenish my energy and reflect where I want to go next.\r\nI have come to realize that saying yes when I shouldn’t comes at a cost to myself and my mental health. Everything I do comes at a cost; I need to have a better gauge of when that cost is worth it, and how much mental reserve I have in my tanks. When my collaborators don’t understand the amount of work I do cleaning their data, when they don’t respect the work I have done managing their data and reconciling the issues with data collection, that’s when I feel like working in research is not worth it.\r\nOne of my industry collaborations was like that, and it pretty much drained nearly all of my energy. I don’t like working with people who expect me to give them an answer and aren’t willing to educate me about what I’m supposed to be looking for. I also don’t like working with people who take advantage of my work in open science to further their own work agenda. I need less of these kind of collaborations, and I need the gumption to walk away from them.\r\nI just want to say this: I am not a rock. I am not strong. But I am a good advocate for others, especially those I feel who have been dealt an unfair hand. I realize that I need to advocate for myself right now. Thank you to other academics such as Dr. Emily Hencken Ritter, who wrote about her struggles with depression and what help she asked for. I’m going to highlight posts about this using the hashtag #DepressionInAcademia on Twitter.\r\nIn short: I’m not going to get better overnight, but I’m trying to take steps to get better. I’ll be taking a little time off this summer as well to recharge and figure out what’s next. I’ll try to chronicle what works for me and to highlight other people’s writings about this.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:47:29-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-09-cascadia-r-2018-how-we-planned-it/",
    "title": "Cascadia-R 2018: How we planned it and the reasons why",
    "description": "More information about the 2018 Cascadia-R conference.",
    "author": [],
    "date": "2018-06-09",
    "categories": [
      "conference"
    ],
    "contents": "\r\n\r\n\r\n\r\nWell, Cascadia-R 2018 has come and gone. This year we tried our best to make it as inclusive, welcoming, and friendly as we could. Considering we had 224 participants this time around, I’d say it was a success.\r\nI just thought I would do a little write up of some of the things we did and why we did them in our conference. I’m hoping it will be useful for other conference planners to create a welcoming environment.\r\nWe created a pull plan based on what we learned last year. Because last year was pretty chaotic in terms of planning, we decided to try to make a pull plan this year. Pull planning is a project management technique in which you work backwards from absolute deadlines (from the day of the conference). Doing so spaces out a lot of the work and makes things potentially more dividable across a group. I’ve made our pull plan public so other conference planners can see what we did and the timing it took to organize the conference.\r\nFind sponsorship money. We ran last year on a shoestring, using only about $3500. We realized that to make the conference more inclusive this year, we had to get sponsorship money. Childcare and diversity scholarships cost money. We wanted sponsors who had values that aligned with our inclusion goals. Lilly Winfree was especially great at finding sponsors who were in tune with what we wanted to accomplish with our conference. Sponsorship also allowed us to pay for lunch for everyone, which we couldn’t last year.\r\nProvide childcare. We wanted parents to be able to attend, so we got DivCare to provide childcare for them in our building. DivCare provided childcare for 8 children, and we had some grateful parents who were happy they could attend.\r\nDiversity Committee. We did this last year, but we had more money this year for diversity scholarships for those who might not be able to attend. I think next year we might think about reaching out to more STEM groups, but I think the real challenge is finding how to get those diverse students who might not even know about data science in and how to properly host and support them.\r\nGet rid of longer form talks. Keynotes and Lightning talks only. This is probably the most controversial choice that we made. However, we realized that the 10-15 minute talks we had in 2017 were mostly academic ones. Honestly, there are already too many academic conferences out there. We wanted a format that was more accessible and encouraged discussion afterwards, so we stayed with the lightning talks.\r\nHave keynote speakers show newcomers how to join the R Community. We chose Kara Woo and Alison Hill as our keynote speakers for a very specific reason: we wanted to encourage people new to R that they could learn things. Alison gave a wonderful talk about ways to approach learning R; Kara gave a great talk talking about how to contribute to various R-projects on GitHub, especially the tidyverse suite of packages. What I really liked about both their talks was that they emphasized learning by doing.\r\nMoar workshops. We wanted beginners and intermediate people feel like they were learning something, and in a safe learning environment. One piece of feedback from last year was that people wanted more workshops. We created a beginner track and an intermediate track for workshops. I would just like to say that none of the workshop people got paid for their work, and I do wish that some of the attendees realized that before they provided feedback such as “BORING” (seriously? please realize that there are people who develop these, and constructive feedback is always better). All of these workshops were done for free as labors of love, unlike conferences like ODSC.\r\nOrganize Volunteers. We had lots of volunteers this year and we finally were able to figure out roles for everyone to do (registration, nametags, TAs, etc).\r\nDo a visualization fest that wasn’t competitive. I know that everyone loves bake-offs like the DREAM Challenges and Kaggle, but we felt that having a competition was in conflict with the community building we wanted to do with the conference.That, and Tidy Tuesday, were the inspirations for the cRaggy. We wanted people to share ideas with each other and to be constructive with each other. We had three great short talks talking about their approach to the visualization and we shared the data with Tidy Tuesday.\r\nTalk honestly about impostor syndrome. We had a panel with three people (the wonderful Paige Bailey, Kevin Watanabe-Smith and Lilly Winfree) and they all talked about how they deal with impostor syndrome. Some of the lessons are: you feel like less of an impostor the more you do and the more you participate.\r\nHave more downtime. We had feedback last year that there wasn’t enough social time. We built more breaks into the schedule and had a third room (the hack room) dedicated to the “hallway track”: often the best times we’ve had conferences is when we play hooky from conference activities. Some people suggested this year that there maybe was too much downtime. I would say to that, there’s always more opportunity to meet people.\r\nSo that’s why Cascadia R was the way it was this year. In another blog post, I’ll talk more about the fun things that happened this year and our hopes for the next year.\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-06-09-cascadia-r-2018-how-we-planned-it/pull-plan-cascadia-r.png",
    "last_modified": "2021-03-19T19:43:48-07:00",
    "input_file": {},
    "preview_width": 1121,
    "preview_height": 588
  },
  {
    "path": "posts/2018-06-06-self-care-and-self-compassion-in-academia/",
    "title": "Self Care and Self Compassion in Academia",
    "description": "Why self-care is important in academia.",
    "author": [],
    "date": "2018-06-05",
    "categories": [],
    "contents": "\r\n\r\nNote: I am not writing the following to complain, or excuse any past behavior. I am writing this just to be honest and transparent about my current struggles in academia. I hope it helps someone, or encourages other to seek help.\r\n\r\nI have to confess that I haven’t really been feeling all that well the past few months. Right now I am plagued with feelings that I am doing my work as an Assistant Professor wrong. I struggle with this persistent voice in my head that even when I am working at full tilt and beyond, that it’s not enough.\r\nFor most of my life, I have had chronic depression. I am a high functioning depressive; I have managed to get things done when I am depressed. However, this effort comes at the cost of self-care. I really think that some past posts have really come from a place of high stress and high anxiety. I apologize for the harsh tone of this post. I am also struggling with burnout at this point. I am beginning to feel like a lot of my efforts to encourage interdisciplinary collaborations are not feeling very productive. Part of this has to do with internal politics.\r\nI don’t think my depression is an excuse for my past behavior. The lack of self-care, however, is a major cause. When I don’t take care of myself, I reach a place of high stress and high anxiety and I can’t think straight. If I have been hurtful or bullying to others in any way because of this, I apologize.\r\nI still struggle with impostor syndrome. I do think that in many ways, this struggle has helped me become more compassionate as a teacher, remembering what concepts I struggled with. I have been trying my best to keep up with the added responsibilities of a faculty member: mentoring students, writing new course material, doing outreach, and also trying to keep the lights on by doing research (I won’t get into grant writing here; that’s another major depressive kettle of fish). I feel like I’ve currently been a disappointment as a faculty member so far.\r\nI am currently in therapy, which has been very helpful in understanding what problems I can address and what problems that are currently endemic to the academic system. I don’t know of any other field where one can accomplish a lot yet still feel like a failure.\r\nThe hardest thing for me is saying no, especially to students. But I realize that I can’t really be my best for students without taking care of myself. That balance between being considerate of others and also being considerate of my self is something that I am struggling with as a faculty member.\r\nI guess my point being is that I need to reframe saying no as being protective of myself and my current students rather than registering the disappointment when I say no. I’d like to help people, but I can help people better when I’m feeling well and better. Part of this may be coming to grips that academia may not be my place in the world if it demands that I sacrifice my mental health just to stay where I am.\r\nSo for the time being, I’d like to focus on getting better and feeling better. For your sake and mine. I will try to welcome newcomers to data science the best I can - just realize that what I may be able to do for you is a little limited right now.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:39:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-24-group-by-summarize-not-just-for-numeric-values/",
    "title": "group_by/summarize: Not just for numeric values!",
    "description": "Learn some more about the many uses of `group_by()`/`summarize()`.",
    "author": [],
    "date": "2018-05-24",
    "categories": [
      "R",
      "tidyverse"
    ],
    "contents": "\r\nEven though I’ve been using the tidyverse for a couple of years, there’s always a couple new applications of tidyverse verbs.\r\nThis one, in retrospect, is pretty simple. I had a one to many table that I wanted to collapse, tidy-style. Let’s look at the diamonds dataset:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>%\r\n  head() %>%\r\n  knitr::kable()\r\n\r\n\r\ncolor\r\ncut\r\nE\r\nIdeal\r\nE\r\nPremium\r\nE\r\nGood\r\nI\r\nPremium\r\nJ\r\nGood\r\nJ\r\nVery Good\r\n\r\nWhat if we wanted to collapse all the entries for each color into a single line? There’s 7 different colors, so we can use a combination of group_by on color and use the paste() function within summarize() to get what we want, which I’ve called all_colors here. By specifying the collapse argument, we can specify the delimiter within that column:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>% \r\n  group_by(color) %>% \r\n  summarize(all_colors=\r\n              paste(cut, collapse=\";\"))\r\n\r\n\r\n# A tibble: 7 x 2\r\n  color all_colors                                                    \r\n* <ord> <chr>                                                         \r\n1 D     Very Good;Very Good;Very Good;Good;Good;Premium;Premium;Ideal~\r\n2 E     Ideal;Premium;Good;Fair;Premium;Premium;Very Good;Very Good;V~\r\n3 F     Premium;Very Good;Very Good;Very Good;Good;Premium;Very Good;~\r\n4 G     Very Good;Ideal;Ideal;Very Good;Premium;Premium;Ideal;Very Go~\r\n5 H     Very Good;Very Good;Very Good;Good;Good;Very Good;Good;Very G~\r\n6 I     Premium;Very Good;Ideal;Good;Premium;Ideal;Ideal;Ideal;Ideal;~\r\n7 J     Good;Very Good;Good;Ideal;Ideal;Good;Good;Very Good;Very Good~\r\n\r\nThanks to Ken Butler, who pointed out that the tidyverse way (via stringr) is to use str_c instead:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>% \r\n  group_by(color) %>% \r\n  summarize(all_colors=\r\n              stringr::str_c(cut, collapse=\";\")) \r\n\r\n\r\n# A tibble: 7 x 2\r\n  color all_colors                                                    \r\n* <ord> <chr>                                                         \r\n1 D     Very Good;Very Good;Very Good;Good;Good;Premium;Premium;Ideal~\r\n2 E     Ideal;Premium;Good;Fair;Premium;Premium;Very Good;Very Good;V~\r\n3 F     Premium;Very Good;Very Good;Very Good;Good;Premium;Very Good;~\r\n4 G     Very Good;Ideal;Ideal;Very Good;Premium;Premium;Ideal;Very Go~\r\n5 H     Very Good;Very Good;Very Good;Good;Good;Very Good;Good;Very G~\r\n6 I     Premium;Very Good;Ideal;Good;Premium;Ideal;Ideal;Ideal;Ideal;~\r\n7 J     Good;Very Good;Good;Ideal;Ideal;Good;Good;Very Good;Very Good~\r\n\r\nFinally, if we wanted to just get the unique values of the cuts in a single line, we can use unique:\r\n\r\n\r\ndiamonds %>% select(color, cut) %>% \r\n  group_by(color) %>% \r\n  summarize(all_colors=\r\n              paste(unique(cut), collapse=\";\")) \r\n\r\n\r\n# A tibble: 7 x 2\r\n  color all_colors                       \r\n* <ord> <chr>                            \r\n1 D     Very Good;Good;Premium;Ideal;Fair\r\n2 E     Ideal;Premium;Good;Fair;Very Good\r\n3 F     Premium;Very Good;Good;Fair;Ideal\r\n4 G     Very Good;Ideal;Premium;Good;Fair\r\n5 H     Very Good;Good;Premium;Fair;Ideal\r\n6 I     Premium;Very Good;Ideal;Good;Fair\r\n7 J     Good;Very Good;Ideal;Premium;Fair\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:39:05-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-17-a-introvert-s-survival-guide-to-conferences/",
    "title": "A Introvert's Survival Guide to Conferences",
    "description": "Are you an introvert? Here's how to make conferences easier.",
    "author": [],
    "date": "2018-05-17",
    "categories": [
      "social",
      "networking"
    ],
    "contents": "\r\nIn academia, it’s inevitable to have to travel and present at conferences and meetings. As an introvert, I’ve been trying to compile a few tips that have helped me navigate large conferences so I don’t feel overwhelmed.\r\nIt is unfortunate that though the academic community has many introverts, conference and meeting structure is heavily biased towards extroverts. (No offense to extroverts, but some of you can sometimes seem like blowhards to us introverts.)\r\nThe question for introverts is how to connect with people at meetings while being true to our nature. We value meaningful and deep connections rather than shallow ones.\r\nSet Yourself Up For Success. Be prepared. If you can, try to connect with people or identify the people you want to meet before the conference even starts. Bring things that make you comfortable. A sweater, water, candy/energy bars. If there are sessions that look interesting, look at the author’s previous papers and think of questions beforehand. An introvert’s strength is that they are good at preparing for things. Use that preparation to your advantage.\r\nBe Curious, Not Scared. You are a curious person. If you weren’t, you wouldn’t be in science. Use that curiosity to connect. Wonder about things. Think about where research could go.\r\nBe the Host, Not the Hosted. According to Susan Cain, introverts do very well socially when they have a specific role. I like moderating sessions for this reason. I’m pretty good at being amiable and making other people comfortable in such situations. By being thoughtful and helping people with their problems, you can form connections.\r\nIt’s Totally Ok to Take a Break. It’s called self-care. Remember, introverts find social situations draining. Go outside for a few minutes, or find a corner that’s quiet and secluded, look at websites on your laptop. Just don’t withdraw. It’s a break, not a jailbreak.\r\nAccept That You’ll Miss Out. Emphasize quality over quantity. As an introvert, I am constantly feeling guilty about not going to every session and meeting people. FOMO can drive me to the point of being overwhelmed. But the truth is, if I can meet two or three people that I can connect with and two or three talks I find inspiring, I consider that a good meeting.\r\nMake Yourself Comfortable at Social Functions. Introverts can do very well socially when the situation is proscribed and the roles are clear, which is why I like poster sessions. There’s a limited set of questions you can ask and be asked. You can also do your homework by looking at the posters before you meet the person. Also: I hate asking public questions in meetings. (This again, is where extroverts can sometimes seem like blowhards to me. Bless their hearts.) I really prefer to go up to the speaker afterwards and try to have a meaningful conversation.\r\nRemember That Most People Are Equally Nervous. Be kind. Be compassionate to yourself and to others. This will make you more relatable. When you focus on making others comfortable, you concentrate less on your own nervousness. Kindness goes a long way in these situations. Help someone out who’s struggling.\r\nAsk Questions and Listen. Again, other people at conferences want to be heard and connect. You may run into raging egotists who don’t listen to you. It’s ok to politely disengage from the conversation. But there is something always interesting about everyone.\r\nGauge When You’re Exhausted. As an introvert, being with people can be draining. At some point, enough is enough. Take the evening off, grab something to go, and watch tv or read a book in your hotel room alone. It’s ok, no one is keeping score of how many social events you are going to. Again, it’s ok to not go to the big reception (unless you’re giving a poster at that time, of course).\r\nIt’s OK, the Right People Will Like You. You will eventually find people you want to talk to. Nurture these relationships. If someone gives you their card, follow up on it. It’s a sign of wanting to make connections.\r\nDon’t Expect The World of Each Social Interaction. I think sometimes we idealize what our perfect interaction would be. We get disappointed when our interactions are less than that. Prepare to be pleasantly surprised, but don’t get bored and wander off. Respect goes a long way, and also serendipity is good. Try to learn more outside your field.\r\nIt’s Totally Okay to be Awkward. Just own it. Most scientists are awkward in some way. (I know that I am, stop denying it people.) Being awkward is a way of being vulnerable - and being vulnerable is a way of connecting. I sometimes consider wearing an “I’m an introvert” button at conferences. At least it would be a good conversation starter. So go ahead, wear something quirky that you like. It can serve as a signal to find like minded people. Remember, own your awkwardness.\r\nBreathe. Conferences can feel like scary and unsafe situations to introverts. If you feel agitated, don’t force it. Take a moment and breathe.\r\nThe world of conferences doesn’t really value introverts, but it needs us (at least there are some people thinking about making conferences more introvert friendly). We’re thoughtful, slow to react, but we can synthesize and bring deeper understanding. Give us space to think and be ourselves, and we will contribute and add diversity to the conversation.\r\nFor more information\r\nThere are lots of blogs written about this. Here are some of my favorites.\r\nHow to Make the Most of a Conference as an Introvert\r\nHow to Survive Big Conferences As an Introvert\r\nHow Introverts can Make the Most of Conferences\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:31:30-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-25-health-informatics-course-materials/",
    "title": "Health Informatics Course Materials",
    "description": "Links to my lectures for our HSMP410 course.",
    "author": [],
    "date": "2018-04-25",
    "categories": [
      "oer",
      "open-education",
      "informatics"
    ],
    "contents": "\r\nSorry for the lack of posts. I have been busy with co-teaching our Health Informatics course (HSMP410) for the OHSU/PSU School of Public Health.\r\nI’m trying to make most of my lectures activity-driven for my students, who are Community Health Education and Nursing majors. I believe that you can teach mathematical concepts visually, so I am experimenting with using LearnR/Shiny to teach the basics of data literacy. I’m also using datacamp-light to show my students a simple intro to data science.\r\nMore information here: HMSP410 Health Informatics and the HMSP410 repository\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:30:25-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-04-a-simple-intro-to-genome-wide-association/",
    "title": "A Simple Intro to Genome Wide Association",
    "description": "Notes on our preprint about synthetic clinical data.",
    "author": [],
    "date": "2018-04-04",
    "categories": [
      "teaching"
    ],
    "contents": "\r\nThis term, I’m co-teaching an undergraduate course for the PSU/OHSU School of Public Health called Health Informatics with a number of my collegues in my department, including Bill Hersh, Eilis Boudreau, Karen Eden, and Virginia Lankes. We’re trying to give students a feel for what informatics is about in an accessible way.\r\nI’m trying to make the lectures as understandable as I can. This week we tackled Genome Wide Association Studies and discussed the strength of evidence behind SNP variants associated with phenotypes. We ended with an activity where they queried the GWAS catalog for information on particular SNP variants.\r\nThe repo for my lectures is here and will be updated as the course goes along: https://github.com/laderast/HSMP410 The lecture is here: An Introduction to Bioinformatics: Genome Wide Association and the activity is here: SNP In class assignment. All my material is free to use and released under Apache 2.0, so if this is interesting to you, please feel free to fork it and remix it!\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-04-04-a-simple-intro-to-genome-wide-association/proportions-hsmp410.png",
    "last_modified": "2021-03-19T19:28:38-07:00",
    "input_file": {},
    "preview_width": 829,
    "preview_height": 395
  },
  {
    "path": "posts/2018-02-28-data-science-and-systems-science/",
    "title": "Data Science and Systems Science",
    "description": "A talk about how system science and data science are connected.",
    "author": [],
    "date": "2018-02-28",
    "categories": [
      "data-science",
      "systems"
    ],
    "contents": "\r\nI gave a talk for the Portland State University Systems Science seminar called How are Data Science and Systems Science Connected?. In this talk, I was highlighting current blind spots in Data Science that I think Systems Science approaches can address, especially that of interactions between features. I talked a little about my dissertation research (surrogate oncogenes), and the problem of black-box interpretability of predictive models.\r\nIf you’re interested in listening to the recording, the playback is available here: https://us.bbcollab.com/recording/059acf5cfed64f9abbf7cf432fdfb566\r\nSlides are here: https://laderast.github.io/sysc_data_sci\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:26:43-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-23-shiny-workshop-materials-posted/",
    "title": "A gRadual Introduction to Shiny",
    "description": "An workshop introduction to Shiny",
    "author": [],
    "date": "2018-01-24",
    "categories": [
      "visualization",
      "R"
    ],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Learning the flow of information between server and ui\r\n\r\n\r\n\r\nI just gave a workshop teaching the basics of Shiny (the interactive web visualization framework) for a group of PDX R users. We had 10 people attend, and most of the attendees managed to get through the material and had lots of good questions. I really enjoyed talking with everyone and I hope everyone learned something. We’re planning to give the workshop again to the larger PDX R user community, and some of the attendees last night have volunteered to be TAs.\r\nThe workshop materials consist of a GitHub repo and a Markdown document that can be done either in person or independently. The materials are freely available under an Apache 2.0 License.\r\nIn the workshop, we build a flexible csv (comma separated value) explorer that can load in csv data files with adaptive controls and tooltips.\r\nIn terms of packages, the workshop uses the tidyverse (mostly dplyr and ggplot2), and plotly to show some basic programming patterns in shiny:\r\nConnecting controls to ggplot2 aesthetics\r\nFiltering data using reactives\r\nThe observe/update_ pattern\r\nTooltips (the hard way/the plotly way)\r\nThe final product\r\nI’d love for more people to take a look at the workshop and would love any suggestions for making it better!\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-01-23-shiny-workshop-materials-posted/shiny-architecture.png",
    "last_modified": "2021-03-19T19:18:53-07:00",
    "input_file": {},
    "preview_width": 989,
    "preview_height": 373
  },
  {
    "path": "posts/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students/",
    "title": "What We learned teaching Python to Neuroscience Students",
    "description": "Our team-taught class introducing Neuroscience Graduate Program students to Python.",
    "author": [],
    "date": "2018-01-17",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nWell, the week of teaching our Python Bootcamp for Neuroscientists is over. I had the pleasure of working with a great group of students, professors and instructors in developing the material, and had a great time teaching complete beginners to programming and Python.\r\nWe had the overall goal of introducting 21 Neuroscience Graduate Program students at OHSU to the basics of programming in Python using data that they were interested in: electrophysiology data, and confocal microscopy data. The course was designed to be a 1 credit course to encourage students to persist and finish it.\r\nThe format of the class was spread over 5 days (2.5 hours a day) and had the following schedule:\r\nIntroduction to basic data types in Python\r\nIntroduction to for loops and Pandas DataFrames\r\nUsing Pandas to analyse electrophysiology data\r\nUsing NumPy to analyse confocal microscopy data\r\nEvaluation of students; installing Python/Juypter; wrap-up with questions.\r\nI’ll just write up some random thoughts about our experiences about the course. We are definitely planning to give the course again next year, given the enthusiastic reception.\r\nThings that really worked well\r\nAvoid the first day blues of installing Python by using JupyterHub. I think one of the major pain points for beginners is installing software before they can even learn. Instead of making them install Python the first day, we had them sign into an AWS server that had JuypterHub deployed. JupyterHub is a multi-user server for Juypter Notebooks which had the right version of Python and our need the dependencies installed. So our students just needed a laptop and a web browser to access our lessons. We could update the notebooks by pulling changes from our course repo.\r\nStephen David, my fellow instructor, figured a lot of the difficult deployment details out. He has put together some handy instructions about deploying JuypterHub to AWS and keeping the accounts updated via a GitHub repo in case other people are interested in using our bootcamp materials.\r\nMake the atmosphere welcoming to beginners. In order to do so, we used many great tips from Software/Data Carpentry: modeling resilience by using live coding (and making mistakes along the way), using post-it notes for students to signal when they need help or are finished, and having plenty of TAs per student (at least 4 students/TA or instructor). We tried to emphasize that learning programming is an ongoing process, and that even we still have to Google errors on Stack Overflow. Showing that you can make mistakes and still recover is a big part of that.\r\nPlan some early wins and make the exercises as interactive as possible. For the most part, we tried to avoid lecturing too long and break up the session with interactive exercises. I also really don’t like workshops where the trainer/teacher moves on no matter whether people understand the material or not. By using the post-its to signal when they were done, we were able to more appropriately pace the workshop. We also planned on stopping points if we couldn’t get through the day’s materials.\r\nEmphasize working together and building a community. From the beginning, we emphasized that everyone needed to work together. I always emphasize the chain of help: 1) First your programming partner, 2) then the TA help. Discussing and working on issues together fosters a sense of community. I think there will be a group of students who will really want to learn more because of this.\r\nGetting feedback along the way. I still feel like being a teacher is about 75% preparation and 25% improvisation. You need to be flexible enough to come up with examples on the fly, and you need to evaluate whether students are getting the material along the way. The exercises we tried to sprinkle throughout the notebooks helped us understand where people were stumbling.\r\nPlanning follow-up sessions. Through BioData Club, we’re planning some follow-up sessions. Through DataCamp in the Classroom, I also got our students premium access. We also pointed students out to other Python-based courses at OHSU.\r\nSome things we could improve on\r\nI believe that given our time frame, we couldn’t really have anticipated many of these issues. We did our best to deal with them in the moment, however.\r\nDescribing the difference between Jupyter Notebooks and Python. At the beginning, we glossed over what a Jupyter Notebook was and really didn’t describe its relation to Python. I think next time we will open with describing the relationship between Jupyter and Python with a diagram, and revisit it on the last day.\r\nAnticipate the JupyterHub server requirements better. On Day 3, we had a large dataset that basically hosed the server because 21 students were trying to open it up at once. We managed to recover by getting another AWS server and dividing the students among the two, but we could have stress tested that day a little more. Lesson learned.\r\nGoing slowly enough. I am a very excitable teacher, to the point of which sometimes I go a little too fast. I have to confess that I may have sped through some of the material a little too fast. As a result, some of the students didn’t quite get what functions like enumerate() were for and the concept of unpacking a list. Luckily, Brad Buran covered these on Day 4 and the students felt comfortable enough to finish the programming test on the final day.\r\nSetting student expectations. It’s vital to show the students that they can learn programming, but also what’s possible if they do. One of the days was a big leap from the previous day, but we did mention that it’s really to show them what’s possible if they continued to learn about programming.\r\nWould we do it again?\r\nI would definitely say yes! We had to waitlist some students who really wanted to take it, and our overall feedback about the course was really positive. I hope that we can have more TAs, and have the future data workshops be more student driven.\r\nAcknowlegements\r\nThis was a collaboration between the Neuroscience Graduate Program (NGP) and the Department of Medical Informatics and Clinical Epidemiology (DMICE).\r\nThe NGP students involved in designing and testing the material were\r\nDaniela Saderi\r\nLucille Moore\r\nCharles Heller\r\nZack Schwartz\r\nFaculty/Instructors involved were:\r\nBrad Buran (Research Instructor)\r\nStephen David (NGP Assistant Professor)\r\nLisa Karstens (DMICE Assistant Professor)\r\nMichael Mooney (DMICE Assistant Professor)\r\nTed Laderas (DMICE Assistant Professor)\r\nThanks very much to Gary Westbrook (Director of the NGP program), Shannon McWeeney (Head of the Division of Bioinformatics and Computational Biology within DMICE), and Bill Hersh (Head of DMICE).\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-01-17-what-we-learned-teaching-python-to-neuroscience-students/py_class.jpg",
    "last_modified": "2021-03-19T19:13:32-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-15-if-you-want-to-talk-with-me-for-an-informational-interview/",
    "title": "If you want to talk with me for an informational interview",
    "description": "I get a lot of requests for informational interviews. Here's how to actually get one.",
    "author": [],
    "date": "2018-01-15",
    "categories": [
      "talking"
    ],
    "contents": "\r\n\r\n\r\nNote: I have come to believe that this post is way too harsh and unwelcoming to people interested in data science. I will post an update to this, and a FAQ for those who want informational interviews. But in the spirit of showing my mistakes, I’ll leave this up here.\r\n\r\n\r\nI have had many people who have asked me for informational interviews. They tell me that they are interested in Data Science and want to hear about what I do on a day to day basis. To be honest, I’ve begun to dread these kinds of interviews.\r\nInevitably, I spend a lot of energy explaining what I do to someone who rarely follows up. Consequently, I don’t find these interviews rewarding at all. So I’ve written this post so that I have a better time doing these kinds of interviews.\r\nI reserve the right to refuse interviews from people who do not read this.\r\nDo your homework. Please don’t expect me to give the five minute spiel about my research, hoping to look for an “in”. Please do some research and try to ask interesting questions about my work. I’ve given you plenty of resources on this website to know more about me. Ask me about my software, ask me about teaching, ask me carefully thought questions about my research.\r\nDon’t offer to buy me coffee. If I talk with you for 30 minutes, know that my time is worth far more than a cup of coffee. Instead, offer to pay it forwards. Volunteer with a group that does scientific communication or education; I don’t work with people who aren’t willing to teach others. I don’t work or talk with selfish people, having been burned many times by such people.\r\nBe specific in your ask. Asking about what next steps to take in learning data science and where to get a job is not specific enough. Again, do your research. What kinds of Data Science are you interested in? Are you interested in predictive analytics in healthcare? Or are you interested in systems modeling of disease? Be specific, and if you ask for something, make sure I can achieve it in five minutes or less.\r\nDon’t ask for a job, ask for connections. I know a decent number of people around Oregon and OHSU, so if you want me to introduce you to one of my connections, I’m happy to. If I know someone, I’m happy to do this, since it’s usually a five-minute ask.\r\nFollow up, and be willing to return the favor. Even a nice thank-you email is good. I’m happy to make you a Linkedin connection, if it means I can help someone else further down the line. If I expend energy on you, I’d like to see my impact. If my efforts meant that you managed to find a job, please let me know!\r\nRead Give and Take. This book by Adam Grant really struck me as the way to actually network. In short, I learned that if I want a return on my energy expenditure, I have to spend my energy on Givers, or people who help others. I am a Giver, but I now tend only to give informational interviewers to other Givers. The rest are too draining for me.\r\nI have given so many of these interviews and have gotten nothing from them. Not even a follow-up, which really is bad practice. So, if I’m considered difficult to approach about these, know it is because your previous askers have really been an energy drain and have been inconsiderate Takers. Make it interesting for me and be considerate. I’m much more likely to help you.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:15:13-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-05-so-you-ve-accidentally-checked-in-a-large-file-into-git/",
    "title": "So You've Accidentally Checked in a Large File Into Git",
    "description": "Don't panic. I'll show you how to scrub your Git history and get rid of it.",
    "author": [],
    "date": "2018-01-05",
    "categories": [
      "git",
      "git hell"
    ],
    "contents": "\r\nNote: after posting this, I heard back from Roberto Tyley, the creator of the BFG. I’d like to note that the BFG actually does its job really well. I was mostly really frustrated about how Git/GitHub doesn’t prevent a user from doing something that’s hard to undo. So my frustration is really about that, not really about the BFG. This post has been edited to reflect that.\r\nGreg Wilson first said it, but I’ve come to agree. Git is an aggressively antisocial piece of software. Git is a piece of software that can make developers with any amount of experience feel dumb.\r\nRecently, I accidentally checked a large file (greater than 100 Megs) into my local repo. When I tried to push to GitHub, of course, it refused it (I know about git large file storage, but I don’t have any).\r\nSo my local repo was screwed up. Of course, I did what seemed like the rational thing and deleted the file from my repo and recommitted. More than once. This is a mistake I’ve done more than once. So you need to scrub your git history with BFG so that GitHub will accept your lowly commits again.\r\nThe BFG documentation specifies how to fix a remote repo. I would say that this situation is much less common than the local situation. So I just decided to share how I got the BFG to work for the local repo situation.\r\nI usually install the BFG through homebrew, using brew install bfg. When you install it this way, you can just run BFG with bfg. You can download it from the website, but you’ll have to call java -jar bfg[VERSION].jar to run it.\r\nSay you’ve accidently checked in a large file into your current repo. The first thing to do is to clone your local repo:\r\ngit clone --mirror local_repo\r\nThis will create another folder called local_repo.git that you will do all the BFG magic on. This local_rep.git is what is called a bare repo. I want to remove any files larger than 100 Megs, so I do this:\r\nbfg -b 100M local_repo.git\r\nIf this doesn’t return an error, you can move on. However, I got the dreaded error:\r\nWarning : no large blobs matching criteria found in packfiles - \r\ndoes the repo need to be packed?\r\nAugh. Ok, some googling later I found that I needed to pack my orignal repo:\r\ncd local_repo\r\ngit repack\r\nOkay, we need to get rid of our cloned repo and redo the last few steps.\r\nrm -rf local_repo.git\r\ngit clone --mirror local_repo\r\nbfg -b 100M local_repo.git\r\nThen comes some git commands that no one has bothered to explain to me (UPDATE: I forgot to add changing directories into local_repo.git - sorry about that!).\r\ncd local_repo.git\r\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive\r\ngit push\r\nUh oh, I get a remote: error: refusing to update checked out branch: refs/heads/master error! More ugh.\r\nHere’s the trick. Since you cloned a local repo, you need to set the origin of your current repo (local_repo.git) to the GitHub remote. Still in our local_repo.git directory, first we remove the current origin, and then add back our remote.\r\ngit remote rm origin\r\ngit remote add origin https://github.com/laderast/remote_repo\r\nFinally, after much gnashing of the teeth, we can\r\ngit push\r\nDon’t forget to remove your now dirty local_repo, and the mirrored copy, and then pull a fresh copy down!\r\n##remove both original local repo and altered bare repo\r\nrm -rf local_repo\r\nrm -rf local_repo.git\r\n##clone a fresh copy from GitHub\r\ngit clone https://github.com/laderast/remote_repo\r\nThis may be obvious to the 10 people in the world who have read all of the git documentation, but I am not one of them. I’m stuck with git, unfortunately. I’m writing this post to remind me of what to do when I innocently do something like commit a large file to my repo.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:08:42-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-14-using-synthetic-data/",
    "title": "Using Synthetic Data for Teaching Data Science",
    "description": "Notes on our preprint about synthetic clinical data.",
    "author": [],
    "date": "2017-12-14",
    "categories": [],
    "contents": "\r\nHi Everyone, our paper called Teaching data science fundamentals through realistic synthetic clinical cardiovascular data is now available to read on Biorxiv.\r\nIn this paper, we talk about a dataset that we synthesized for teaching aspects of clinical data that may be tricky to understand in data science. This dataset is interesting because it’s derived from a multivariate distribution based on real patient data, modeled as a Bayesian Network. Even when we knew true marginals for the real data, there was a lot of fine tuning to the Bayesian Network.\r\nWe’ve used this dataset for a couple of classes, and we’ve found that it helps highlight real issues in predictive modeling of clinical data. One of the largest is that most predictive models are based on a much older patient cohort (50+), which means that we don’t know much about how to predict cardiovascular risk in younger patients. Part of the teaching exercise is having the students choose a cohort of interest and then attempt to predict on that patient cohort.\r\nThe data is currently available as an R package here, including vignettes about how the data was generated: https://github.com/laderast/cvdRiskData\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T19:19:57-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-11-07-ODSC-Notes/",
    "title": "Notes on Open Data Science Conference West 2017",
    "description": "Some notes on ODSC West 2017.",
    "author": [],
    "date": "2017-11-07",
    "categories": [],
    "contents": "\r\nI just came back from the Open Data Science Conference (ODSC) in San Francisco and I found it really stimulating and interesting. I learned a ton, met some great people working in very different fields, and overall found it quite worthwhile.\r\nHere are some of the highlights from my notes:\r\nWorkshops\r\nscikit-learn intro Workshop and Advanced\r\nI admit that I am not really a Python person. But I am helping to develop some materials for an introductory workshop and I found this workshop and its materials to be a very beginner-friendly to scikit-learn and machine learning concepts, much like caret for R. All the slides and workshop materials are available at the above links.\r\nSparklyR Workshop\r\nI liked this workshop from John Mount of Win-Vector. It started out with a dplyr intro, and introduced us to the basics of Apache Spark, which is a cluster-computing based machine learning framework, which is designed to do very large queries and machine learning. RStudio’s Edgar Ruiz managed to get us each an RStudio Pro Instance running on AWS with all the required packages installed so we could test out the SparklyR package, which uses dplyr’s commands to run Spark jobs.\r\nIn-Memory Computing Essentials for Data Scientists\r\nThis was an introduction to Apache Ignite, which is a distributed, in-memory database that can be leveraged by different languages. The really interesting thing about Ignite is that it will colocate related data on the same cluster node, resulting in rapid queries within each node. I think this technology will become very important as we need more datasets to be openly accessible to compute on.\r\nTalks\r\nThese were the most interesting talks that I attended.\r\nVisually Explaining Statistical and Machine Learning Concepts\r\nThis was a great talk by Mike Freedman about his process of how he put together D3.js based visualizations to explain some statistical concepts. I thought the explanation of his process (isolate specific ideas, identify data structures, leverage visualization algorithms). Check out the slides above. They’re very cool.\r\nThe Wonder Twins: Data Science and Human Centered Design\r\nThis was a really interesting talk about the interplay between data science and design in helping encourage a mobile money system in Tanzania. It was inspiring to see how they had both designers and data scientists embedded and looking at how the mobile payment system worked. One interesting example was doing network analysis of the Mobile Money Agents, who distribute cash. They targeted a highly influential group of these agents based on this analysis. Very cool.\r\nThe People’s Data and The Deontology of Data Science\r\nI thought these were really interesting sides about the human side of data science. DJ Patil, who was chief data scientist under the Obama administration, talked about citizen-driven data projects and how it enabled a number of advances. The most interesting case was basically a parent built an online community of people who had a very rare disease condition so he could help his son with the condition.\r\nIgor Perisic (of LinkedIn) followed this with a talk about ethical issues in data science. In particular, he identified three different areas to concentrate on: 1) The Ethics of Data, 2) The Ethics of Algorithms, and 3) The Ethics of practice. He concentrated on the recent New York Times article about using machine learning to identify potential re-offenders in the prison system. The lack of transparency in how the algorithm identifies potential reoffenders is a huge ethical problem.\r\nIn all, I had an interesting time and I met lots of people in industry, which was a nice contrast to the academic side of things.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:07:51-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-05-interesting-user2017talks/",
    "title": "Interesting useR 2017 Talks",
    "description": "useR 2017 talks and links.",
    "author": [],
    "date": "2017-07-05",
    "categories": [],
    "contents": "\r\nSince I didn’t get to go to useR 2017 this year, I’m compiling the interesting talks. This is an ongoing list.\r\nhttps://user2017.sched.com/event/AxqM/automatically-archiving-reproducible-studies-with-docker\r\nhttps://user2017.sched.com/event/Axq4/clouds-containers-and-r-towards-a-global-hub-for-reproducible-and-collaborative-data-science\r\nhttps://user2017.sched.com/event/Axq9/scraping-data-with-rvest-and-purrr\r\nhttps://user2017.sched.com/event/Axq1/using-the-alphabetr-package-to-determine-paired-t-cell-receptor-sequences\r\nhttps://user2017.sched.com/event/AxqG/show-me-the-errors-you-didnt-look-for\r\nhttps://user2017.sched.com/event/AxqR/community-based-learning-and-knowledge-sharing\r\nhttps://user2017.sched.com/event/AxqT/r-based-computing-with-big-data-on-disk\r\nhttps://user2017.sched.com/event/AxqA/codebookr-codebooks-in-r\r\nhttps://user2017.sched.com/event/Axor/how-we-built-a-shiny-app-for-700-users Useful concepts: reactiveTrigger to force a rerender.\r\nhttps://user2017.sched.com/event/AxsL/ensemble-packages-with-user-friendly-interface-an-added-value-for-the-r-community\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:06:19-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-06-28-HowToNotBeAfraid/",
    "title": "How to Not Be Afraid of Your Data",
    "description": "A Past Workshop about EDA and data exploration.",
    "author": [],
    "date": "2017-06-28",
    "categories": [],
    "contents": "\r\nI’m going to be giving a talk for the PDX RLang Meetup on July 11 called “How to Not Be Afraid of Your Data: Teaching EDA using Shiny”. Abstract below.\r\nMany graduate students in the basic sciences are afraid of data exploration and cleaning, which can greatly impact their downstream analysis results. By using a synthetic dataset, some simple dplyr commands, and a shiny dashboard, we teach graduate students how to explore their data and how to handle issues that can arise (missing values, differences in units). For this talk, we’ll run through a simple EDA example (combining two weight loss datasets) with a general data explorer in shiny that can be easily customized to teach specific EDA concepts.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:05:38-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-06-07-CascadiaRNotes/",
    "title": "Some Lessons We Learned Running Cascadia-R 2017",
    "description": "How we started and ran the first PNW Regional R Conference, Cascadia-R",
    "author": [],
    "date": "2017-06-07",
    "categories": [],
    "contents": "\r\nWell, the first Cascadia R Conference has come and gone. I have to say that it was super fun, and well attended (over 190 people!). I had a blast meeting and chatting with everyone. Hopefully, we showed newbies that R is learnable and others that there are lots more things to learn about R.\r\nThe following is my attempt to document what we learned from organizing Cascadia-R. It’s not complete; I may add and subtract from it as I think of more things to say about the planning process.\r\nDecide the tone. Our goals with Cascadia-R were modest. We wanted to get a diverse group of R users together in a safe and encouraging environment. We wanted our workshops to be accessible to even beginners, and encourage them in the use of R.\r\nPart of meeting these goals of this is setting the tone. We really wanted to encourage all levels of R users to attend. All of our flyers, emails and promotional tweets encouraged beginners to come. We got help with making a Code of Conduct for the conference. Part of creating a supportive environment is encouraging diversity in both speakers and attendees. We did our best to reach out to current groups that encourage diversity, such as Women in Science Portland, and R-Ladies Global.\r\nWe also offered diversity scholarships to encourage people from diverse backgrounds to attend, and made diversity part of our criteria for selecting talks.\r\nStart planning early. As junior faculty at OHSU, I’m lucky enough to be able to book facilities here, including the large learning studios where we held the conference. Having the venue secured early on made the remaining logistics of the conference much easier.\r\nMuch like wedding planning, there are plenty of conference planning services out there who would be happy to take over aspects of your conference, for a fee. You can spend however much you want to on these things. However, I believe that such a approach is not financially responsible. I also feel that taking a more DIY/bespoke approach can make a conference most engaging (see csvconf). We tried to do most things ourselves (including design, promotion, talk submission, workshops, and registration/logistics).\r\nIterate your budget. Think of a conference as a project with lots of linked dependencies. Your first plan is probably not going to be your final plan. Start a plan, iterate, realize that things are going to shift, have a backup plan. What if registration is not going to pay for the venue rental fee? Talking to simpatico sponsors can take much of the financial stress. In our case, the Rstudio foundation and ROpenSci stepped up to contribute some money as a cushion.\r\nRemember, there are fixed costs (such as venue rental, and recording/streaming costs) and variable costs that scale with the number of attendees (food, badges, alcohol). Separate these out. When possible, pay off the fixed costs first, so that it’s easier to manage the variable costs.\r\nAgain, who is your desired audience and can they afford your conference? We decided to make our conference as affordable as possible to encourage as many different kinds of people to attend. We initially wanted to make attendance free for students. The problem with free is that literally it’s free. It has no value in the mind of a person who accepts free admission. So we decided to charge students a small fee just to emphasize that the conference has value.\r\nTalk with others who have done it. We were very clueless about much of the logistics side at OHSU. I managed to get through by talking with a number of people here (including Robin Champieux and Shannon McWeeney) who have done conferences here at OHSU. Thank you so much for your invaluable advice.\r\nEncourage each other and delegate. No one of us could have done all of the conference planning alone. Each of us took on various aspects of conference organization and brought in the others as support as needed. Some of us selected talks, some of us did design, and we all pitched in to get registration working as efficiently and quickly as possible.\r\nOur slack channel on pdxdata.slack.com is full of our decisions. Slack was so useful as a planning mechanism that we only met online via Google Hangouts a few times, and only had two in-person planning sessions.\r\nBe Willing to Make Mistakes. Lord knows I made a bunch of mistakes when I made announcements and hosted the lightning sessions. However, I owned up to these mistakes, shrugged, and moved on. Improvising in the moment can be just as important as planning.\r\nThink about the future. What should the next Cascadia-R look like? I know it just happened, but we’re trying to envision what it would look like. Based on the feedback we’ve gotten so far, people really want more workshops!\r\nIn a following post, I’m also going to talk about lessons I learned when Chester and I put on our tidyverse workshop.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:04:48-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-19-Breadth-and-depth/",
    "title": "On Breadth and Depth in Your Academic Career",
    "description": "Why Learning and hobbies outside of your field are important.",
    "author": [],
    "date": "2017-04-19",
    "categories": [],
    "contents": "\r\nI was talking with a student and they were complaining that when at conferences, they would try to inject other topics of interest (such as cooking) into discussions with colleagues. Unfortunately, one of the after effects of this was that they were looked at as “not a serious scientist”. There’s an expectation that a scientist must be all depth, only talking and thinking about their sub-field.\r\nAs a cross disciplinarian, I have to say that is hogwash. The genesis of so many creative ideas in science has happened because of cross-pollination across disciplines. For example, microwave technology might never have been invented without the intersection of disciplines. We know that the Arts Foster Scientific Success - a large number of Nobel and National Academy members do art in some form or other. Bernstein et al theorize that\r\n\r\n“there exist functional connections between scientific talent and arts, crafts, and communications talents so that inheriting or developing one fosters the other.”\r\n\r\nHaving breadth and depth enables you to make connections that no one else has. It is the hallmark of a curious and creative person. These kinds of people are desparately needed to push science in new directions.\r\nI have a parallel career in performance and improvisational music. Music, for me, is endlessly inspiring and has forced me out of my introverted shell. One of the reasons I took up cello is that I can play many roles; accompanist, rhythm, solo. This flexibility in playing music has translated to my flexibility in collaboration. Being able to adjust to new circumstances and improvise new ideas to explore is a critical component of being a responsible scientist. My background improvisation has helped me pivot ideas. I have become less attached to dogmatic ideas. Many of my good ideas come from idle wondering about data that has captured my imagination. This is part of the reason why I teach students how to explore their data.\r\nSo, the next time another scientist looks down at you for being a polymath, pity them. Their world and their ideas are not as rich as yours.\r\nFurther Reading\r\nThe Correlation Between Arts and Crafts and a Nobel Prize\r\nArts Foster Scientific Success\r\nDual Thinking for Scientists\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T15:43:18-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-17-Building-A-Peer-Mentoring/",
    "title": "Fostering a Peer Mentoring Culture",
    "description": "A bit about BioData Club",
    "author": [],
    "date": "2017-04-17",
    "categories": [],
    "contents": "\r\nI realize that it has been an embarrasingly long time since I updated this blog. I had all sorts of grandiose plans for it, and I think my problem was that I was thinking too broad, too pie-in-the-sky. I’m going to try to focus on short and informative blog posts.\r\nOne of the things that I have been thinking about graduate school is the idea of building a Peer Mentoring culture in our department. I believe that students should help and support each other, and we need to provide a forum to do that. Not just assign mentors, but provide a time and a place to do that.\r\nWe try to foster a mentoring culture within our student group, BioData-Club. Students are free to talk about issues that concern them, especially about datasets, and are encouraged to share their experiences of software that they’ve used. I believe that we try to give students a psychologically safe place to talk about their issues with data. We try to make people feel like they’re not alone, and coach beginners so they can get over the hump.\r\nWe’re now embarking on an experiment to reach even more people at OHSU, because we know there are lots of students who struggle with practical skills in data analysis. Our group is growing, and that’s exciting.\r\nI’m going to try and get everyone in our group to write a paper about Peer Mentoring Culture and how to encourage it in other schools.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T18:03:22-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-19-statcheck-package/",
    "title": "Statcheck Interview",
    "description": "Interesting interview with the developer of statcheck",
    "author": [],
    "date": "2015-11-19",
    "categories": [],
    "contents": "\r\nDue to the usual postdoc busy-ness, I haven’t had the energy to update this blog as much as I would like, but I thought this interview on Retraction Watch from Michèle B. Nuijten, the developer of the R-package statcheck to be fascinating. Her package essentially automates the checking of p-values given published data in papers, from converting the papers from pdf to text, and sees if the calculated p-values are correct. There was a lot of trial and error in parsing known formats for p-values, but now the package is available.\r\nI see an potentially really interesting master’s thesis in forensic bioinformatics in using the package to assess reproducibility of results in a field. Note that the student probably wouldn’t make any friends in high places, but it would be a potentially high impact thesis.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T16:39:56-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-26-mutational-burden-in-skin/",
    "title": "Somatic Mutations in Skin Paper",
    "description": "More about somatic mutations in skin cancer.",
    "author": [],
    "date": "2015-05-26",
    "categories": [],
    "contents": "\r\nThis paper, High burden and pervasive positive selection of somatic mutations in normal human skin is fascinating. It suggests that the mutational burden is much higher than we expected in skin cells due to UV exposure. In addition, subclones exist in the skin that are positively selected for oncogenes.\r\nIt also makes me want to stock up on sunscreen.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T16:36:46-07:00",
    "input_file": {}
  }
]
